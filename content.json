{"meta":{"title":"WEAF 周刊","subtitle":null,"description":null,"author":"WEAF","url":"http://weafteam.github.io"},"pages":[],"posts":[{"title":"内存中的栈、堆和静态区的用法","slug":"2018-05-28/内存中的栈、堆和静态区的用法","date":"2018-07-29T09:46:14.000Z","updated":"2018-08-07T08:56:15.425Z","comments":true,"path":"posts/978fb366/","link":"","permalink":"http://weafteam.github.io/posts/978fb366/","excerpt":"","text":"本周的第三篇博客，这个题目应该也是老生常谈的问题。 1 堆区 先了解下Heap的作用，堆区是专门用来存储对象的实例的，也就是我们平时通过new出来的对象，但是实际上这里面也只是保存了对象实例的属性值，属性的类型和对象本身的标记等一些内容，其中值得注意的是它里面并不保存对象的方法（方法也可以理解成指令，保存在stack中） 所以我们加以总结和延伸下： 1 它存储的都是对象，而且每个对象都包含一个与之对应的class的信息。这样做的目的是能够让对象得到操作的指令集。 2 需要注意的是JVM中只有一块堆区（Heap），而且它是被所有线程共享的。堆区中存放的只有对象本身，不存放基本数据类型和对象的引用。 3 关于堆区的分配和释放一般是由程序员做这个操作。其实这一点很好理解，程序员new一个对象相当于对这块区域做了分配的工作，而当程序员做delete操作时，相当于释放了这部分空间，同时如果程序员的习惯不好，不对无用的对象进行释放的话，当程序结束时，也会由OS做这个工作。 2 栈 然后我们再了解下Stack的作用，当我们在堆区中为我们new出来的对象分配好空间之后，我们需要在栈中保存一个4字节的Heap的地址，用来定位该对象实例在Heap中的位置，便于找到该对象实例。 其实Stack的功能不止于此： 1 不同于Heap，Stack的个数是和线程挂钩的，每个线程都包含一个Stack，而且栈中只包含了基本的数据类型的变量和对象的引用。 2 栈之间是相互独立的，所以栈中的数据都是私有的，其他的栈不能访问。 3 栈可以分为三个部分：基本数据类型变量区、执行环境的上下文、操作指令区 4 栈相对于堆区，它的分配和释放的工作是系统自己做的 3 静态区/方法区 类似于堆区，而且是被所有线程所共享的。方法区中存放了整个程序中所有的永远唯一的变量，例如：class信息和static变量等。 它也是分了三块：全局变量和静态变量的存储是放在一块的，初始化的全局变量和静态变量在一块区域， 未初始化的全局变量和未初始化的静态变量在相邻的另一块区域。 注意 栈是运行时的单位，而堆是存储时的单位 值得注意的是：堆和栈不都可以存储数据吗，为什么要将这两者区分开来？ ** 1 ** 首先从软件设计的角度来看这个问题，栈其实代表了处理逻辑，而堆代表了数据存储。这种分而治之的思想，使得我们的处理逻辑更为清晰。 ** 2 ** 因为堆是共享的，所以堆中的内容也是被多个栈共享的。这样做的收益有两个方面：一方面这种共享提供了一种有效的数据交互方式，另一方面，堆中的共享常量和缓存可以被所有的栈访问到。 以上。感谢驻足~","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://weafteam.github.io/categories/JAVA/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://weafteam.github.io/tags/面试/"}]},{"title":"StackOverflow上简单数字识别案例学习","slug":"2018-05-21/StackOverflow上简单数字识别案例学习","date":"2018-07-24T04:59:38.000Z","updated":"2018-07-24T08:16:53.881Z","comments":true,"path":"posts/8b09dcdd/","link":"","permalink":"http://weafteam.github.io/posts/8b09dcdd/","excerpt":"","text":"正如题目那样，这次的学习是StackOverflow上的一个案例，它的地址为https://stackoverflow.com/questions/9413216/simple-digit-recognition-ocr-in-opencv-python，但是源地址上的代码略微有点旧，很多函数在OpenCV3中有了些许的改动，所以我还是会贴上我自己改正过后的代码，然后附上我对作者的思路的理解整理以及对所有代码的理解，如有问题，欢迎和我进行沟通交流，邮箱地址：cliugeek@us-forever.com 思路： 整体的思路：我们需要有一个训练（其实严格意义上说不上是训练，其实我们最终要做的是每个数字的特征提取并且将他们与实际的数字字符一一对应起来，姑且就称这个过程为“训练”吧）的图像，然后我们提取其中所有的数字的特征，然后根据一一对应的的关系，将特征和对应的字符存储起来，以便我们后续做其他图像中的数字识别使用。 所以先贴一下训练用到的数据图： 其实这样的训练数据存在着一个问题，那就是所有的字母都是相同的字体和大小，所以整体的鲁棒性很受影响。所以这是本程序需要做继续优化的地方。 接下来我们讲具体的训练过程： 1 加载图像 2 检测数字（通过轮廓查找找，并且通过对检测到的字母的面积和高度做相应的约束，来避免错误的检测） 3 对检测到的字母绘制边框 4 每绘制出一个边框，我们就需要在键盘上键入相应的数字键，这一步很关键，否则会导致我们训练过程无效，导致后面一系列的工作无法进行 5 当我们按下相应的数字键之后，边框里面的东西便会进行重新的resize，变成10x10的大小，然后将其存入到一个100维的数组中（其实就是将这些像素值都存了进去作为我们的特征提取），相应的也会将我们键入的数字存到另一个data文件当中去 6 训练过程完成之后，系统生成两个data文件。 在手动数字分类结束时，训练数据（其实就是上面给大家的那张图像）中所有的数字，都已经由我们手工标记，结果如下： 下面就是针对上述过程的代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344import sysimport numpy as npimport cv2im = cv2.imread('digit.png')im3 = im.copy()gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)blur = cv2.GaussianBlur(gray, (5, 5), 0)thresh = cv2.adaptiveThreshold(blur, 255, 1, 1, 11, 2)################# Now finding Contours ###################image, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)samples = np.empty((0, 100))responses = []keys = [i for i in range(48, 58)]for cnt in contours: if cv2.contourArea(cnt) &gt; 50: [x, y, w, h] = cv2.boundingRect(cnt) if h &gt; 28: cv2.rectangle(im, (x, y), (x + w, y + h), (0, 0, 255), 2) roi = thresh[y:y + h, x:x + w] roismall = cv2.resize(roi, (10, 10)) cv2.imshow('norm', im) key = cv2.waitKey(0) if key == 27: # (escape to quit) sys.exit() elif key in keys: responses.append(int(chr(key))) sample = roismall.reshape((1, 100)) samples = np.append(samples, sample, 0)responses = np.array(responses, np.float32)responses = responses.reshape((responses.size, 1))print(\"training complete\")np.savetxt('generalsamples.data', samples)np.savetxt('generalresponses.data', responses) 代码其实也很简单，所以简单的说一下，首先对图像进行灰度化，然后进行高斯去噪，再然后做二值化。接下来进行轮廓检测，然后对每个检测到的轮廓在面积和高度上进行控制，如果符合设定的标准就进行画框、resize和display，同时将输入的key进行相应的存储，到最后进行save，将他们分别存到generalsamples.data和generalresponses.data中。 接下来进行测试部分，同样先给出测试所用的图像： 再然后就是测试部分的步骤：首先将刚才我们生成的data文件load进来，也就是将刚才的模型加载到内存中，然后我们用KNN（K邻近值法，选取与当前的测试图像中最相近的作为我们预测出来的值）做预测，然后针对于检测出来的每个轮廓都进行这样的操作，预测出来的图像，我们通过puttext方法放到将要生成的图像out上，最后将原图im和生成图out显示出来。 代码如下： 12345678910111213141516171819202122232425262728293031323334353637import cv2import numpy as np####### training part ###############samples = np.loadtxt('generalsamples.data', np.float32)responses = np.loadtxt('generalresponses.data', np.float32)responses = responses.reshape((responses.size, 1))model = cv2.ml.KNearest_create()model.train(samples, cv2.ml.ROW_SAMPLE, responses)# model.train(samples,responses)############################# testing part #########################im = cv2.imread('dig.png')out = np.zeros(im.shape, np.uint8)gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)thresh = cv2.adaptiveThreshold(gray, 255, 1, 1, 11, 2)image, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)for cnt in contours: if cv2.contourArea(cnt) &gt; 50: [x, y, w, h] = cv2.boundingRect(cnt) if h &gt; 28: cv2.rectangle(im, (x, y), (x + w, y + h), (0, 255, 0), 2) roi = thresh[y:y + h, x:x + w] roismall = cv2.resize(roi, (10, 10)) roismall = roismall.reshape((1, 100)) roismall = np.float32(roismall) retval, results, neigh_resp, dists = model.findNearest(roismall, k=1) string = str(int((results[0][0]))) cv2.putText(out, string, (x, y + h), 0, 1, (0, 255, 0))cv2.imshow('im', im)cv2.imshow('out', out)cv2.waitKey(0) 这部分的代码其实按照刚才的流程理解就可以，所以不再赘述。 最后给大家看下效果： 我仔细对比了一遍，准确率可以达到100%。对于这个简单的例子，这样的结果可以称得上完美吧。 以上就是本次学习的所有内容。感谢驻足~","categories":[{"name":"OCR","slug":"OCR","permalink":"http://weafteam.github.io/categories/OCR/"}],"tags":[{"name":"OpenCV-Python","slug":"OpenCV-Python","permalink":"http://weafteam.github.io/tags/OpenCV-Python/"}]},{"title":"Thread和Runnable的区别","slug":"2018-05-14/Thread和Runnable的区别","date":"2018-07-23T07:32:42.000Z","updated":"2018-07-24T03:47:30.783Z","comments":true,"path":"posts/42fb2e58/","link":"","permalink":"http://weafteam.github.io/posts/42fb2e58/","excerpt":"","text":"两周的小暑假也算是过完了，接下来得好好做东西了，博客从今天开始也要跟上进度了。 1. 两种创建的方式 1 继承Thread类，并且重写其中的run()方法 2 实现Runnable接口，重写其中的run()方法 但是在实现应用时，我们多用实现Runnable接口的方式，这是因为Java的单继承多实现的机制，这样一来就可以避免由于这个机制代码的局限性。其实我们用Runnable的原因不止于此，最主要的一点是Runnable是可以进行数据共享的，但因此它也是线程不安全的。现在对这两点进行逐一解释，我们首先通过一个老生常谈的售票的例子，对数据共享这一点进行解释。 2. 示例代码 示例代码1 首先我们看一下我们自己定义一个MyThread，让它继承Thread，然后我们定义一个count，这个count参数可以想象成我们要共享的资源。然后代码就很简单了，如下： 123456789101112131415161718192021222324252627282930public class TestThread &#123; public static void main(String[] args) &#123; // TODO Auto-generated method stub MyThread thread1 = new MyThread(\"一号窗口\"); MyThread thread2 = new MyThread(\"二号窗口\"); MyThread thread3 = new MyThread(\"三号窗口\"); thread1.start(); thread2.start(); thread3.start(); &#125; public static class MyThread extends Thread &#123; private String name; public MyThread(String name) &#123; this.name = name; &#125; int count = 5; @Override public void run() &#123; // TODO Auto-generated method stub while (count &gt; 0) &#123; System.out.println(\"当前售票窗口为：\" + this.name + \" 售票：\" + count--); &#125; &#125; &#125;&#125; 运行结果1： 我们先将两个代码都给大家，稍后再做分析。 示例代码2 同样，我们定义一个内部类，让它实现Runnable接口，代码如下： 12345678910111213141516171819202122232425262728public class testRunnable &#123; public static void main(String[] args) &#123; // TODO Auto-generated method stub MyThread mt = new MyThread(); Thread t1 = new Thread(mt, \"一号窗口\"); Thread t2 = new Thread(mt, \"二号窗口\"); Thread t3 = new Thread(mt, \"二号窗口\"); t1.start(); t2.start(); t3.start(); &#125; public static class MyThread implements Runnable &#123; private int count = 5; @Override public void run() &#123; // TODO Auto-generated method stub while (count &gt; 0) &#123; System.out.println(\"当前线程为：\" + Thread.currentThread().getName() + \" 售票为：\" + count--); &#125; &#125; &#125;&#125; 运行结果2： 3. 分析 通过代码运行结果我们不难看出，这两种方法的截然不同，相信大家现在也能理解了，为什么说Runnable是可以进行资源共享的，不过还是在多说一点吧。 我们继承了Thread的MyThead类，通过三次实例化，分别的将这三个再start起来，就相当于给了三个售票窗口每人一个卖5张票的任务，他们因为是三个实例，各自做各自的事情，各自完成这个卖票的任务。 而我们实现了Runnable的MyThread类，相当于先创建了一个任务（其实就是我们new MyThread过程）,然后通过实例化三个Thread，创建了三个线程共同去完成这个任务。 那为什么说Runnable是线程不安全的呢，理由其实很简单：如果我们在System.out…之前加上一个线程休眠操作的话，就会很有可能导致我们的count最后能输出-1，也就是说存在一张票被售卖两次的状况。如果想对于这一点进行测试的话，大家要注意一点，票数要设置的大一些，休眠时间写成1毫秒就行，这样的测试效果比较明显。 那么如何才能保证我们用Runnable时的线程安全性呢？思路很简单，就是我们需要在这个地方加上同步操作（互斥锁），确保在同一时刻只有一个线程在执行售票的操作。而我们之前的继承Thread的方法并不需要这么做，原因就是每个线程执行自己的Thread对象中的代码，不存在多个线程共同执行同一个方法的情况。 以上是本次的所有内容，感谢驻足~","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://weafteam.github.io/categories/JAVA/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://weafteam.github.io/tags/面试/"}]},{"title":"Java中List和ArrayList的区别","slug":"2018-05-07/Java中List和ArrayList的区别","date":"2018-06-23T11:10:07.000Z","updated":"2018-06-24T01:34:18.543Z","comments":true,"path":"posts/e85880/","link":"","permalink":"http://weafteam.github.io/posts/e85880/","excerpt":"","text":"开始补博客了从5月初到现在应该有8篇博客需要补，废话不多说，开始写吧 区别 这俩个的区别很明显，List是一个接口，而ArrayList是一个类，它继承AbstractList抽象类并且实现了List接口。 所以当我们需要实例化一个List的时候，我们并不能直接的new一个List（显然是废话，接口肯定是不能通过new实例化的），而只能是实例化一个继承并实现它的类的实例并将这个实例化的值赋值给我们的list实现List的实例化。说了这么多，其实可以用一行代码来表示（其实就是创建一个指向List对象的引用，这就是面向对象编程中多态的优势吧！）。 12345//例如我们要拿来比较的ArrayListList list = new ArrayList();//或者继承List的其他类List b = new LinkedList&lt;&gt;(); 所以通过以上的代码，可以很清晰的看到，List的实例化的具体操作，当然你也可以直接的写一行 1List list; 只不过这样做出来的list是一个空的列表，你可以在后续的操作为它做填充。 注意事项： 我们接着刚才的 List list = new ArrayList(); 来说。首先list是一个List类型的对象，虽然我们是new的一个ArrayList的，但是有些ArrayList具有的而List类中没有的属性和方法，list都不能再用了。 那么如何才能保证ArrayList中独有的方法和属性都能被使用到呢？很简单，毕竟ArrayList是一个类，直接实例化是没问题的，这样它所有的属性和方法就都可以使用了。 1ArrayList A = new ArrayList(); 我们再接着讲List list = new ArrayList();如果List和ArrayList中存在相同的属性（例如：int i）和相同的方法（例如：f()），这时候list.i的时候，我们调用的是List中的属性，而a.f()调用的是ArrayList中的方法f()。 其他 有关于List和ArrayList本文用到的是没有指定泛化类型的写法，这个也是一个很重要的点，有关实装箱和拆箱的内容，我这提供一个网址。 http://www.cnblogs.com/a164266729/p/4561651.html 本文参考 http://www.cnblogs.com/aisiteru/articles/1151874.html https://blog.csdn.net/erlian1992/article/details/51298276","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://weafteam.github.io/categories/JAVA/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://weafteam.github.io/tags/面试/"}]},{"title":"asyncio 不完全指北（七）","slug":"2018-06-04/guide-to-asyncio-7","date":"2018-06-09T09:35:25.000Z","updated":"2018-08-07T13:19:42.444Z","comments":true,"path":"posts/a6235d78/","link":"","permalink":"http://weafteam.github.io/posts/a6235d78/","excerpt":"","text":"书接上文。 使用 aiohttp 作为 Web 服务器 上篇文章中提到，aiohttp 不仅仅是一个 http 客户端，同时也是一个 Web 服务器。在这一节，我们使用 aiohttp 实现一个简单的 Web 程序，同时与 flask 比较一下性能上的差别。 准备工作 首先安装我们需要的第三方库： 12pip install aiohttppip install flask 然后准备好要使用的 Web 容器，这里我们使用对 aiohttp 和 flask 都很友好的 gunicorn。为了让 flask 得到异步支持， 需要同时安装 gevent： 1pip install gunicorn[gevent] 安装 wrk，它是一个简单的性能测试工具： 1brew install wrk Hello, world 我们的起手式当然是 Hello, world。这里，我们分别使用 flask 和 aiohttp 实现一个返回 Hello, world 的 Web 服务。 flask 123456789# flask_app.pyfrom flask import Flaskapp = Flask(\"flask_app\")@app.route(\"/\")def hello(): return \"Hello, world!\" 非常简单！ 接下来让我们看一下性能测试的结果。首先用 gunicorn 启动应用，将 socket 绑定到 localhost:5000，打开访问日志，使用 4 个 worker，并使用 gevent 作为 worker 的类型： 1gunicorn -b localhost:5000 --access-logfile - -w 4 -k gevent flask_app:app 然后就可以进行性能测试了，这里我们使用 8 个线程，每个线程负责 200 个请求，共测试 10 秒，并开启详细日志： 1wrk -t8 -c200 -d10s --latency http://localhost:5000 我们会得到这样的结果： 1234567891011121314Running 10s test @ http://localhost:5000 8 threads and 200 connections Thread Stats Avg Stdev Max +/- Stdev Latency 9.80ms 94.91ms 1.67s 99.06% Req/Sec 1.07k 555.92 2.14k 63.76% Latency Distribution 50% 1.55ms 75% 1.82ms 90% 2.27ms 99% 20.89ms 23325 requests in 10.01s, 3.96MB read Socket errors: connect 0, read 0, write 0, timeout 8Requests/sec: 2330.74Transfer/sec: 405.20KB 这里只关注几个重要的信息： Latency，可以理解为响应时间，wrk 提供了平均值，标准差，最大值，以及正负一个标准差的占比； Req/Sec，每个线程每秒钟的完成的请求数，同样有以上数据类型； Latency Distribution，响应时间的分布情况，50%、75%、90%、99%的请求在多长时间内结束； Socket errors，在测试中有多少错误发生； Requests/sec，每秒钟完成多少请求； Transfer/sec，每秒钟产生的数据量； 知道了上述信息的含义，就可以对应用程序的性能有一个大概的了解了。 aiohttp 同样我们用 aiohttp 实现一个 Hello world 应用： 12345678910111213# aio_app.pyfrom aiohttp import webroutes = web.RouteTableDef()@routes.get(\"/\")async def hello(request): return web.Response(text=\"Hello, world!\")app = web.Application()app.add_routes(routes) 然后用 gunicorn 启动它： 1gunicorn -b localhost:5000 --access-logfile - -w 4 -k aiohttp.GunicornWebWorker aio_app:app 大部分参数都相同，唯一的区别是使用了 aiohttp 的 wroker 类型。 也用同样的参数启动 wrk，得到以下结果： 12345678910111213Running 10s test @ http://localhost:5000 8 threads and 200 connections Thread Stats Avg Stdev Max +/- Stdev Latency 32.58ms 15.57ms 120.43ms 68.40% Req/Sec 773.91 167.40 1.16k 67.25% Latency Distribution 50% 36.83ms 75% 42.40ms 90% 46.73ms 99% 66.48ms 61709 requests in 10.03s, 9.65MB readRequests/sec: 6150.18Transfer/sec: 0.96MB 对比 我们把性能测试的结果放在一块对比一下，左边是 flask，右边是 aiohttp： 1234567891011121314Running 10s test @ http://localhost:5000 Running 10s test @ http://localhost:5000 8 threads and 200 connections 8 threads and 200 connections Thread Stats Avg Stdev Max +/- Stdev Thread Stats Avg Stdev Max +/- Stdev Latency 9.80ms 94.91ms 1.67s 99.06% Latency 32.58ms 15.57ms 120.43ms 68.40% Req/Sec 1.07k 555.92 2.14k 63.76% Req/Sec 773.91 167.40 1.16k 67.25% Latency Distribution Latency Distribution 50% 1.55ms 50% 36.83ms 75% 1.82ms 75% 42.40ms 90% 2.27ms 90% 46.73ms 99% 20.89ms 99% 66.48ms 23325 requests in 10.01s, 3.96MB read 61709 requests in 10.03s, 9.65MB read Socket errors: connect 0, read 0, write 0, timeout 8Requests/sec: 2330.74 Requests/sec: 6150.18Transfer/sec: 405.20KB Transfer/sec: 0.96MB 可以看出 flask 在单个请求的耗时上明显胜于 aiohttp，但是标准差巨大，在压力场景下最大耗时长达 1.67s，甚至出现了 8 个超时的连接，而 aiohttp 的请求耗时比较稳定；最重要的区别在于，aiohttp 每秒完成了多达 6150.18 个请求，是 flask 的近 3 倍！flask 中 1% 超过 20.89ms 的请求严重影响了整体的性能。 点击计数 通过上面的 Hello, world 程序我们可以发现，使用 aiohttp 可以显著提升 Web 程序的性能。当然 Web 程序并不止于此，它还需要数据库、缓存、消息队列等等组件协同工作。asyncio 的周边虽然在迅速发展，不过仍不完善。好消息是 RabbitMQ 的 Python 驱动在下一个版本也加入了 asyncio 支持，基本组件大部分都支持了 asyncio。在这一节，我们增加 redis 支持，制作一个简单的点击计数器。 准备工作 在本节，我们需要安装 redis，并启动它： 12brew install redisbrew services start redis 准备好支持 asyncio 的 redis 驱动： 1pip install aioredis 定义 App 123from aiohttp import webapp = web.Application() 初始化 redis 在 aiohttp 中使用 redis 需要在应用启动前初始化连接池，并在应用退出后关闭连接池： 12345678import aioredisasync def setup_redis(app): redis_url = \"redis://@localhost/0\" app[\"redis\"] = await aioredis.create_redis_pool(redis_url) yield app[\"redis\"].close() await app[\"redis\"].wait_closed() 并将初始化函数注册到 app 的清理上下文： 1app.cleanup_ctx.append(setup_redis) 编写路由 12345678routes = web.RouteTableDef()@routes.get(\"/hit\")async def hello(request): redis = request.app[\"redis\"] return web.json_response(&#123;\"hit\": await redis.incr(\"hit\")&#125;)app.add_routes(routes) 这样，一个点击计数器就完成了。我们试着请求一下： 12345678910$ http localhost:5000/hitHTTP/1.1 200 OKContent-Length: 10Content-Type: application/json; charset=utf-8Date: Sat, 09 Jun 2018 08:37:33 GMTServer: Python/3.6 aiohttp/3.3.1&#123; \"hit\": 1&#125; 之后每次请求 hit 的值都会 +1。 对比 同样，我也编写了一个 flask 版本的点击计数器，在这里只展示一下对比结果： aiohttp 版本： 12345678910111213Running 10s test @ http://localhost:5000/hit 8 threads and 200 connections Thread Stats Avg Stdev Max +/- Stdev Latency 35.83ms 7.18ms 90.80ms 70.37% Req/Sec 699.60 86.70 0.92k 68.62% Latency Distribution 50% 35.01ms 75% 40.39ms 90% 44.61ms 99% 58.03ms 55816 requests in 10.04s, 9.10MB readRequests/sec: 5559.97Transfer/sec: 0.91MB flask 版本： 1234567891011121314Running 10s test @ http://localhost:5000/hit 8 threads and 200 connections Thread Stats Avg Stdev Max +/- Stdev Latency 123.99ms 171.06ms 1.89s 95.32% Req/Sec 262.03 163.41 670.00 69.57% Latency Distribution 50% 106.59ms 75% 156.67ms 90% 179.60ms 99% 1.05s 20484 requests in 10.07s, 3.33MB read Socket errors: connect 0, read 19, write 0, timeout 0Requests/sec: 2033.33Transfer/sec: 338.51KB 可以看出增加了 redis 的内存 io 操作后 aiohttp 的领先优势巨大，而 Web 应用大多是重 io 的，可以预见到 asyncio 在未来会占据更重要的地位。 结语 写完这篇，这个系列的文章就到此为止了。整体上 asyncio 仍在继续发展，有越来越多的基础组件已经有了可用的 asyncio 版本，一些大厂已经有转向 asyncio 的趋势了。asyncio 是未来，是一定要了解的~","categories":[],"tags":[]},{"title":"Redisson 的介绍与使用","slug":"2018-06-04/Redisson-的介绍与使用","date":"2018-06-04T18:04:44.000Z","updated":"2018-08-07T13:31:12.091Z","comments":true,"path":"posts/eae731fe/","link":"","permalink":"http://weafteam.github.io/posts/eae731fe/","excerpt":"","text":"一、简介 Redisson是一个在Redis的基础上实现的Java驻内存数据网格（In-Memory Data Grid）。它不仅提供了一系列的分布式的Java常用对象，还提供了许多分布式服务。其中包括(BitSet, Set, Multimap, SortedSet, Map, List, Queue, BlockingQueue, Deque, BlockingDeque, Semaphore, Lock, AtomicLong, CountDownLatch, Publish / Subscribe, Bloom filter, Remote service, Spring cache, Executor service, Live Object service, Scheduler service) Redisson提供了使用Redis的最简单和最便捷的方法。Redisson的宗旨是促进使用者对Redis的关注分离（Separation of Concern），从而让使用者能够将精力更集中地放在处理业务逻辑上。 二、分布式锁 今天也只是就Redisson分布式锁这一部分进行讲解。 在很多时候，一个服务器已经不满足我们对服务性能的要求了，所以我们引进和很多相关的技术，引入了分布式服务的方式，但是这样既带来了方便，其实也带来了问题，多台服务器一起运行相同的代码，一旦两台服务运行了相同的代码，那就要保证服务“幂等性”的设计。而且要考虑服务并发带来的可能性问题。 那么我们现在引入分布式锁的概念，顾名思义，就是使用锁，将我们要紧行操作的对象或者数据加锁。操作后释放锁。使得此时操作这个对象的服务器只有一个（或者说只能对当前对象进行当前拿到锁的操作，拿不到锁的都不允许操作）。这样保证数据的每次操作前后都不会有问题。当然同一个服务器也只能对当前对象进行一个操作。 其实为了保证这一点，其实还有很多锁，相信大家也知道，乐观锁，悲观锁、表锁、行锁等。 三、具体配置 此文是以SpringBoot为基础来实现的。 123456789101112131415161718192021222324252627282930313233343536package lock;import org.redisson.Redisson;import org.redisson.api.RedissonClient;import org.redisson.config.Config;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;/** * @Author ：yaxuSong * @Description: * @Date: 11:30 2018/6/30 * @Modified by: */@Configurationpublic class RedissionServiceConfig &#123; @Value(\"$&#123;spring.redis.host&#125;\") private String redisHost; @Value(\"$&#123;spring.redis.port&#125;\") private String redisPort; @Value(\"$&#123;spring.redis.password&#125;\") private String redisPassword; @Value(\"$&#123;spring.redis.database&#125;\") private Integer redisDatabase; @Bean(\"redisson\") public RedissonClient redissionConfig()&#123; Config config = new Config(); config.useSingleServer().setAddress(redisHost + \":\" + redisPort).setPassword(redisPassword) .setDatabase(redisDatabase); return Redisson.create(config); &#125;&#125; 四、实现与应用 实现部分 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package lock;import xxx.ExceptionCodeEnum;import lombok.extern.slf4j.Slf4j;import org.redisson.api.RLock;import org.redisson.api.RedissonClient;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Qualifier;import org.springframework.stereotype.Service;import java.util.concurrent.TimeUnit;/** * @Author ：yaxuSong * @Description: * @Date: 11:30 2018/6/30 * @Modified by: */@Slf4j@Service(\"redissonLockService\")public class RedissonLockServiceImpl implements LockService &#123; @Autowired @Qualifier(\"redisson\") private RedissonClient redisson; @Override public boolean tryLock(String key) &#123; return tryLock(key); &#125; @Override public boolean tryLock(String key, long timeout, TimeUnit unit) &#123; return tryLock(key, timeout, LockService.DEFAULT_EXPIRE_TIME, unit); &#125; @Override public boolean tryLock(String key, long waitTime, long leaseTime, TimeUnit unit) &#123; try &#123; RLock lock = redisson.getLock(key); boolean res = lock.tryLock(waitTime, leaseTime, unit); return res; &#125; catch (InterruptedException e) &#123; RedissonLockServiceImpl.log.error(\"获取redisson锁异常, key is &#123;&#125;, timeout is &#123;&#125;,leaseTime is &#123;&#125;, unit is &#123;&#125;.\", key, waitTime, leaseTime, unit, e); throw new LockException(ExceptionCodeEnum.CACHE_EXCEPTION_LOCK_FAIL); &#125; &#125; @Override public void lock(String key) &#123; RLock lock = redisson.getLock(key); lock.lock(); &#125; @Override public void unLock(String key) &#123; try &#123; RLock lock = redisson.getLock(key); lock.unlock(); &#125; catch (Exception e) &#123; RedissonLockServiceImpl.log.error(\"redisson解锁失败,key=&#123;&#125;\", key, e); &#125; &#125;&#125; 应用 123456789String lockKey = \"xxxx\"; if(!lockService.tryLock(lockKey))&#123; log.warn(\"current request can't not be locked!\"); throw new LockException(code,msg); &#125;//...lockService.unLock(lockKey); 五、相关问题 大家都知道，如果负责储存这个分布式锁的Redis节点宕机以后，而且这个锁正好处于锁住的状态时，这个锁会出现锁死的状态。为了避免这种情况的发生，Redisson内部提供了一个监控锁的看门狗，它的作用是在Redisson实例被关闭前，不断的延长锁的有效期。默认情况下，看门狗的检查锁的超时时间是30秒钟，也可以通过修改Config.lockWatchdogTimeout来另行指定。 参考文章 ：https://github.com/redisson/redisson/wiki/%E7%9B%AE%E5%BD%95","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://weafteam.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://weafteam.github.io/tags/JAVA/"}]},{"title":"chapter-11-AIR","slug":"2018-05-21/chapter-11-AIR","date":"2018-05-29T03:27:34.000Z","updated":"2018-06-23T11:04:29.831Z","comments":true,"path":"posts/45d29694/","link":"","permalink":"http://weafteam.github.io/posts/45d29694/","excerpt":"","text":"TensorFlow 实现Siamese Network 这次我们来实现一个基于lenet的Siamese Network，大家如果想了解Siamese Network，给大家推荐Andrew NG的课deeplearning.ai作为了解 接下来我们直接上code：大家也可以直接移步到我的github 仓库里面找代码代码 123456789代码目录组织结构：siameasenetwork __init__.py mnist： __init__.py train.py lenet.py util.py DataShuffler.py train.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2018/5/25 22:16# @Author : milittle# @Site : www.weaf.top# @File : train.py# @Software: PyCharmimport tensorflow as tfimport numpy as np from siameasenetwork.mnist import util # 这是外部依赖文件from siameasenetwork.mnist.DataShuffler import * #这是打乱数据文件from siameasenetwork.mnist.lenet import Lenet # 主网络结构SEED = 10def compute_euclidean_distance(x, y): \"\"\" Computes the euclidean distance between two tensorflow variables \"\"\" with tf.name_scope('euclidean_distance') as scope: #d = tf.square(tf.sub(x, y)) #d = tf.sqrt(tf.reduce_sum(d)) # What about the axis ??? d = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(x, y)), 1)) return ddef compute_contrastive_loss(left_feature, right_feature, label, margin, is_target_set_train=True): \"\"\" Compute the contrastive loss as in http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf L = 0.5 * (Y) * D^2 + 0.5 * (1-Y) * &#123;max(0, margin - D)&#125;^2 OR MAYBE THAT L = 0.5 * (1-Y) * D^2 + 0.5 * (Y) * &#123;max(0, margin - D)&#125;^2 **Parameters** left_feature: First element of the pair right_feature: Second element of the pair label: Label of the pair (0 or 1) margin: Contrastive margin **Returns** Return the loss operation \"\"\" with tf.name_scope(\"contrastive_loss\"): label = tf.to_float(label) one = tf.constant(1.0) d = compute_euclidean_distance(left_feature, right_feature) d_sqrt = tf.sqrt(d) loss = label * tf.square(tf.maximum(0., margin - d_sqrt)) + (1 - label) * d loss = 0.5 * tf.reduce_mean(loss) # #first_part = tf.mul(one - label, tf.square(d)) # (Y-1)*(d^2) # #first_part = tf.mul(label, tf.square(d)) # (Y-1)*(d^2) # between_class = tf.exp(tf.multiply(one-label, tf.square(d))) # (1-Y)*(d^2) # max_part = tf.square(tf.maximum(margin-d, 0)) # # within_class = tf.multiply(label, max_part) # (Y) * max((margin - d)^2, 0) # #second_part = tf.mul(one-label, max_part) # (Y) * max((margin - d)^2, 0) # # loss = 0.5 * tf.reduce_mean(within_class + between_class) # return loss, tf.reduce_mean(within_class), tf.reduce_mean(between_class) return lossdef compute_contrastive_loss_andre(left_feature, right_feature, label, margin, is_target_set_train=True): \"\"\" Compute the contrastive loss as in https://gitlab.idiap.ch/biometric/xfacereclib.cnn/blob/master/xfacereclib/cnn/scripts/experiment.py#L156 With Y = [-1 +1] --&gt; [POSITIVE_PAIR NEGATIVE_PAIR] L = log( m + exp( Y * d^2)) / N **Parameters** left_feature: First element of the pair right_feature: Second element of the pair label: Label of the pair (0 or 1) margin: Contrastive margin **Returns** Return the loss operation \"\"\" with tf.name_scope(\"contrastive_loss_andre\"): label = tf.to_float(label) d = compute_euclidean_distance(left_feature, right_feature) loss = tf.log(tf.exp(tf.multiply(label, d))) loss = tf.reduce_mean(loss) # Within class part genuine_factor = tf.multiply(label-1, 0.5) within_class = tf.reduce_mean(tf.log(tf.exp(tf.multiply(genuine_factor, d)))) # Between class part impostor_factor = tf.multiply(label + 1, 0.5) between_class = tf.reduce_mean(tf.log(tf.exp(tf.multiply(impostor_factor, d)))) # first_part = tf.mul(one - label, tf.square(d)) # (Y-1)*(d^2) return loss, between_class, within_classdef main(): BATCH_SIZE = 128 BATCH_SIZE_TEST = 128 #BATCH_SIZE_TEST = 300 ITERATIONS = 10000 VALIDATION_TEST = 100 perc_train = 0.9 CONTRASTIVE_MARGIN = 1 USE_GPU = True ANDRE_LOSS = False LOG_NAME = 'logs' train_data, train_labels, test_data, test_labels = util.load_mnist() data = np.concatenate((train_data, test_data), axis=0) label = np.concatenate((train_labels, test_labels), axis=0) data_shuffler = DataShuffler(train_data, train_labels, scale=True) # Creating the variables lenet_architecture = Lenet(seed=SEED, use_gpu=USE_GPU) # Siamease place holders - Training train_left_data = tf.placeholder(tf.float32, shape=(BATCH_SIZE*2, 28, 28, 1), name=\"left\") train_right_data = tf.placeholder(tf.float32, shape=(BATCH_SIZE*2, 28, 28, 1), name=\"right\") labels_data = tf.placeholder(tf.int32, shape=BATCH_SIZE*2) # Creating the graphs for training lenet_train_left = lenet_architecture.create_lenet(train_left_data) lenet_train_right = lenet_architecture.create_lenet(train_right_data) # Siamease place holders - Validation validation_data = tf.placeholder(tf.float32, shape=(data_shuffler.validation_data.shape[0], 28, 28, 1), name=\"validation\") labels_data_validation = tf.placeholder(tf.int32, shape=BATCH_SIZE_TEST) # Creating the graphs for validation lenet_validation = lenet_architecture.create_lenet(validation_data, train=False) if ANDRE_LOSS: loss, between_class, within_class = compute_contrastive_loss_andre(lenet_train_left, lenet_train_right, labels_data, CONTRASTIVE_MARGIN) else: # Regular contrastive loss loss = compute_contrastive_loss(lenet_train_left, lenet_train_right, labels_data, CONTRASTIVE_MARGIN) distances = compute_euclidean_distance(lenet_train_left, lenet_train_right) #regularizer = (tf.nn.l2_loss(lenet_architecture.W_fc1) + tf.nn.l2_loss(lenet_architecture.b_fc1) + # tf.nn.l2_loss(lenet_architecture.W_fc2) + tf.nn.l2_loss(lenet_architecture.b_fc2)) #loss += 5e-4 * regularizer # Defining training parameters batch = tf.Variable(0) learning_rate = tf.train.exponential_decay( 0.01, # Learning rate batch * BATCH_SIZE, data_shuffler.train_data.shape[0], 0.95 # Decay step ) #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=batch) optimizer = tf.train.AdamOptimizer(0.01, name='Adam').minimize(loss, global_step=batch) # pp = PdfPages(\"groups.pdf\") # Training with tf.Session() as session: #Trying to write things on tensor board train_writer = tf.summary.FileWriter(LOG_NAME + '/train', session.graph) test_writer = tf.summary.FileWriter(LOG_NAME + '/test', session.graph) tf.summary.scalar('loss', loss) # tf.summary.scalar('between_class', between_class) # tf.summary.scalar('within_class', within_class) tf.summary.scalar('lr', learning_rate) merged = tf.summary.merge_all() init = tf.global_variables_initializer() session.run(init) for step in range(ITERATIONS+1): batch_left, batch_right, labels = data_shuffler.get_pair(BATCH_SIZE, zero_one_labels=not ANDRE_LOSS) feed_dict = &#123;train_left_data: batch_left, train_right_data: batch_right, labels_data: labels&#125; _, l, lr, summary = session.run([optimizer, loss, learning_rate, merged], feed_dict=feed_dict) train_writer.add_summary(summary, step) print(\"Step &#123;0&#125;. Loss Validation = &#123;1&#125;\". format(step, l)) if step % VALIDATION_TEST == 0: # Siamese validation batch_left, batch_right, labels = data_shuffler.get_pair(n_pair=BATCH_SIZE_TEST, is_target_set_train=False, zero_one_labels=not ANDRE_LOSS) feed_dict = &#123;train_left_data: batch_left, train_right_data: batch_right, labels_data: labels&#125; d, lv, summary = session.run([distances, loss, merged], feed_dict=feed_dict) test_writer.add_summary(summary, step) ################################### # Siamese as a feature extractor ################################### batch_train_data, batch_train_labels = data_shuffler.get_batch( data_shuffler.validation_data.shape[0], train_dataset=True) features_train = session.run(lenet_validation, feed_dict=&#123;validation_data: batch_train_data[:]&#125;) batch_validation_data, batch_validation_labels = data_shuffler.get_batch( data_shuffler.validation_data.shape[0], train_dataset=False) features_validation = session.run(lenet_validation, feed_dict=&#123;validation_data: batch_validation_data[:]&#125;) acc = util.compute_accuracy(features_train, batch_train_labels, features_validation, batch_validation_labels, 10) print(\"Step &#123;0&#125;. Loss Validation = &#123;1&#125;, acc = &#123;2&#125;\". format(step, lv, acc)) # fig = util.plot_embedding_lda(features_validation, batch_validation_labels) # pp.savefig(fig) #if ANDRE_LOSS: # positive_scores = d[numpy.where(labels == -1)[0]].astype(\"float\") # negative_scores = d[numpy.where(labels == 1)[0]].astype(\"float\") #else: # positive_scores = d[numpy.where(labels == 1)[0]].astype(\"float\") # negative_scores = d[numpy.where(labels == 0)[0]].astype(\"float\") #threshold = bob.measure.eer_threshold(negative_scores, positive_scores) #far, frr = bob.measure.farfrr(negative_scores, positive_scores, threshold) #eer = ((far + frr) / 2.) * 100. print(\"End !!\") train_writer.close() test_writer.close() # pp.close()if __name__ == '__main__': main() lenet.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121#!/usr/bin/env python# vim: set fileencoding=utf-8 :# @author: Tiago de Freitas Pereira &lt;tiago.pereira@idiap.ch&gt;# @date: Wed 11 May 2016 09:39:36 CEST &quot;&quot;&quot;Class that creates the lenet architecture&quot;&quot;&quot;from siameasenetwork.mnist.util import *class Lenet(object): def __init__(self, conv1_kernel_size=5, conv1_output=16, conv2_kernel_size=5, conv2_output=32, fc1_output=400, n_classes=10, seed=10, use_gpu = False): &quot;&quot;&quot; Create all the necessary variables for this CNN **Parameters** conv1_kernel_size=5, conv1_output=32, conv2_kernel_size=5, conv2_output=64, fc1_output=400, n_classes=10 seed = 10 &quot;&quot;&quot; # First convolutional self.W_conv1 = create_weight_variables([conv1_kernel_size, conv1_kernel_size, 1, conv1_output], seed=seed, name=&quot;W_conv1&quot;, use_gpu=use_gpu) self.b_conv1 = create_bias_variables([conv1_output], name=&quot;bias_conv1&quot;, use_gpu=use_gpu) # Second convolutional self.W_conv2 = create_weight_variables([conv2_kernel_size, conv2_kernel_size, conv1_output, conv2_output], seed=seed, name=&quot;W_conv2&quot;, use_gpu=use_gpu) self.b_conv2 = create_bias_variables([conv2_output], name=&quot;bias_conv2&quot;, use_gpu=use_gpu) # First fc self.W_fc1 = create_weight_variables([(28 // 4) * (28 // 4) * conv2_output, fc1_output], seed=seed, name=&quot;W_fc1&quot;, use_gpu=use_gpu) self.b_fc1 = create_bias_variables([fc1_output], name=&quot;bias_fc1&quot;, use_gpu=use_gpu) # Second FC fc self.W_fc2 = create_weight_variables([fc1_output, n_classes], seed=seed, name=&quot;W_fc2&quot;, use_gpu=use_gpu) self.b_fc2 = create_bias_variables([n_classes], name=&quot;bias_fc2&quot;, use_gpu=use_gpu) self.seed = seed def create_lenet(self, data, train=True): &quot;&quot;&quot; Create the Lenet Architecture **Parameters** data: Input data train: **Returns features_back: Features for backpropagation features_val: Features for validation &quot;&quot;&quot; # Creating the architecture # First convolutional with tf.name_scope(&apos;conv_1&apos;) as scope: conv1 = create_conv2d(data, self.W_conv1) # relu1 = create_relu(conv1, self.b_conv1) # relu1 = create_sigmoid(conv1, self.b_conv1) with tf.name_scope(&apos;tanh_1&apos;) as scope: tanh_1 = create_tanh(conv1, self.b_conv1) # Pooling # pool1 = create_max_pool(relu1) # pool1 = create_max_pool(relu1) with tf.name_scope(&apos;pool_1&apos;) as scope: pool1 = create_max_pool(tanh_1) # Second convolutional with tf.name_scope(&apos;conv_2&apos;) as scope: conv2 = create_conv2d(pool1, self.W_conv2) # relu2 = create_relu(conv2, self.b_conv2) # relu2 = create_sigmoid(conv2, self.b_conv2) with tf.name_scope(&apos;tanh_2&apos;) as scope: # pool2 = create_max_pool(relu2) #tanh_2 = create_relu(conv2, self.b_conv2) # pool2 = create_max_pool(conv2) tanh_2 = create_tanh(conv2, self.b_conv2) # Pooling with tf.name_scope(&apos;pool_2&apos;) as scope: pool2 = create_max_pool(tanh_2) #if train: #pool2 = tf.nn.dropout(pool2, 0.5, seed=self.seed) # Reshaping all the convolved images to 2D to feed the FC layers # FC1 with tf.name_scope(&apos;fc_1&apos;) as scope: pool_shape = pool2.get_shape().as_list() reshape = tf.reshape(pool2, [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]]) #fc1 = tf.nn.relu(tf.matmul(reshape, self.W_fc1) + self.b_fc1) fc1 = tf.nn.tanh(tf.matmul(reshape, self.W_fc1) + self.b_fc1) #if train: #fc1 = tf.nn.dropout(fc1, 0.5, seed=self.seed) # FC2 with tf.name_scope(&apos;fc_2&apos;) as scope: fc2 = tf.matmul(fc1, self.W_fc2) + self.b_fc2 #fc2 = tf.nn.softmax(tf.matmul(fc1, self.W_fc2) + self.b_fc2) return fc2 util.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293#!/usr/bin/env python# vim: set fileencoding=utf-8 :# @author: Tiago de Freitas Pereira &lt;tiago.pereira@idiap.ch&gt;# @date: Wed 11 May 2016 09:39:36 CEST import numpyimport tensorflow as tfnumpy.random.seed(10)from tensorflow.examples.tutorials.mnist import input_datadef create_weight_variables(shape, seed, name, use_gpu=False): &quot;&quot;&quot; Create gaussian random neurons with mean 0 and std 0.1 **Paramters** shape: Shape of the layer &quot;&quot;&quot; #import ipdb; ipdb.set_trace() if len(shape) == 4: in_out = shape[0] * shape[1] * shape[2] + shape[3] else: in_out = shape[0] + shape[1] import math stddev = math.sqrt(3.0 / in_out) # XAVIER INITIALIZER (GAUSSIAN) initializer = tf.truncated_normal(shape, stddev=stddev, seed=seed) if use_gpu: with tf.device(&quot;/gpu&quot;): return tf.get_variable(name, initializer=initializer, dtype=tf.float32) else: with tf.device(&quot;/cpu&quot;): return tf.get_variable(name, initializer=initializer, dtype=tf.float32)def create_bias_variables(shape, name, use_gpu=False): &quot;&quot;&quot; Create the bias term &quot;&quot;&quot; initializer = tf.constant(0.1, shape=shape) if use_gpu: with tf.device(&quot;/gpu&quot;): return tf.get_variable(name, initializer=initializer, dtype=tf.float32) else: with tf.device(&quot;/cpu&quot;): return tf.get_variable(name, initializer=initializer, dtype=tf.float32) def create_conv2d(x, W): &quot;&quot;&quot; Create a convolutional kernel with 1 pixel of stride **Parameters** x: input layer W: Neurons &quot;&quot;&quot; return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&apos;SAME&apos;)def create_max_pool(x): &quot;&quot;&quot; Create max pooling using a patch of 2x2 **Parameters** x: input layer &quot;&quot;&quot; return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&apos;SAME&apos;)def create_tanh(x, bias): &quot;&quot;&quot; Create the Tanh activations **Parameters** x: input layer bias: bias term &quot;&quot;&quot; return tf.nn.tanh(tf.nn.bias_add(x, bias))def create_relu(x, bias): &quot;&quot;&quot; Create the ReLU activations **Parameters** x: input layer bias: bias term &quot;&quot;&quot; return tf.nn.relu(tf.nn.bias_add(x, bias))def create_sigmoid(x, bias): &quot;&quot;&quot; Create the Sigmoid activations **Parameters** x: input layer bias: bias term &quot;&quot;&quot; return tf.nn.sigmoid(tf.nn.bias_add(x, bias))def scale_mean_norm(data, scale=0.00390625): mean = numpy.mean(data) data = (data - mean) * scale return datadef evaluate_softmax(data, labels, session, network, data_node): &quot;&quot;&quot; Evaluate the network assuming that the output layer is a softmax &quot;&quot;&quot; predictions = numpy.argmax(session.run( network, feed_dict=&#123;data_node: data[:]&#125;), 1) return 100. * numpy.sum(predictions == labels) / predictions.shape[0]def load_mnist(): mnist_data = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True) train_data = mnist_data.train.images test_data = mnist_data.test.images train_labels = mnist_data.train.labels test_labels = mnist_data.test.labels return train_data, train_labels, test_data, test_labelsdef plot_embedding_pca(features, labels): &quot;&quot;&quot; Trains a PCA using bob, reducing the features to dimension 2 and plot it the possible clusters :param features: :param labels: :return: &quot;&quot;&quot; import bob.learn.linear import matplotlib.pyplot as mpl colors = [&apos;#FF0000&apos;, &apos;#FFFF00&apos;, &apos;#FF00FF&apos;, &apos;#00FFFF&apos;, &apos;#000000&apos;, &apos;#AA0000&apos;, &apos;#AAAA00&apos;, &apos;#AA00AA&apos;, &apos;#00AAAA&apos;, &apos;#330000&apos;] # Training PCA trainer = bob.learn.linear.PCATrainer() machine, lamb = trainer.train(features.astype(&quot;float64&quot;)) # Getting the first two most relevant features projected_features = machine(features.astype(&quot;float64&quot;))[:, 0:2] # Plotting the classes n_classes = max(labels)+1 fig = mpl.figure() for i in range(n_classes): indexes = numpy.where(labels == i)[0] selected_features = projected_features[indexes,:] mpl.scatter(selected_features[:, 0], selected_features[:, 1], marker=&apos;.&apos;, c=colors[i], linewidths=0, label=str(i)) mpl.legend() return figdef plot_embedding_lda(features, labels): &quot;&quot;&quot; Trains a LDA using bob, reducing the features to dimension 2 and plot it the possible clusters :param features: :param labels: :return: &quot;&quot;&quot; import bob.learn.linear import matplotlib.pyplot as mpl colors = [&apos;#FF0000&apos;, &apos;#FFFF00&apos;, &apos;#FF00FF&apos;, &apos;#00FFFF&apos;, &apos;#000000&apos;, &apos;#AA0000&apos;, &apos;#AAAA00&apos;, &apos;#AA00AA&apos;, &apos;#00AAAA&apos;, &apos;#330000&apos;] n_classes = max(labels)+1 # Training PCA trainer = bob.learn.linear.FisherLDATrainer(use_pinv=True) lda_features = [] for i in range(n_classes): indexes = numpy.where(labels == i)[0] lda_features.append(features[indexes, :].astype(&quot;float64&quot;)) machine, lamb = trainer.train(lda_features) #import ipdb; ipdb.set_trace(); # Getting the first two most relevant features projected_features = machine(features.astype(&quot;float64&quot;))[:, 0:2] # Plotting the classes fig = mpl.figure() for i in range(n_classes): indexes = numpy.where(labels == i)[0] selected_features = projected_features[indexes,:] mpl.scatter(selected_features[:, 0], selected_features[:, 1], marker=&apos;.&apos;, c=colors[i], linewidths=0, label=str(i)) mpl.legend() return figdef compute_eer(data_train, labels_train, data_validation, labels_validation, n_classes): from scipy.spatial.distance import cosine # Creating client models models = [] for i in range(n_classes): labels_num = numpy.argmax(labels_train, axis=1) indexes = numpy.where(labels_num == i) models.append(numpy.mean(data_train[indexes, :], axis=0)) # Probing positive_scores = numpy.zeros(shape=0) negative_scores = numpy.zeros(shape=0) for i in range(n_classes): # Positive scoring val_num = numpy.argmax(labels_validation) indexes = numpy.where(val_num == i) positive_data = data_validation[indexes, :] p = [cosine(models[i], positive_data[j]) for j in range(positive_data.shape[0])] positive_scores = numpy.hstack((positive_scores, p)) # negative scoring indexes = numpy.where(val_num != i) negative_data = data_validation[indexes, :] n = [cosine(models[i], negative_data[j]) for j in range(negative_data.shape[0])] negative_scores = numpy.hstack((negative_scores, n)) # Computing performance based on EER negative_scores = (-1) * negative_scores positive_scores = (-1) * positive_scores # threshold = bob.measure.eer_threshold(negative_scores, positive_scores) # far, frr = bob.measure.farfrr(negative_scores, positive_scores, threshold) # eer = (far + frr) / 2. return negative_scores, positive_scoresdef compute_accuracy(data_train, labels_train, data_validation, labels_validation, n_classes): from scipy.spatial.distance import cosine # Creating client models models = [] for i in range(n_classes): index = numpy.argmax(labels_train, axis=1) indexes = numpy.where(index == i)[0] data_t = [data_train[i] for i in indexes] models.append(numpy.mean(data_t, axis=0)) # Probing tp = 0 for i in range(data_validation.shape[0]): d = data_validation[i,:] l = labels_validation[i] scores = [cosine(m, d) for m in models] predict = numpy.argmin(scores) ll = numpy.argmax(l, axis=0) if predict == ll: tp += 1 return (float(tp) / data_validation.shape[0]) * 100def compute_acc(data_val, labels_val): tp = 0 data_size = data_val.shape[0] for i in range(data_size): index = numpy.argmax(data_val[i]) if index == numpy.argmax(labels_val[i]): tp += 1 return (float(tp) / data_size) DataShuffler.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192#!/usr/bin/env python# vim: set fileencoding=utf-8 :# @author: Tiago de Freitas Pereira &lt;tiago.pereira@idiap.ch&gt;# @date: Wed 11 May 2016 09:39:36 CEST import numpydef scale_mean_norm(data, scale=0.00390625): mean = numpy.mean(data) data = (data - mean) * scale return data, mean&quot;&quot;&quot;Data&quot;&quot;&quot;class DataShuffler(object): def __init__(self, data, labels, perc_train=0.9, scale=True): &quot;&quot;&quot; Some base functions for neural networks **Parameters** data: &quot;&quot;&quot; scale_value = 0.00390625 total_samples = data.shape[0] #总共的样本数量 indexes = numpy.array(range(total_samples)) numpy.random.shuffle(indexes) # Spliting train and validation train_samples = int(round(total_samples * perc_train)) #训练样本数目 validation_samples = total_samples - train_samples #验证集样本数目 data = numpy.reshape(data, (data.shape[0], 28, 28, 1)) #所有输入样本 self.train_data = data[indexes[0:train_samples], :, :, :] # 训练样本 self.train_labels = labels[indexes[0:train_samples]] # 训练labels self.validation_data = data[indexes[train_samples:train_samples + validation_samples], :, :, :] self.validation_labels = labels[indexes[train_samples:train_samples + validation_samples]] self.total_labels = 10 if scale: # data = scale_minmax_norm(data,lower_bound = -1, upper_bound = 1) self.train_data, self.mean = scale_mean_norm(self.train_data) self.validation_data = (self.validation_data - self.mean) * scale_value def get_batch(self, n_samples, train_dataset=True): if train_dataset: data = self.train_data label = self.train_labels else: data = self.validation_data label = self.validation_labels # Shuffling samples indexes = numpy.array(range(data.shape[0])) numpy.random.shuffle(indexes) selected_data = data[indexes[0:n_samples], :, :, :] selected_labels = label[indexes[0:n_samples]] return selected_data.astype(&quot;float32&quot;), selected_labels def get_pair(self, n_pair=1, is_target_set_train=True, zero_one_labels=True): &quot;&quot;&quot; Get a random pair of samples **Parameters** is_target_set_train: Defining the target set to get the batch **Return** &quot;&quot;&quot; def get_genuine_or_not(input_data, input_labels, genuine=True): if genuine: # TODO: THIS KEY SELECTION NEEDS TO BE MORE EFFICIENT # Getting a client index = numpy.random.randint(self.total_labels) # Getting the indexes of the data from a particular client arg_max = numpy.argmax(input_labels, axis=1) indexes = numpy.where(arg_max == index)[0] numpy.random.shuffle(indexes) # Picking a pair data = input_data[indexes[0], :, :, :] data_p = input_data[indexes[1], :, :, :] else: # Picking a pair from different clients index = numpy.random.choice(self.total_labels, 2, replace=False) # Getting the indexes of the two clients arg_max = numpy.argmax(input_labels, axis=1) indexes = numpy.where(arg_max == index[0])[0] indexes_p = numpy.where(arg_max == index[1])[0] numpy.random.shuffle(indexes) numpy.random.shuffle(indexes_p) # Picking a pair data = input_data[indexes[0], :, :, :] data_p = input_data[indexes_p[0], :, :, :] return data, data_p if is_target_set_train: target_data = self.train_data target_labels = self.train_labels else: target_data = self.validation_data target_labels = self.validation_labels total_data = n_pair * 2 c = target_data.shape[3] w = target_data.shape[1] h = target_data.shape[2] data = numpy.zeros(shape=(total_data, w, h, c), dtype=&apos;float32&apos;) data_p = numpy.zeros(shape=(total_data, w, h, c), dtype=&apos;float32&apos;) labels_siamese = numpy.zeros(shape=total_data, dtype=&apos;float32&apos;) genuine = True for i in range(total_data): data[i, :, :, :], data_p[i, :, :, :] = get_genuine_or_not(target_data, target_labels, genuine=genuine) if zero_one_labels: labels_siamese[i] = not genuine else: labels_siamese[i] = -1 if genuine else +1 genuine = not genuine return data, data_p, labels_siamese def get_triplet(self, n_labels, n_triplets=1, is_target_set_train=True): &quot;&quot;&quot; Get a triplet **Parameters** is_target_set_train: Defining the target set to get the batch **Return** &quot;&quot;&quot; def get_one_triplet(input_data, input_labels): # Getting a pair of clients index = numpy.random.choice(n_labels, 2, replace=False) label_positive = index[0] label_negative = index[1] # Getting the indexes of the data from a particular client indexes = numpy.where(input_labels == index[0])[0] numpy.random.shuffle(indexes) # Picking a positive pair data_anchor = input_data[indexes[0], :, :, :] data_positive = input_data[indexes[1], :, :, :] # Picking a negative sample indexes = numpy.where(input_labels == index[1])[0] numpy.random.shuffle(indexes) data_negative = input_data[indexes[0], :, :, :] return data_anchor, data_positive, data_negative, label_positive, label_positive, label_negative if is_target_set_train: target_data = self.train_data target_labels = self.train_labels else: target_data = self.validation_data target_labels = self.validation_labels c = target_data.shape[3] w = target_data.shape[1] h = target_data.shape[2] data_a = numpy.zeros(shape=(n_triplets, w, h, c), dtype=&apos;float32&apos;) data_p = numpy.zeros(shape=(n_triplets, w, h, c), dtype=&apos;float32&apos;) data_n = numpy.zeros(shape=(n_triplets, w, h, c), dtype=&apos;float32&apos;) labels_a = numpy.zeros(shape=n_triplets, dtype=&apos;float32&apos;) labels_p = numpy.zeros(shape=n_triplets, dtype=&apos;float32&apos;) labels_n = numpy.zeros(shape=n_triplets, dtype=&apos;float32&apos;) for i in range(n_triplets): data_a[i, :, :, :], data_p[i, :, :, :], data_n[i, :, :, :], \\ labels_a[i], labels_p[i], labels_n[i] = \\ get_one_triplet(target_data, target_labels) return data_a, data_p, data_n, labels_a, labels_p, labels_n","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]},{"title":"asyncio 不完全指北（六）","slug":"2018-05-21/guide-to-asyncio-6","date":"2018-05-27T10:39:08.000Z","updated":"2018-08-07T13:19:42.442Z","comments":true,"path":"posts/74232ae7/","link":"","permalink":"http://weafteam.github.io/posts/74232ae7/","excerpt":"","text":"前言 前五篇文章介绍了 asyncio 的 API，从这篇开始，就要讲一些 Real World（并不）的东西了。 使用 aiohttp 作为 HTTP 客户端 aiohttp 是一个基于 asyncio 的异步 HTTP 客户端和服务器库，也是 asyncio 生态中发展最迅速的第三方库之一。在这一节，我们使用 aiohttp 作为 HTTP 客户端来比较一下同步、基于线程的异步和基于 asyncio 的异步的差别。 准备工作 首先我们安装好所需的第三方库： 12pip install requestspip install aiohttp 准备一些用于并发请求的 url，共 45 个： 12345678910111213141516171819202122# url.pyurls = [ 'http://caipiao.hao123.com/', 'http://game.hao123.com/', 'http://mail.10086.cn/', 'http://mail.126.com/', 'http://mail.163.com/', 'http://mail.aliyun.com/', 'http://mail.qq.com/', 'http://mail.sina.com.cn/', 'http://music.163.com/', 'http://tuijian.hao123.com/', 'http://www.12306.cn/', 'http://www.163.com/', 'http://www.37.com/', 'http://www.4399.com/', 'http://www.abchina.com/', 'http://www.baidu.com/', 'http://www.bankcomm.com/', 'http://www.boc.cn/', 'http://www.ccb.com/', 'http://www.chsi.com.cn/', 'http://www.cmbchina.com/', 'http://www.cnki.net/', 'http://www.eastmoney.com/', 'http://www.fang.com/', 'http://www.icbc.com.cn/icbc/', 'http://www.ifeng.com', 'http://www.iqiyi.com/', 'http://www.psbc.com/', 'http://www.qq.com/', 'http://www.sina.com.cn/', 'http://www.sohu.com/', 'http://www.tianya.cn/', 'http://www.zhihu.com/', 'http://wyyx.hao123.com/', 'https://mail.qq.com/', 'https://mail.sohu.com/', 'https://tieba.baidu.com/', 'https://weibo.com/', 'https://www.autohome.com.cn/', 'https://www.bilibili.com/', 'https://www.booking.com/', 'https://www.douyu.com/', 'https://www.qunar.com/', 'https://www.suning.com/', 'https://www.taobao.com/'] 同步的请求 首先导入所需的库： 123import timeimport requestsfrom url import urls 完成请求单个 url 的函数，这个函数会以 bytes 形式返回网站内容： 123def fetch(session, url): resp = session.get(url) return resp.content 同步请求所有的 url，打印出字节的长度： 12345def main(): session = requests.Session() for url in urls: data = fetch(session, url) print(f'&#123;url&#125;: &#123;len(data)&#125;') 记录完成请求所需的时间： 1234if __name__ == '__main__': start = time.time() main() print(time.time() - start) 上述代码的结果： 123456789101112131415161718http://caipiao.hao123.com/: 109299http://game.hao123.com/: 179707http://mail.10086.cn/: 52500http://mail.126.com/: 13063http://mail.163.com/: 137118http://mail.aliyun.com/: 725http://mail.qq.com/: 8206http://mail.sina.com.cn/: 2837http://music.163.com/: 92606...https://www.autohome.com.cn/: 656200https://www.bilibili.com/: 26642https://www.booking.com/: 457842https://www.douyu.com/: 75286https://www.qunar.com/: 140898https://www.suning.com/: 188523https://www.taobao.com/: 12628324.28531312942505 可以看出打印的顺序是和 url 列表的顺序完全一致的，同步的代码耗时约 24s。 基于线程的请求 首先导入所需的库： 1234import timefrom concurrent.futures import ThreadPoolExecutor, as_completedimport requestsfrom url import urls 完成单个请求的函数： 123def fetch(session, url): resp = session.get(url) return resp.content 使用线程池请求所有的 url： 1234567def main(): session = requests.Session() with ThreadPoolExecutor() as executor: tasks = &#123;executor.submit(fetch, session, url): url for url in urls&#125; for task in as_completed(tasks.keys()): data = task.result() print(f'&#123;tasks[task]&#125;: &#123;len(data)&#125;') 记录完成所需的时间： 1234if __name__ == '__main__': start = time.time() main() print(time.time() - start) 上述代码的结果： 123456789101112131415161718http://www.ccb.com/: 276http://www.12306.cn/: 1480http://www.cnki.net/: 59235http://mail.aliyun.com/: 725http://www.tianya.cn/: 7867http://www.icbc.com.cn/icbc/: 157227http://www.bankcomm.com/: 3473http://www.chsi.com.cn/: 34188http://mail.sina.com.cn/: 2837...http://www.sina.com.cn/: 584540https://tieba.baidu.com/: 137714https://www.autohome.com.cn/: 656154https://www.taobao.com/: 126283https://www.booking.com/: 457849http://www.iqiyi.com/: 599940http://tuijian.hao123.com/: 5114659.722297191619873 可以看到返回结果的顺序并不和 url 列表一致，准确的说，是按照请求完成的顺序排列的。同时，请求所需的时间大幅缩短，降到了约 9s。 基于 asyncio 的请求 首先导入所需的库： 1234import asyncioimport timeimport aiohttpfrom url import urls 完成单个请求的函数： 123async def fetch(session, url): async with session.get(url) as resp: return url, await resp.read() 这里同时返回了请求的 url 和网站内容，是因为后面的代码不容易在请求完成后获得请求的 url。 使用 aiohttp 请求所有的 url： 123456async def main(): async with aiohttp.ClientSession() as session: tasks = [fetch(session, url) for url in urls] for task in asyncio.as_completed(tasks): url, data = await task print(f'&#123;url&#125;: &#123;len(data)&#125;') 开启事件循环，并记录所需的时间： 12345if __name__ == '__main__': start = time.time() loop = asyncio.get_event_loop() loop.run_until_complete(main()) print(time.time() - start) 上述代码的结果： 1234567891011121314151617http://www.psbc.com/: 404http://www.eastmoney.com/: 392883http://www.icbc.com.cn/icbc/: 157227http://www.ifeng.com: 438464http://mail.aliyun.com/: 725http://www.tianya.cn/: 7867...http://tuijian.hao123.com/: 510052http://mail.qq.com/: 8023http://game.hao123.com/: 179707https://tieba.baidu.com/: 137723http://www.iqiyi.com/: 599830https://www.taobao.com/: 126283https://weibo.com/: 6117http://www.zhihu.com/: 22696https://www.booking.com/: 4578512.0516560077667236 和使用线程一样，返回结果是按照请求完成顺序排列的。请求的时间比线程更短，只用了约 2s 就完成了所有的请求。和使用线程的方式相比，asyncio 避免了创建线程的开销。 保存请求的结果 需要注意的是，上述请求只是简单的获取了内容，这些 bytes 只在内存中存在。一旦我们需要把结果保存到磁盘，就会有另一个会导致异步代码退化到同步的地方：磁盘 I / O。 现在我们增加一个保存请求内容到磁盘的函数： 12345from urllib.parse import quote_plusdef save_to_file(filename, data): with open(f'async_data/&#123;quote_plus(filename)&#125;.html', 'wb') as f: f.write(data) 同时增加一个函数，用来同时发起请求并把结果保存到文件： 1234async def fetch_and_save(session, url): url, data = await fetch(session, url) save_to_file(url, data * 500) # 把文件大小扩大 500 倍，使结果更明显 return url 同时更新一下 main() 函数： 123456async def main(): async with aiohttp.ClientSession() as session: tasks = [fetch_and_save(session, url) for url in urls] for task in asyncio.as_completed(tasks): url = await task print(f'save: &#123;url&#125;') 上述代码的结果： 123456789101112131415161718save: http://www.psbc.com/save: http://www.cnki.net/save: http://www.eastmoney.com/save: http://tuijian.hao123.com/save: http://caipiao.hao123.com/save: http://www.ifeng.comsave: http://www.fang.com/save: http://www.qq.com/...save: https://www.bilibili.com/save: https://www.autohome.com.cn/save: https://www.booking.com/save: https://mail.sohu.com/save: https://weibo.com/save: http://mail.qq.com/save: http://mail.10086.cn/save: http://www.zhihu.com/10.63579511642456 可以看到消耗的时间增加到了约 10s。 有没有什么方法可以将同步的文件系统操作变为异步的呢？答案就是结合使用线程和 asyncio。修改一下 fetch_and_save() 函数，使其在其他线程中执行保存操作： 12345async def fetch_and_save(session, url): url, data = await fetch(session, url) loop = asyncio.get_event_loop() loop.run_in_executor(None, save_to_file, url, data * 500) # 默认使用 ThreadPoolExecutor return url 修改后的结果： 12345678910111213141516171819save: http://www.psbc.com/save: http://www.12306.cn/save: http://www.ccb.com/save: http://www.cnki.net/save: http://www.sohu.com/save: http://www.37.com/save: http://www.icbc.com.cn/icbc/save: http://mail.sina.com.cn/save: http://www.ifeng.com...save: https://mail.qq.com/save: http://mail.163.com/save: https://mail.sohu.com/save: https://weibo.com/save: http://mail.qq.com/save: http://mail.10086.cn/save: http://www.zhihu.com/save: https://www.booking.com/4.817075967788696 效果很明显，所需的时间缩短到了约 5s。 NOTE：需要注意的是，大多数操作系统上并未提供文件系统的异步 I / O 操作（Linux kernel 提供了文件系统异步 I / O，不过它需要一个额外的库 aio），大部分的异步框架都是使用线程处理文件系统 I / O 的。如果需要统一的 API，可以选择 aiofiles。","categories":[],"tags":[]},{"title":"Lombok的使用","slug":"2018-05-27/Lombok使用","date":"2018-05-26T18:04:44.000Z","updated":"2018-08-07T13:19:52.785Z","comments":true,"path":"posts/3859dc89/","link":"","permalink":"http://weafteam.github.io/posts/3859dc89/","excerpt":"","text":"一、简介 使用lombok减少代码冗余（Reducing Boilerplate Code with Project Lombok）。“Boilerplate”是用来描述许多部分重复的代码的术语。这也是Java语言最常提出批评之一,是大多数项目中都存在此类代码而且数量很多。这个问题经常是各种库中设计决策的结果，但是由于语言本身的限制而导致的。Lombok 旨在通过一组简单的注释取代这些问题。 虽然注释用于指示用法，实现绑定甚至生成框架使用的代码并不罕见，但它们通常不用于生成应用程序直接使用的代码。 部分原因是这样做需要在开发时急切地处理注释。 Lombok正是这样做的。 通过集成到IDE中，Project Lombok能够注入开发人员可立即使用的代码。 例如，简单地将@Data注释添加到数据类（如下所示）会在IDE中生成许多新方法： 二、安装 ======= 1.eclipse可以使用以下方法安装 ————– ### 1) 下载lombok.jar包https://projectlombok.org/download.html 2) 运行Lombok.jar: Java -jar D:.jar D:.jar这是windows下lombok.jar所在的位置 数秒后将弹出一框，以确认eclipse的安装路径 3) 确认完eclipse的安装路径后，点击install/update按钮，即可安装完成 4) 安装完成之后，请确认eclipse安装路径下是否多了一个lombok.jar包，并且其 配置文件eclipse.ini中是否 添加了如下内容: -javaagent:lombok.jar -Xbootclasspath/a:lombok.jar 如果上面的答案均为true，那么恭喜你已经安装成功，否则将缺少的部分添加到相应的位置即可 5) 重启eclipse或myeclipse 2.IntelliJ IDEA安装 ——— ### 1）按照以下步骤打开设置Settings安装 ### 2）安装后重启 3.最后需要在项目中引入jar包或者，使用maven配置好坐标。 12345678910111213&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;0.9.2&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;projectlombok.org&lt;/id&gt; &lt;url&gt;http://projectlombok.org/mavenrepo&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt; 三、Lombok的注解有哪些 @Getter @Setter @Data @NonNull @ToString @EqualsAndHashCode @Cleanup @Synchronized @SneakyThrows @AllArgsConstructor @Builder @Generated @NoArgsConstructor @RequiredArgsConstructor @Singular @val @var @Value @FieldNameConstants @Log, @Log4j, @Log4j2, @Slf4j, @XSlf4j, @CommonsLog, @JBossLog, @Flogger @Delegate @Wither 四、Lombok注解的使用 1 @Getter 生成相应的get方法,其中boolean类型会生成 isFoo(); 并且可以配合AccessLevel使用 2 @Setter 生成相应的set方法,其中boolean类型会生产 setFoo(); 3 @Data 生成get、set、toString、equals、hashCode等方法。 4 @NonNull 可以再方法引用、和传参时如果值为null,抛出空指针异常。 5 @ToString 生成toString方法 6 @EqualsAndHashCode 生成equals和hashCode方法 7 @Cleanup 参考地址：http://jnb.ociweb.com/jnb/jnbJan2010.html","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://weafteam.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://weafteam.github.io/tags/JAVA/"}]},{"title":"Spring中@transactional的使用","slug":"2018-05-21/Spring中@transactional的使用","date":"2018-05-20T18:04:44.000Z","updated":"2018-07-17T08:31:55.168Z","comments":true,"path":"posts/435ecce8/","link":"","permalink":"http://weafteam.github.io/posts/435ecce8/","excerpt":"","text":"一、简介 我们在工作中多多少少会使用到和事务相关的，但是程序运行总不会一定不出现任何异常。一旦整个流程中出现一个异常，也许我们就需要整个操作不继续进行下去，并且需要之前做过的操作都不起作用。那么我们就需要使用事务来控制。 在Spring中，我们通过@transactional来启用事务。 二、@transactional详解 @Transactional注解中常用参数说明 参数名称 描述 rollbackFor 该属性用于设置需要进行回滚的异常类数组，当方法中抛出指定异常数组中的异常时，则进行事务回滚。例如：指定单一异常类：@Transactional(rollbackFor=RuntimeException.class)指定多个异常类：@Transactional(rollbackFor={RuntimeException.class, Exception.class}) rollbackForClassName 该属性用于设置需要进行回滚的异常类名称数组，当方法中抛出指定异常名称数组中的异常时，则进行事务回滚。例如：指定单一异常类名称：@Transactional(rollbackForClassName=“RuntimeException”)指定多个异常类名称：@Transactional(rollbackForClassName={“RuntimeException”,“Exception”}) noRollbackFor 该属性用于设置不需要进行回滚的异常类数组，当方法中抛出指定异常数组中的异常时，不进行事务回滚。例如：指定单一异常类：@Transactional(noRollbackFor=RuntimeException.class)指定多个异常类：@Transactional(noRollbackFor={RuntimeException.class, Exception.class}) noRollbackForClassName 该属性用于设置不需要进行回滚的异常类名称数组，当方法中抛出指定异常名称数组中的异常时，不进行事务回滚。例如：指定单一异常类名称：@Transactional(noRollbackForClassName=“RuntimeException”)指定多个异常类名称：@Transactional(noRollbackForClassName={“RuntimeException”,“Exception”}) propagation 该属性用于设置事务的传播行为，具体取值可参考表6-7。例如：@Transactional(propagation=Propagation.NOT_SUPPORTED,readOnly=true) isolation 该属性用于设置底层数据库的事务隔离级别，事务隔离级别用于处理多事务并发的情况，通常使用数据库的默认隔离级别即可，基本不需要进行设置 timeout 该属性用于设置事务的超时秒数，默认值为-1表示永不超时 readOnly 该属性用于设置当前事务是否为只读事务，设置为true表示只读，false则表示可读写，默认值为false。例如：@Transactional(readOnly=true) 事物传播行为介绍: @Transactional(propagation=Propagation.REQUIRED) ：如果有事务, 那么加入事务, 没有的话新建一个(默认情况下) @Transactional(propagation=Propagation.NOT_SUPPORTED) ：容器不为这个方法开启事务 @Transactional(propagation=Propagation.REQUIRES_NEW) ：不管是否存在事务,都创建一个新的事务,原来的挂起,新的执行完毕,继续执行老的事务 @Transactional(propagation=Propagation.MANDATORY) ：必须在一个已有的事务中执行,否则抛出异常 @Transactional(propagation=Propagation.NEVER) ：必须在一个没有的事务中执行,否则抛出异常(与Propagation.MANDATORY相反) @Transactional(propagation=Propagation.SUPPORTS) ：如果其他bean调用这个方法,在其他bean中声明事务,那就用事务.如果其他bean没有声明事务,那就不用事务. 事物超时设置: @Transactional(timeout=30) //默认是30秒 事务隔离级别: @Transactional(isolation = Isolation.READ_UNCOMMITTED)：读取未提交数据(会出现脏读, 不可重复读) 基本不使用 @Transactional(isolation = Isolation.READ_COMMITTED)：读取已提交数据(会出现不可重复读和幻读) @Transactional(isolation = Isolation.REPEATABLE_READ)：可重复读(会出现幻读) @Transactional(isolation = Isolation.SERIALIZABLE)：串行化 MYSQL: 默认为REPEATABLE_READ级别 SQLSERVER: 默认为READ_COMMITTED 脏读 : 一个事务读取到另一事务未提交的更新数据 不可重复读 : 在同一事务中, 多次读取同一数据返回的结果有所不同, 换句话说, 后续读取可以读到另一事务已提交的更新数据. 相反, “可重复读”在同一事务中多次 读取数据时, 能够保证所读数据一样, 也就是后续读取不能读到另一事务已提交的更新数据 幻读 : 一个事务读到另一个事务已提交的insert数据 三、特别需要注意的几点 spring 事务管理器,由spring来负责数据库的打开,提交,回滚.默认遇到运行期例外(throw new RuntimeException(“注释”);)会回滚，即遇到不受检查（unchecked）的例外时回滚；而遇到需要捕获的例外(throw new Exception(“注释”);)不会回滚,即遇到受检查的例外（就是非运行时抛出的异常，编译器会检查到的异常叫受检查例外或说受检查异常）时，需我们指定方式来让事务回滚要想所有异常都回滚,要加上 @Transactional( rollbackFor={Exception.class,其它异常}) .如果让unchecked例外不回滚 @Transactional 注解应该只被应用到 public 可见度的方法上。 如果你在 protected、private 或者 package-visible 的方法上使用 @Transactional 注解，它也不会报错， 但是这个被注解的方法将不会展示已配置的事务设置。 @Transactional 注解可以被应用于接口定义和接口方法、类定义和类的 public 方法上。然而，请注意仅仅 @Transactional 注解的出现不足于开启事务行为，它仅仅 是一种元数据，能够被可以识别 @Transactional 注解和上述的配置适当的具有事务行为的beans所使用。上面的例子中，其实正是 元素的出现 开启 了事务行为。 Spring团队的建议是你在具体的类（或类的方法）上使用 @Transactional 注解，而不要使用在类所要实现的任何接口上。你当然可以在接口上使用 @Transactional 注解，但是这将只能当你设置了基于接口的代理时它才生效。因为注解是不能继承的，这就意味着如果你正在使用基于类的代理时，那么事务的设置将不能被基于类的代理所识别，而且对象也将不会被事务代理所包装（将被确认为严重的）。因此，请接受Spring团队的建议并且在具体的类上使用 @Transactional 注解。 如果异常被try｛｝catch｛｝了，事务就不回滚了，如果想让事务回滚必须再往外抛try｛｝catch｛throw Exception｝。 使用了@Transactional的方法，对同一个类里面的方法调用， @Transactional无效。比如有一个类Test，它的一个方法A，A再调用Test本类的方法B（不管B是否public还是private），但A没有声明注解事务，而B有。则外部调用A之后，B的事务是不会起作用的。（经常在这里出错） 放在方法上的注解会覆盖类上边的注解的参数属性。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://weafteam.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://weafteam.github.io/tags/JAVA/"}]},{"title":"chapter-10-AIR","slug":"2018-05-14/chapter-10-AIR","date":"2018-05-20T11:52:00.000Z","updated":"2018-06-23T11:04:29.829Z","comments":true,"path":"posts/78b2bf24/","link":"","permalink":"http://weafteam.github.io/posts/78b2bf24/","excerpt":"","text":"TensorFlow SVM 简单的介绍一下Support Vector Machine，SVM是一个二分类机器学习算法，目的就是为了让两类之间的margin更大，SVM还可以实现多分类任务，那么需要扩展非线性kernels进去，接下来逐渐介绍SVM 和线性回归一起工作的SVM,使用得数据是IRIS数据集，取两个特征维度。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126importimport matplotlib.pyplotmatplot as pltimport numpy as npimport tensorflow as tffrom sklearn import datasetsfrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()iris = datasets.load_iris()x_vals = np.array([[x[0], x[3]] for x in iris.data])y_vals = np.array([1 if y == 0 else -1 for y in iris.target])train_indices = np.random.choice(len(x_vals), round(len(x_vals)*0.8), replace=False)test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))x_vals_train = x_vals[train_indices]x_vals_test = x_vals[test_indices]y_vals_train = y_vals[train_indices]y_vals_test = y_vals[test_indices]batch_size = 100x_data = tf.placeholder(shape=[None, 2], dtype=tf.float32)y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)A = tf.Variable(tf.random_normal(shape=[2, 1]))b = tf.Variable(tf.random_normal(shape=[1, 1]))model_output = tf.subtract(tf.matmul(x_data, A), b)l2_norm = tf.reduce_sum(tf.square(A))prediction = tf.sign(model_output)accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, y_target), tf.float32))my_opt = tf.train.GradientDescentOptimizer(0.01)train_step = my_opt.minimize(loss)init = tf.global_variables_initializer()sess.run(init)loss_vec = []train_accuracy = []test_accuracy = []for i in range(500): rand_index = np.random.choice(len(x_vals_train), size=batch_size) rand_x = x_vals_train[rand_index] rand_y = np.transpose([y_vals_train[rand_index]]) sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) temp_loss = sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) loss_vec.append(temp_loss) train_acc_temp = sess.run(accuracy, feed_dict=&#123; x_data: x_vals_train, y_target: np.transpose([y_vals_train])&#125;) train_accuracy.append(train_acc_temp) test_acc_temp = sess.run(accuracy, feed_dict=&#123; x_data: x_vals_test, y_target: np.transpose([y_vals_test])&#125;) test_accuracy.append(test_acc_temp) if (i + 1) % 100 == 0: print('Step #&#123;&#125; A = &#123;&#125;, b = &#123;&#125;'.format( str(i+1), str(sess.run(A)), str(sess.run(b)) )) print('Loss = ' + str(temp_loss)) [[a1], [a2]] = sess.run(A)[[b]] = sess.run(b)slope = -a2/a1y_intercept = b/a1x1_vals = [d[1] for d in x_vals]best_fit = []for i in x1_vals: best_fit.append(slope*i+y_intercept)setosa_x = [d[1] for i, d in enumerate(x_vals) if y_vals[i] == 1]setosa_y = [d[0] for i, d in enumerate(x_vals) if y_vals[i] == 1]not_setosa_x = [d[1] for i, d in enumerate(x_vals) if y_vals[i] == -1]not_setosa_y = [d[0] for i, d in enumerate(x_vals) if y_vals[i] == -1]plt.plot(setosa_x, setosa_y, 'o', label='I. setosa')plt.plot(not_setosa_x, not_setosa_y, 'x', label='Non-setosa')plt.plot(x1_vals, best_fit, 'r-', label='Linear Separator', linewidth=3)plt.ylim([0, 10])plt.legend(loc='lower right')plt.title('Sepal Length vs Pedal Width')plt.xlabel('Pedal Width')plt.ylabel('Sepal Length')plt.show()plt.plot(train_accuracy, 'k-', label='Training Accuracy')plt.plot(test_accuracy, 'r--', label='Test Accuracy')plt.title('Train and Test Set Accuracies')plt.xlabel('Generation')plt.ylabel('Accuracy')plt.legend(loc='lower right')plt.show()plt.plot(loss_vec, 'k-')plt.title('Loss per Generation')plt.xlabel('Generation')plt.ylabel('Loss')plt.show() 初探SVM 大家一定有很多问题，我之前也说过，我只介绍在tensorflow下面的一些实现，不叫原理，原理大家可以在网上找一些资料了解。邮箱air@weaf.top。","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]},{"title":"chapter-09-AIR","slug":"2018-05-07/chapter-09-AIR","date":"2018-05-20T11:31:09.000Z","updated":"2018-06-23T11:04:29.819Z","comments":true,"path":"posts/befe0ef0/","link":"","permalink":"http://weafteam.github.io/posts/befe0ef0/","excerpt":"","text":"TensorFlow Logistic Regression 还是得先和大家开个头，我最近有点忙，所以有时候需要抽开时间补上拉下的博客，这是我2018年5月20号补5月七号那个月的博客，先来介绍一下逻辑回归的概念，也就是相对于线性回归，其实逻辑回归就是一个分类问题，大家可以这么理解，也就是最后将值得分布确定在几类当中，就像我们最开始在博客一开始学习得fashion mnist一样，只不过，我们这节课来一个，简单得二分类问题。也就是： \\[ y=sigmoid(A*x+b) \\] 最后我们得到得y的预测值都会是0，或者是1，我们使用的数据是github的一个数据： 正式开始代码的编写，其实很简单。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tfimport requestsfrom tensorflow.python.framework import opsimport os.pathimport csv# 上面一如既往的模块导入ops.reset_default_graph()# 创建我们的会话sess = tf.Session()# 数据的文件名birth_weight_file = 'birth_weight.csv'# 下载数据if not os.path.exists(birth_weight_file): birthdata_url = 'https://github.com/nfmcclure/tensorflow_cookbook/' + \\ 'raw/master/01_Introduction/07_Working_with_Data_Sources/birthweight_data/birthweight.dat' birth_file = requests.get(birthdata_url) birth_data = birth_file.text.split('\\r\\n') birth_header = birth_data[0].split('\\t') birth_data = [[float(x) for x in y.split('\\t') if len(x)&gt;=1] for y in birth_data[1:] if len(y)&gt;=1] with open(birth_weight_file, \"w\") as f: writer = csv.writer(f) writer.writerow(birth_header) writer.writerows(birth_data) f.close()# 读取数据，就像我们之前说的，要放在placeholder得数据是要加载在数组里面得birth_data = []with open(birth_weight_file, newline='') as csvfile: csv_reader = csv.reader(csvfile) birth_header = next(csv_reader) for row in csv_reader: birth_data.append(row)birth_data = [[float(x) for x in row] for row in birth_data]# 分数据y_vals = np.array([x[0] for x in birth_data])x_vals = np.array([x[1:8] for x in birth_data])seed = 99np.random.seed(seed)tf.set_random_seed(seed)# 分离训练集和测试集train_indices = np.random.choice(len(x_vals), round(len(x_vals)*0.8), replace=False)test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))x_vals_train = x_vals[train_indices]x_vals_test = x_vals[test_indices]y_vals_train = y_vals[train_indices]y_vals_test = y_vals[test_indices]# 归一化数据函数def normalize_cols(m): col_max = m.max(axis=0) col_min = m.min(axis=0) return (m-col_min) / (col_max - col_min) x_vals_train = np.nan_to_num(normalize_cols(x_vals_train))x_vals_test = np.nan_to_num(normalize_cols(x_vals_test))# 定义批处理大小batch_size = 25# 初始化 placeholdersx_data = tf.placeholder(shape=[None, 7], dtype=tf.float32)y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)# 创建回归变量A = tf.Variable(tf.random_normal(shape=[7,1]))b = tf.Variable(tf.random_normal(shape=[1,1]))# 定义模型图的操作model_output = tf.add(tf.matmul(x_data, A), b)# 定义交叉熵损失函数loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output, labels=y_target))# 定义优化器my_opt = tf.train.GradientDescentOptimizer(0.01)train_step = my_opt.minimize(loss)# 初始化全局变量init = tf.global_variables_initializer()sess.run(init)# 预测值prediction = tf.round(tf.sigmoid(model_output))predictions_correct = tf.cast(tf.equal(prediction, y_target), tf.float32)accuracy = tf.reduce_mean(predictions_correct)# 开始训练loss_vec = []train_acc = []test_acc = []for i in range(1500): rand_index = np.random.choice(len(x_vals_train), size=batch_size) rand_x = x_vals_train[rand_index] rand_y = np.transpose([y_vals_train[rand_index]]) sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) temp_loss = sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) loss_vec.append(temp_loss) temp_acc_train = sess.run(accuracy, feed_dict=&#123;x_data: x_vals_train, y_target: np.transpose([y_vals_train])&#125;) train_acc.append(temp_acc_train) temp_acc_test = sess.run(accuracy, feed_dict=&#123;x_data: x_vals_test, y_target: np.transpose([y_vals_test])&#125;) test_acc.append(temp_acc_test) if (i+1)%300==0: # 每三百次打一次损失函数 print('Loss = ' + str(temp_loss)) ## 画出loss的值plt.plot(loss_vec, 'k-')plt.title('Cross Entropy Loss per Generation')plt.xlabel('Generation')plt.ylabel('Cross Entropy Loss')plt.show()# 画出训练和测试的准确率plt.plot(train_acc, 'k-', label='Train Set Accuracy')plt.plot(test_acc, 'r--', label='Test Set Accuracy')plt.title('Train and Test Accuracy')plt.xlabel('Generation')plt.ylabel('Accuracy')plt.legend(loc='lower right')plt.show() 这是补上周的博客，让大家认识一下逻辑回归模型的基本建立。有什么疑问，发邮件air@weaf.top。期待你哦！！！","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]},{"title":"Pillow的简单使用","slug":"2018-04-30/Pillow的简单使用","date":"2018-05-17T02:03:18.000Z","updated":"2018-05-17T05:49:32.492Z","comments":true,"path":"posts/33d9791f/","link":"","permalink":"http://weafteam.github.io/posts/33d9791f/","excerpt":"","text":"最近导师让我根据已有的笔画位置信息，生成相对应的图像信息，其中用到的就是Pillow库，所以接下来就按照这个任务要求，对Pillow这个库进行使用讲解。 数据 先介绍下数据吧，当时导师给了我一份600M的JSON文件，看到之后我的内心毫无波澜，甚至还想来一份黄焖鸡米饭。虽然数据量很大，但是每条数据是很规范的，例如这样： 1&#123;&quot;_id&quot;:&#123;&quot;$oid&quot;:&quot;58cbeb882398b78ed0c8165b&quot;&#125;,&quot;word&quot;:&quot;640,83;640,83;642,93;645,102;650,112;653,121;654,128;656,136;654,143;649,152;641,164;630,178;616,193;599,208;583,224;571,234;564,241;558,246;557,250;556,254;562,257;567,260;573,263;575,264;578,270;576,278;572,289;566,303;557,320;549,332;542,342;539,348;537,352;538,353;539,354;541,354;543,355;544,356;544,358;544,363;543,372;541,384;538,396;536,405;536,411;536,415;537,417;542,418;546,420;551,420;554,421;555,422;555,427;552,437;546,455;533,479;514,511;493,548;473,580;460,604;452,622;448,632;448,638;451,639;464,637;484,621;504,600;518,581;527,566;530,558;531,552;529,548;526,545;525,542;523,539;526,534;533,526;546,519;555,514;565,514;569,515;570,522;567,537;560,557;551,577;544,592;540,600;540,604;540,605;541,606;544,607;548,609;550,610;552,613;552,618;552,627;551,637;549,647;548,653;547,658;547,661;549,664;550,665;553,668;554,672;556,676;556,686;555,697;553,708;550,717;549,722;549,725;551,728;553,730;557,733;560,736;563,743;565,754;561,777;552,804;540,833;524,862;508,884;496,899;490,906;485,908;484,903;484,890;494,863;506,842;522,829;543,821;568,835;589,855;608,886;622,923;629,946;633,966;633,977;633,979#577,414;577,414;593,416;614,420;648,423;666,423;689,418;704,414;724,404;742,390;755,381&quot;,&quot;wordIndex&quot;:8826,&quot;str&quot;:&quot;ᠠᠯᠳᠠᠬᠤ&quot;,&quot;createAtDate&quot;:&#123;&quot;$date&quot;:&quot;2017-03-17T13:58:32.767Z&quot;&#125;,&quot;updateAtDate&quot;:&#123;&quot;$date&quot;:&quot;2017-03-17T13:58:32.767Z&quot;&#125;,&quot;phoneId&quot;:&quot;9e29fc4021b45fbe&quot;,&quot;userId&quot;:&quot;0161132290&quot;,&quot;paid&quot;:false,&quot;__v&quot;:0&#125; word字段就是笔画的信息，所以现在找个画笔跟着这个笔画画就行了。 思路 因为这些笔画的位置和大小并不是特别规范的（因为收集这些手写体的时候，就是利用手机的手写输入收集的，所以不是很规范），所以首先生成一个3000*3000的灰度图（底是白色），然后用一个很粗的画笔（当时用来一个width = 18的画笔）将上述的笔画画出来，在这之前我们可以先找出笔画的横向最大最小值和纵向的最大最小值，然后根据这四个值，在画布上裁剪出一个矩形框，最后我们可以将裁剪出的矩形区域resize，按照要求是生成宽度为32，高度等比例缩放的图像，当然这个时候我们可以加一个参数，叫做抗锯齿，稍后我会讲到。 因为传统的PIL库不支持Python3，所以使用从PIL派生出来的Pillow库。 Image和ImageDraw类 Pillow中最重要的就是Image类，该类存在于同名的模块中。我们本次用到的实例化方式是直接创建一个图片。 生成画布 1image = Image.new(\"L\",(3000,3000),\"white\") ImageDraw类，从名字中不难看出他的作用，那么接下来就开始画图吧。 顺便说一下，word字段里面，\\(;\\)分割的是点，\\(,\\)分割的是x和y，\\(#\\)分割的是笔画，所以根据这个层级关系，三层嵌套的方式就可以将这些笔画刻画出来。 生成画笔 1draw = ImageDraw.Draw(image) 画图 123456789lines = [[[int(x) for x in point.split(',')] for point in line.split(';')] for line in fileJson['word'].split('#')]for line in lines: if(len(line)&gt;1): draw.line((line[0][0], -line[0][1], line[1][0], -line[1][1]),fill='black',width=18) temp = [line[1][0],line[1][1]] for point in line: draw.line((temp[0],temp[1],point[0],point[1]),fill='black',width=18) temp=[point[0],point[1]] 裁剪 计算出横向纵向上的最大最小值，进行裁剪。（之所以加30，是因为有白边的情况下看着比较舒服，但是min的值并没有加，原因是容易产生黑杠） 12box = (hor_min , ver_min , hor_max + 30 , ver_max + 30)b = image.crop(box) resize并保存 12a = b.resize((32,length),Image.ANTIALIAS)a.save(path) 本次实验很简单，用到了Pillow中不多的函数，其实Pillow中还有很多有趣的操作，请大家自行研究它的中文文档。 效果 最后给大家看下效果：","categories":[],"tags":[{"name":"Pillow Python OCR","slug":"Pillow-Python-OCR","permalink":"http://weafteam.github.io/tags/Pillow-Python-OCR/"}]},{"title":"Spring Boot 关乎java程序员","slug":"2018-05-14/Spring-Boot--about-java-software-engineer","date":"2018-05-16T18:04:44.000Z","updated":"2018-07-17T08:31:55.166Z","comments":true,"path":"posts/1cf77d1e/","link":"","permalink":"http://weafteam.github.io/posts/1cf77d1e/","excerpt":"","text":"一、简介 Spring Boot2.0一推出就激起了一阵学习Spring Boot的热浪，就百度和搜索引擎的数据报告显示Spring Boot相关搜索指数急剧增加。 那么Spring Boot到底是什么？（想必大家都有了一些了解） 那么为什么会有这么多人去学习他呢？（值得思考的问题） Spring Boot的诞生 随着使用Spring的人越来越多，Spring就开始从一个简单、单一的小框架变成一个大而全的开源软件，都后来，几乎用整个Spring就可以支撑起企业中所有的服务了。但是随之也使得带来一些问题。 之前的Spring 有着各种各样的配置文件。导致我们在使用的时候不得不配置很多东西。 后来Spring慢慢的发现了这个问题。为了解决这一问题，开始了Spring Boot项目的研发。 Spring Boot是什么？ Spring Boot是由Pivotal团队提供的全新框架，其设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通过这种方式，Spring Boot致力于在蓬勃发展的快速应用开发领域(rapid application development)成为领导者。 特点 创建独立的Spring应用程序 嵌入的Tomcat，无需部署WAR文件 简化Maven配置 自动配置Spring 提供生产就绪型功能，如指标，健康检查和外部配置 绝对没有代码生成和对XML没有要求配置 二、SpringBoot关乎Java程序员 首先我们要知道，现在企业中很多都是用Spring开源框架，并且现在Spring Boot2.0作为Spring的优秀产物，已经吸引了很多技术和企业去使用它。 这使得我们Java程序员不得不去学习他的一个原因。 其次，Spring Boot 还具有一下特点： Spring Boot 让开发变得更简单 登录网址 http://start.spring.io/ 选择对应的组件直接下载 导入项目，直接开发 Spring Boot 使测试变得更简单 引入spring-boot-start-test依赖包 对数据库、Mock、 Web 等各种情况进行测试。 Spring Boot 让配置变得更简单 Spring Boot 让部署变得更简单 Spring Boot 让监控变得更简单 三、项目生产 Spring Boot项目搭建非常简单 我们可以使用spring提供的网站直接生产自己想要的项目 https://start.spring.io/ 这里生产的demo可以将你所需的所有东西都集成进去。 我们可以通过点击 Switch to the full version 来查看所有支持的列表 四、目录解析 我们可以看出默认给出的demo的项目目录很简单。 都是我们通常使用的三个目录 src/main/java/ src/main/resources/ src/test/java/ 这里resources 我使用的是yaml文件。而且SpringBoot对它的支持也是非常好的。 。 参考文档： https://spring.io/projects/spring-boot http://blog.51cto.com/ityouknow/2128700","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://weafteam.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://weafteam.github.io/tags/JAVA/"}]},{"title":"asyncio 不完全指北（五）","slug":"2018-05-07/guide-to-asyncio-5","date":"2018-05-13T15:09:56.000Z","updated":"2018-08-07T13:19:42.442Z","comments":true,"path":"posts/112d613e/","link":"","permalink":"http://weafteam.github.io/posts/112d613e/","excerpt":"","text":"书接上文。 用协程和流实现异步 I / O 本节将重新实现 echo 服务器和客户端的两个示例程序，只不过会使用协程和 asyncio 流 API 而不是 Protocol 和 Transport 类抽象。这些示例在比前面讨论的Protocol API 更低的抽象级别上操作，但是处理的事件是相似的。 Echo 服务器 服务器程序首先导入所需的 asyncio 和 logging 模块，然后创建事件循环对象： 1234567891011121314import asyncioimport loggingimport sysSERVER_ADDRESS = ('localhost', 10000)logging.basicConfig( level=logging.DEBUG, format='%(name)s: %(message)s', stream=sys.stderr,)log = logging.getLogger('main')event_loop = asyncio.get_event_loop() 然后定义一个协程来处理通信。每次客户端连接时，都会调用协程的新实例，从而在该函数中的代码一次只能与一个客户端通信。Python 的语言运行时管理每个协程实例的状态，因此应用程序代码不需要管理任何额外的数据结构来跟踪单独的客户端。 协程接受的参数是与新连接关联的 StreamReader 和 StreamWriter 实例。与 Transport 一样，可以通过 writer 的 get_extra_info() 方法访问客户端地址： 1234async def echo(reader, writer): address = writer.get_extra_info('peername') log = logging.getLogger('echo_&#123;&#125;_&#123;&#125;'.format(*address)) log.debug('connection accepted') 虽然在建立连接时调用协程，但可能还没有任何要读取的数据。为了避免在读取时阻塞，协程使用 await read() 来允许事件循环继续处理其他任务，直到有数据要读取： 12while True: data = await reader.read(128) 如果客户端发送了数据，则从 await 返回数据，并可通过将其传递给 writer 发送回客户端。对 write() 的多个调用可用于缓冲传出的数据，然后使用 drain() 刷新结果。由于刷新网络 I / O 可能会阻塞，因此再次使用 await 来恢复对事件循环的控制，事件循环监视写入 socket，并在可能发送更多数据时调用 writer： 12345if data: log.debug(f'received &#123;data&#125;') writer.write(data) await writer.drain() log.debug(f'sent &#123;data&#125;') 如果客户端未发送任何数据，read() 将返回一个空字节串，以指示连接已关闭。服务器需要关闭 socket 以写入客户端，然后 协程可以返回以指示它已完成： 1234else: log.debug('closing') writer.close() return 启动服务器有两个步骤。首先，应用程序告诉事件循环要监听的主机名和 socket，使用协程创建新的服务器对象。 start_server() 方法本身就是一个协程，因此必须由事件循环处理结果才能实际启动服务器。完成协程产生了绑定到事件循环的 asyncio.Server 实例： 123factory = asyncio.start_server(echo, *SERVER_ADDRESS)server = event_loop.run_until_complete(factory)log.debug('starting up on &#123;&#125; port &#123;&#125;'.format(*SERVER_ADDRESS)) 需要运行事件循环以处理事件和客户端请求。对于长期运行的服务，run_forever() 方法是最简单的方法。当事件循环停止时，无论是通过应用程序代码还是通过发信号通知进程，服务器都可以关闭以正确清理 socket，然后可以关闭事件循环以在程序退出之前完成对任何其他事务的处理： 12345678910try: event_loop.run_forever()except KeyboardInterrupt: passfinally: log.debug('closing server') server.close() event_loop.run_until_complete(server.wait_closed()) log.debug('closing event loop') event_loop.close() Echo 客户端 使用协程构建客户端非常类似于构建服务器。代码再次开始于导入 asyncio 和 logging 模块，然后创建事件循环对象： 12345678910111213141516171819import asyncioimport loggingimport sysMESSAGES = [ b'This is the message. ', b'It will be sent ', b'in parts.',]SERVER_ADDRESS = ('localhost', 10000)logging.basicConfig( level=logging.DEBUG, format='%(name)s: %(message)s', stream=sys.stderr,)log = logging.getLogger('main')event_loop = asyncio.get_event_loop() echo_client 协程接受两个参数，告诉它服务器在哪里以及要发送什么消息： 1async def echo_client(address, messages): 当任务启动时调用协程，但它没有可用的活动连接。因此，第一步是让客户端建立自己的连接。它使用 await 来避免在 open_connection() 协程运行时阻塞其他活动： 1234log = logging.getLogger('echo_client')log.debug('connecting to &#123;&#125; port &#123;&#125;'.format(*address))reader, writer = await asyncio.open_connection(*address) open_connection() 协程返回与新 socket 关联的 StreamReader 和 StreamWriter 实例。下一步是使用 writer 向服务器发送数据。与服务器一样，writer 将缓冲传出的数据，直到 socket 就绪或使用 drain() 刷新结果。由于刷新网络 I / O 可能会阻塞，因此再次使用 await 来恢复对事件循环的控制，事件循环监视写入 socket，并在可能发送更多数据时调用 writer： 123456for msg in messages: writer.write(msg) log.debug(f'sending &#123;msg&#125;')if writer.can_write_eof(): writer.write_eof()await writer.drain() 接下来，客户端通过尝试读取数据直到没有要读取的内容来获取来自服务器的响应。为了避免阻塞单个 read() 调用，await 将控制权交还给事件循环。如果服务器已发送数据，则会记录数据。如果服务器未发送任何数据，read() 将返回一个空字节串，指示连接已关闭。客户端需要关闭 socket 以发送到服务器，然后返回以指示已完成： 123456789log.debug('waiting for response')while True: data = await reader.read(128) if data: log.debug(f'received &#123;data&#125;') else: log.debug('closing') writer.close() return 要启动客户端，使用协程调用事件循环以创建客户端。使用 run_until_complete() 可避免客户端程序中出现无限循环。与Protocol 示例不同，协程完成时不需要单独的 future 发出信号，因为 echo_client() 包含所有客户端逻辑本身，并且在收到响应并关闭服务器连接之前不会返回： 12345try: event_loop.run_until_complete(echo_client(SERVER_ADDRESS, MESSAGES))finally: log.debug('closing event loop') event_loop.close() 输出 在一个窗口中运行服务器而在另一个窗口中运行客户端。 客户端将产生以下输出： 123456789asyncio: Using selector: SelectSelectorecho_client: connecting to localhost port 10000echo_client: sending b'This is the message. 'echo_client: sending b'It will be sent 'echo_client: sending b'in parts.'echo_client: waiting for responseecho_client: received b'This is the message. It will be sent in parts.'echo_client: closingmain: closing event loop 虽然客户端总是单独发送消息，但客户端第一次运行时，服务器会收到一条大消息，并将该消息返回给客户端。根据网络的繁忙程度以及是否在准备所有数据之前刷新网络缓冲区，这些结果在后续运行中会有所不同： 123456asyncio: Using selector: SelectSelectormain: starting up on localhost port 10000echo_::1_11075: connection acceptedecho_::1_11075: received b'This is the message. It will be sent in parts.'echo_::1_11075: sent b'This is the message. It will be sent in parts.'echo_::1_11075: closing 123456echo_::1_11200: connection acceptedecho_::1_11200: received b'This is the message. It will be sent 'echo_::1_11200: sent b'This is the message. It will be sent 'echo_::1_11200: received b'in parts.'echo_::1_11200: sent b'in parts.'echo_::1_11200: closing 与子进程协作 为了利用现有代码而不重写，或者访问 Python 中不可用的库或功能，我们经常需要使用其他程序或进程。与网络 I / O 一样，asyncio 包括两个抽象，用于启动另一个程序，然后与它交互。 使用子进程的 Protocol 抽象 这个例子使用协程启动一个进程来运行 Unix 命令 df，以便查看在本地磁盘上的可用空间。它使用 subprocess_exec() 启动进程，并将其绑定到知道如何读取 df 命令输出并对其进行分析的 Protocol 类。Protocol 类的方法是根据子进程的 I / O 事件自动调用的。因为 stdin 和 stderr 参数都设置为 None，所以这些通信通道不会连接到新进程： 12345678910111213141516171819202122232425import asyncioimport functoolsasync def run_df(loop): print('in run_df') cmd_done = asyncio.Future(loop=loop) factory = functools.partial(DFProtocol, cmd_done) proc = loop.subprocess_exec( factory, 'df', '-hl', stdin=None, stderr=None, ) try: print('launching process') transport, protocol = await proc print('waiting for process to complete') await cmd_done finally: transport.close() return cmd_done.result() 类 DFProtocol 继承自 SubprocessProtocol，该 Protocol 定义了类通过管道与另一进程通信的 API。done 参数是调用者用来监视进程是否完成的 future： 12345678class DFProtocol(asyncio.SubprocessProtocol): FD_NAMES = ['stdin', 'stdout', 'stderr'] def __init__(self, done_future): self.done = done_future self.buffer = bytearray() super().__init__() 与 socket 通信一样，在设置新进程的输入通道时调用 connection_made()。transport 参数是 BaseSubprocessTransport 子类的一个实例。如果进程被配置为接收输入，则它可以读取进程输出的数据并将数据写入进程的输入流： 123def connection_made(self, transport): print(f'process started &#123;transport.get_pid()&#125;') self.transport = transport 当进程生成输出时，pipe_data_received() 将使用发送数据的文件描述符和从管道读取的实际数据作为参数调用。Protocol类将进程的标准输出通道的输出保存在缓冲区中，以供以后处理： 1234def pipe_data_received(self, fd, data): print(f'read &#123;len(data)&#125; bytes from &#123;self.FD_NAMES[fd]&#125;') if fd == 1: self.buffer.extend(data) 当进程终止时，process_exited() 将被调用。通过调用 get_returncode() 可以从 transport 对象获得进程的退出代码。在这种情况下，如果没有报告错误，则可以在通过 future 实例返回可用输出之前对其进行解码和分析。如果出现错误，则结果为空。设置 future 的结果会告诉 run_df() 进程已退出，因此它会清理并返回结果： 12345678910def process_exited(self): print('process exited') return_code = self.transport.get_returncode() print(f'return code &#123;return_code&#125;') if not return_code: cmd_output = bytes(self.buffer).decode() results = self._parse_results(cmd_output) else: results = [] self.done.set_result((return_code, results)) 命令的输出被解析成一系列字典，将每行输出的标题名称映射到值，并返回结果列表： 123456789def _parse_results(self, output): print('parsing results') if not output: return [] lines = output.splitlines() headers = lines[0].split() devices = lines[1:] results = [dict(zip(headers, line.split())) for line in devices] return results run_df() 协程使用 run_until_complete() 运行，然后检查结果并打印每个设备上的可用空间： 123456789101112event_loop = asyncio.get_event_loop()try: return_code, results = event_loop.run_until_complete(run_df(event_loop))finally: event_loop.close()if return_code: print(f'error exit &#123;return_code&#125;')else: print('\\nFree space:') for r in results: print(f'&#123;r[\"Mounted\"]:25&#125;: &#123;r[\"Avail\"]&#125;') 下面的输出显示了执行步骤的顺序，以及系统中驱动器的可用空间： 123456789101112in run_dflaunching processprocess started 6170waiting for process to completeread 375 bytes from stdoutprocess exitedreturn code 0parsing resultsFree space:/ : 41G... 用协程和流调用子进程 若要使用协程直接运行进程，而不是通过 Protocol 子类访问进程，请调用 create_subprocess_exec()，并指定一个连接到管道的标准输出、标准错误和标准输入。产生子进程的协程的结果是一个 Process 实例，可用于操作子进程或与其通信： 1234567891011121314151617import asyncioimport asyncio.subprocessasync def run_df(): print('in run_df') buffer = bytearray() create = asyncio.create_subprocess_exec( 'df', '-hl', stdout=asyncio.subprocess.PIPE, ) print('launching process') proc = await create print(f'process started &#123;proc.pid&#125;') 在这个例子中，df 除了命令行参数之外不需要任何输入，因此下一步是读取所有输出。对于 Protocol，无法控制一次读取多少数据。这个例子中使用了 readline()，但也可以直接调用 read() 读取不是按行组织的数据。命令的输出被缓冲，就像 Protocol 示例一样，因此稍后可以对其进行分析： 1234567while True: line = await proc.stdout.readline() print(f'read &#123;line!r&#125;') if not line: print('no more output from command') break buffer.extend(line) readline() 方法在程序已完成不再有输出时返回空字节串。为确保正确清除进程，下一步是等待进程完全退出： 12print('waiting for process to complete')await proc.wait() 此时可以检查退出状态，以确定是解析输出还是将错误视为未生成输出。解析逻辑与前面的示例相同，但处于独立函数中，因为没有可以包装它的 Protocol 类。解析数据后，结果和退出代码将返回给调用方： 123456789return_code = proc.returncodeprint(f'return code &#123;return_code&#125;')if not return_code: cmd_output = bytes(buffer).decode() results = _parse_results(cmd_output)else: results = []return (return_code, results) 主程序看起来类似于基于 Protocol 的示例，因为实现的改变被隔离在 run_df() 中： 123456789101112event_loop = asyncio.get_event_loop()try: return_code, results = event_loop.run_until_complete(run_df())finally: event_loop.close()if return_code: print(f'error exit &#123;return_code&#125;')else: print('\\nFree space:') for r in results: print(f'&#123;r[\"Mounted\"]:25&#125;: &#123;r[\"Avail\"]&#125;') 由于 df 的输出可以一次读取一行，因此它将显示程序的进度。否则，输出看起来与前面的示例类似： 123456789101112131415in run_dflaunching processprocess started 7354read b'Filesystem Size Used Avail Use% Mounted on\\n'read b'/dev/vda1 50G 6.0G 41G 13% /\\n'...read b''no more output from commandwaiting for process to completereturn code 0parsing resultsFree space:/ : 41G... 向子进程发送数据 前面的两个示例都仅使用单个通信信道来从子进程读取数据。通常需要将数据发送到命令中进行处理。下面将定义一个协程，用于执行 Unix 命令 tr 以转换其输入流中的字符。这个例子中tr 用于将小写字母转换为大写字母。 to_upper() 协程将输入字符串作为参数。它产生运行 tr [:lower:] [:upper:] 的子进程： 1234567891011121314151617import asyncioimport asyncio.subprocessasync def to_upper(input): print('in to_upper') create = asyncio.create_subprocess_exec( 'tr', '[:lower:]', '[:upper:]', stdout=asyncio.subprocess.PIPE, stdin=asyncio.subprocess.PIPE, ) print('launching process') proc = await create print(f'pid &#123;proc.pid&#125;') 接下来 to_upper() 使用 Process 的 communicate() 方法将输入字符串发送到命令，并异步读取所有生成的输出。与 subprocess.Popen 版本的方法相同，communicate() 返回完整的输出字节串。如果一个命令可能产生的数据超出了可以充裕的放入内存的范围，或者无法一次产生输入，或者必须增量处理输出，则可以直接使用进程的 stdin、stdout 和 stderr 句柄，而不是调用 communicate() ： 12print('communicating with process')stdout, stderr = await proc.communicate(input.encode()) I / O 完成后，等待进程完全退出可确保进程得到正确清理： 12print('waiting for process to complete')await proc.wait() 然后可以检查返回代码，并对输出字节串进行解码，以准备协程的返回值： 12345678return_code = proc.returncodeprint(f'return code &#123;return_code&#125;')if not return_code: results = bytes(stdout).decode()else: results = ''return (return_code, results) 程序的主要部分构建要转换的消息字符串，然后设置事件循环以运行 to_upper() 并打印结果： 12345678910111213141516MESSAGE = \"\"\"This message will be convertedto all caps.\"\"\"event_loop = asyncio.get_event_loop()try: return_code, results = event_loop.run_until_complete(to_upper(MESSAGE))finally: event_loop.close()if return_code: print(f'error exit &#123;return_code&#125;')else: print(f'Original: &#123;MESSAGE!r&#125;'.format(MESSAGE)) print(f'Changed : &#123;results!r&#125;') 输出显示操作序列，然后显示如何转换简单文本消息： 12345678in to_upperlaunching processpid 12428communicating with processwaiting for process to completereturn code 0Original: '\\nThis message will be converted\\nto all caps.\\n'Changed : '\\nTHIS MESSAGE WILL BE CONVERTED\\nTO ALL CAPS.\\n' 接收 Unix 信号 UNIX 系统事件通知通常会中断应用程序，从而触发其处理程序。当与 asyncio 一起使用时，信号处理程序回调与事件循环管理的其他协程和回调交错执行。这导致中断函数较少，因此需要提供安全防护来清理不完整的操作。 信号处理程序必须是常规的可调用程序，而不是协程： 12345678import asyncioimport functoolsimport osimport signaldef signal_handler(name): print(f'signal_handler(&#123;name!r&#125;)') 信号处理程序是使用 add_signal_handler() 注册的。第一个参数是信号，第二个参数是回调。回调不传递参数，因此如果需要参数，可以使用 functools.partical() 包装函数： 12345678910111213event_loop = asyncio.get_event_loop()event_loop.add_signal_handler( signal.SIGHUP, functools.partial(signal_handler, name='SIGHUP'),)event_loop.add_signal_handler( signal.SIGUSR1, functools.partial(signal_handler, name='SIGUSR1'),)event_loop.add_signal_handler( signal.SIGINT, functools.partial(signal_handler, name='SIGINT'),) 本示例程序使用协程通过 os.kill() 向自身发送信号。在发送每个信号之后，协程将让出控制权以允许处理程序执行。在一个正常的应用程序中，会有很多应用程序代码让步给事件循环的地方，而不需要这样的人工让步： 12345678910async def send_signals(): pid = os.getpid() print(f'starting send_signals for &#123;pid&#125;') for name in ['SIGHUP', 'SIGHUP', 'SIGUSR1', 'SIGINT']: print(f'sending &#123;name&#125;') os.kill(pid, getattr(signal, name)) print('yielding control') await asyncio.sleep(0.01) return 主程序运行 send_signals()，直到它发送完所有信号： 1234try: event_loop.run_until_complete(send_signals())finally: event_loop.close() 输出显示当 send_signals() 在发送信号后让出控制时如何调用处理程序： 12345678910111213starting send_signals for 23185sending SIGHUPyielding controlsignal_handler('SIGHUP')sending SIGHUPyielding controlsignal_handler('SIGHUP')sending SIGUSR1yielding controlsignal_handler('SIGUSR1')sending SIGINTyielding controlsignal_handler('SIGINT') 将协程与线程和进程相结合 许多现有库尚未准备好与 asyncio 配合使用。它们可能会阻塞或依赖模块中不可用的并发功能。通过使用来自 concurrent.futures 的 executor 在单独的线程或单独的进程中运行代码，仍然可以在基于 asyncio 的应用程序中使用这些库。 线程 事件循环的 run_in_executor() 方法接受的参数为 executor 实例，要调用的常规可调用对象以及要传递给可调用对象的任何参数。它返回一个可用于等待函数完成其工作并返回某些内容的 future。如果没有传入 executor，则会创建 ThreadPoolExecutor。此示例显式创建一个 executor，以限制可用的工作线程数。 ThreadPoolExecutor启动其工作线程，然后在线程中调用每个提供的函数一次。此示例说明如何将 run_in_executor() 和 wait() 组合起来，以便在阻塞单独线程中运行的函数的同时，对事件循环具有协程让步控制，然后在这些函数完成时将其唤醒： 1234567891011121314151617181920212223242526272829303132333435363738394041424344import asyncioimport concurrent.futuresimport loggingimport sysimport timedef blocks(n): log = logging.getLogger(f'blocks(&#123;n&#125;)') log.info('running') time.sleep(0.1) log.info('done') return n**2async def run_blocking_tasks(executor): log = logging.getLogger('run_blocking_tasks') log.info('starting') log.info('creating executor tasks') loop = asyncio.get_event_loop() blocking_tasks = [loop.run_in_executor(executor, blocks, i) for i in range(6)] log.info('waiting for executor tasks') completed, pending = await asyncio.wait(blocking_tasks) results = [t.result() for t in completed] log.info(f'results: &#123;results!r&#125;') log.info('exiting')if __name__ == '__main__': logging.basicConfig( level=logging.INFO, format='%(threadName)10s %(name)18s: %(message)s', stream=sys.stderr, ) executor = concurrent.futures.ThreadPoolExecutor(max_workers=3,) event_loop = asyncio.get_event_loop() try: event_loop.run_until_complete(run_blocking_tasks(executor)) finally: event_loop.close() 这个程序使用 logging 来方便地指示哪些线程和函数正在生成的日志消息。因为每次调用 blocks() 时使用单独的 Logger，所以输出清楚地显示了相同的线程被重用，以调用具有不同参数的函数的多个副本： 1234567891011121314151617MainThread run_blocking_tasks: startingMainThread run_blocking_tasks: creating executor tasksThreadPoolExecutor-0_0 blocks(0): runningThreadPoolExecutor-0_1 blocks(1): runningThreadPoolExecutor-0_2 blocks(2): runningMainThread run_blocking_tasks: waiting for executor tasksThreadPoolExecutor-0_0 blocks(0): doneThreadPoolExecutor-0_0 blocks(3): runningThreadPoolExecutor-0_1 blocks(1): doneThreadPoolExecutor-0_2 blocks(2): doneThreadPoolExecutor-0_1 blocks(4): runningThreadPoolExecutor-0_2 blocks(5): runningThreadPoolExecutor-0_0 blocks(3): doneThreadPoolExecutor-0_1 blocks(4): doneThreadPoolExecutor-0_2 blocks(5): doneMainThread run_blocking_tasks: results: [16, 25, 1, 4, 0, 9]MainThread run_blocking_tasks: exiting 进程 ProcessPoolExecutor 的工作方式大致相同，它创建一组工作进程而不是线程。使用单独的进程需要更多的系统资源，但是对于计算密集型操作，在每个 CPU 内核上运行单独的任务是有意义的： 12345678910111213141516...if __name__ == '__main__': logging.basicConfig( level=logging.INFO, format='PID %(process)5s %(name)18s: %(message)s', stream=sys.stderr, ) executor = concurrent.futures.ProcessPoolExecutor(max_workers=3,) event_loop = asyncio.get_event_loop() try: event_loop.run_until_complete(run_blocking_tasks(executor)) finally: event_loop.close() 从线程转移到进程所需的唯一更改是创建不同类型的 executor。本示例还将日志记录格式更改为包含进程 id 而不是线程名称，以证明任务实际上正在单独的进程中运行： 1234567891011121314151617PID 24417 run_blocking_tasks: startingPID 24417 run_blocking_tasks: creating executor tasksPID 24417 run_blocking_tasks: waiting for executor tasksPID 24461 blocks(0): runningPID 24460 blocks(1): runningPID 24459 blocks(2): runningPID 24460 blocks(1): donePID 24459 blocks(2): donePID 24460 blocks(3): runningPID 24461 blocks(0): donePID 24459 blocks(4): runningPID 24461 blocks(5): runningPID 24460 blocks(3): donePID 24459 blocks(4): donePID 24461 blocks(5): donePID 24417 run_blocking_tasks: results: [16, 1, 25, 0, 4, 9]PID 24417 run_blocking_tasks: exiting 调试 asyncio 内置了几个有用的调试功能。 首先，事件循环使用 logging 在运行时发出状态消息。如果在应用程序中启用了日志记录，则其中一些是可用的。其他的可以通过告诉循环发出更多调试消息来打开。调用 set_debug()，传递一个布尔值，指示是否应启用调试。 由于基于 asyncio 构建的应用程序对无法让出控制的“贪婪”协程非常敏感，因此支持检测事件循环中的缓慢回调。通过启用调试将其打开，并通过将循环的 slow_callback_duration 属性设置为应发出警告的秒数来定义 “缓慢”。 最后，如果使用 asyncio 的应用程序在不清理某些协程或其他资源的情况下退出，这可能意味着存在逻辑错误，无法运行某些应用程序代码。启用 ResourceWarning 警告会在程序退出时报告这些情况： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import argparseimport asyncioimport loggingimport sysimport timeimport warningsparser = argparse.ArgumentParser('debugging asyncio')parser.add_argument( '-v', dest='verbose', default=False, action='store_true',)args = parser.parse_args()logging.basicConfig( level=logging.DEBUG, format='%(levelname)7s: %(message)s', stream=sys.stderr,)LOG = logging.getLogger('')async def inner(): LOG.info('inner starting') # 模拟缓慢的任务 time.sleep(0.1) LOG.info('inner completed')async def outer(loop): LOG.info('outer starting') await asyncio.ensure_future(loop.create_task(inner())) LOG.info('outer completed')event_loop = asyncio.get_event_loop()if args.verbose: LOG.info('enabling debugging') # 启用调试 event_loop.set_debug(True) # 定义一个很小阈值表示“缓慢” event_loop.slow_callback_duration = 0.001 # 报告管理异步资源的所有错误 warnings.simplefilter('always', ResourceWarning)LOG.info('entering event loop')event_loop.run_until_complete(outer(event_loop)) 在未启用调试的情况下运行时，此应用程序的所有内容看起来都很好： 123456DEBUG: Using selector: SelectSelector INFO: entering event loop INFO: outer starting INFO: inner starting INFO: inner completed INFO: outer completed 开启调试会暴露出一些问题，包括 inner() 完成所花的时间比设定的 slow_callback_duration 还要长，而且当程序结束时，事件循环并未正确关闭： 12345678 DEBUG: Using selector: SelectSelector INFO: enabling debugging INFO: entering event loop INFO: outer starting INFO: inner starting INFO: inner completedWARNING: Executing &lt;Task finished coro=&lt;inner() done, defined at *.py:25&gt; result=None created at *.py:33&gt; took 0.093 seconds INFO: outer completed","categories":[],"tags":[]},{"title":"Kafka在SpringBoot 2.0中的整合-1","slug":"2018-05-07/SpringBoot-integration-with-Kafka-1","date":"2018-05-07T10:13:02.000Z","updated":"2018-06-23T11:04:29.818Z","comments":true,"path":"posts/6e26f2f1/","link":"","permalink":"http://weafteam.github.io/posts/6e26f2f1/","excerpt":"","text":"一、简介 之前我讲过关于SpringBoot整合Kafka的demo,但是后来使用过程中，越来与发现这个已经不能适合我们现在使用的环境了。 之前是简单的发送消息而已，而现在我们需要的是一个高吞吐量的web中间件。 现在每秒大概能发送1000条左右的数据，但是接受这边并不能承受这么高的吞吐量，导致kafka消息堆积，消息堆积的越来越多。 恰巧我们的产品所需要的数据实时性非常高。所以这不得以我要改进我读的速度。 二、入门 简单介绍一点相关的概念 2.1 消费消息基本流程 Kafka 订阅者在订阅消息时的基本流程是： Poll 数据 执行消费逻辑 再次 poll 数据 2.2 负载均衡 每个 Consumer Group 可以包含多个消费实例，即可以启动多个 Kafka Consumer，并把参数 group.id 设置成相同的值。属于同一个 Consumer Group 的消费实例会负载消费订阅的 Topic。 举例：Consumer Group A 订阅了 Topic A，并开启三个消费实例 C1、C2、C3，则发送到 Topic A 的每条消息最终只会传给 C1、C2、C3 的某一个。Kafka 默认会均匀地把消息传给各个消息实例，以做到消费负载均衡。 Kafka 负载消费的内部原理是，把订阅的 Topic 的分区，平均分配给各个消费实例。因此，消费实例的个数不要大于分区的数量，否则会有实例分配不到任何分区而处于空跑状态。这个负载均衡发生的时间，除了第一次启动上线之外，后续消费实例发生重启、增加、减少等变更时，都会触发一次负载均衡。 消息队列 Kafka 的每个 Topic 的分区数量默认是 16 个，已经足够满足大部分场景的需求，且云上服务会根据容量调整分区数。 2.3 多个订阅 一个 Consumer Group 可以订阅多个 Topic。一个 Topic 也可以被多个 Consumer Group 订阅，且各个 Consumer Group 独立消费 Topic 下的所有消息。 举例：Consumer Group A 订阅了 Topic A，Consumer Group B 也订阅了 Topic A，则发送到 Topic A 的每条消息，不仅会传一份给 Consumer Group A 的消费实例，也会传一份给 Consumer Group B 的消费实例，且这两个过程相互独立，相互没有任何影响。 ### 2.4 消费位点 每个 Topic 会有多个分区，每个分区会统计当前消息的总条数，这个称为最大位点 MaxOffset。Kafka Consumer 会按顺序依次消费分区内的每条消息，记录已经消费了的消息条数，称为ConsumerOffset。 剩余的未消费的条数（也称为消息堆积量） = MaxOffset - ConsumerOffset 2.5 消费位点提交 Kafka 消费者有两个相关参数： enable.auto.commit：默认值为 true。 auto.commit.interval.ms： 默认值为 1000，也即 1s。 这两个参数组合的结果就是，每次 poll 数据前会先检查上次提交位点的时间，如果距离当前时间已经超过参数auto.commit.interval.ms规定的时长，则客户端会启动位点提交动作。 因此，如果将enable.auto.commit设置为 true，则需要在每次 poll 数据时，确保前一次 poll 出来的数据已经消费完毕，否则可能导致位点跳跃。 如果想自己控制位点提交，请把 enable.auto.commit 设为 false，并调用 commit(offsets)函数自行控制位点提交。 2.6 消息重复和消费幂等 Kafka 消费的语义是 “at least once”， 也就是至少投递一次，保证消息不丢，但是不会保证消息不重复。在出现网络问题、客户端重启时均有可能出现少量重复消息，此时应用消费端如果对消息重复比较敏感（比如说订单交易类），则应该做到消息幂等。 以数据库类应用为例，常用做法是： 发送消息时，传入 key 作为唯一流水号ID； 消费消息时，判断 key 是否已经消费过，如果已经消费过了，则忽略，如果没消费过，则消费一次； 当然，如果应用本身对少量消息重复不敏感，则不需要做此类幂等检查。 2.7 消费失败 Kafka 是按分区一条一条消息顺序向前推进消费的，如果消费端拿到某条消息后执行消费逻辑失败，比如应用服务器出现了脏数据，导致某条消息处理失败，等待人工干预，那么有以下两种处理方式： 失败后一直尝试再次执行消费逻辑。这种方式有可能造成消费线程阻塞在当前消息，无法向前推进，造成消息堆积； 由于 Kafka 自身没有处理失败消息的设计，实践中通常会打印失败的消息、或者存储到某个服务（比如创建一个 Topic 专门用来放失败的消息），然后定时 check 失败消息的情况，分析失败原因，根据情况处理。 ### 2.8 消费阻塞以及堆积 消费端最常见的问题就是消费堆积，最常造成堆积的原因是： 消费速度跟不上生产速度，此时应该提高消费速度，详见下一节《提高消费速度》； 消费端产生了阻塞。 消费端拿到消息后，执行消费逻辑，通常会执行一些远程调用，如果这个时候同步等待结果，则有可能造成一直等待，消费进程无法向前推进。 消费端应该竭力避免堵塞消费线程，如果存在等待调用结果的情况，建议设置等待的超时时间，超时后作消费失败处理。 2.9 提高消费速度 提高消费速度有以下两个办法： 增加 Consumer 实例个数 增加消费线程 ##### 增加 Consumer 实例 可以在进程内直接增加（需要保证每个实例对应一个线程，否则没有太大意义），也可以部署多个消费实例进程；需要注意的是，实例个数超过分区数量后就不再能提高速度，将会有消费实例不工作。 增加消费线程 增加 Consumer 实例本质上也是增加线程的方式来提升速度，因此更加重要的性能提升方式是增加消费线程，最基本的步骤如下： 定义一个线程池； Poll 数据； 把数据提交到线程池进行并发处理； 等并发结果返回成功后，再次 poll 数据执行。 2.10 消息过滤 Kafka 自身没有消息过滤的语义。实践中可以采取以下两个办法： 如果过滤的种类不多，可以采取多个 Topic 的方式达到过滤的目的； 如果过滤的种类多，则最好在客户端业务层面自行过滤。 实践中请根据业务具体情况进行选择，也可以综合运用上面两种办法。 2.11 消息广播 Kafka 自身没有消息广播的语义，可以通过创建不同的 Consumer Group 来模拟实现。 2.12 订阅关系 同一个 Consumer Group 内，各个消费实例订阅的 Topic 最好保持一致，避免给排查问题带来干扰。 三、开始抛代码 我们的问题是由于消费的速度不够引起的。根据2.9的办法。我提供了以下代码 消费者配置： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980package com.xxx.xxxxxx.kafka;import org.apache.kafka.clients.CommonClientConfigs;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.common.config.SaslConfigs;import org.apache.kafka.common.config.SslConfigs;import org.apache.kafka.common.serialization.StringDeserializer;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.kafka.annotation.EnableKafka;import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;import org.springframework.kafka.config.KafkaListenerContainerFactory;import org.springframework.kafka.core.ConsumerFactory;import org.springframework.kafka.core.DefaultKafkaConsumerFactory;import org.springframework.kafka.listener.BatchLoggingErrorHandler;import org.springframework.kafka.listener.ConcurrentMessageListenerContainer;import java.util.HashMap;import java.util.Map;/** * @Author ：yaxuSong * @Description: * @Date: 18:15 2018/4/24 * @Modified by: */@Configuration@EnableKafkapublic class ReceiverConfig &#123; @Value(\"$&#123;spring.kafka.bootstrap-servers&#125;\") private String bootstrapServers; @Value(\"$&#123;app.group.id&#125;\") private String groupId; @Value(\"$&#123;kafka.client.truststore&#125;\") private String kafkaClientTruststore; @Value(\"$&#123;app.topic.test&#125;\") private String topic; @Bean public Map&lt;String, Object&gt; consumerConfigs() &#123; Map&lt;String, Object&gt; props = new HashMap&lt;&gt;(); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class); props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId); props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG,kafkaClientTruststore); props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG,\"KafkaOnsClient\"); props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG,\"SASL_SSL\"); props.put(SaslConfigs.SASL_MECHANISM,\"ONS\"); props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,false); props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG,1000); props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG,50); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,\"latest\"); return props; &#125; @Bean public ConsumerFactory&lt;String, String&gt; consumerFactory() &#123; return new DefaultKafkaConsumerFactory&lt;&gt;(consumerConfigs()); &#125; @Bean public ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; kafkaListenerContainerFactory() &#123; ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; factory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;(); factory.setConsumerFactory(consumerFactory()); factory.setBatchListener(true); factory.setAutoStartup(true); factory.setAckDiscarded(true); factory.setConcurrency(1); return factory; &#125;&#125; 以下是消费代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package com.xxx.xxxxxx.kafka;import com.alibaba.fastjson.JSONObject;import com.xxx.xxxxxx.common.Constants;import com.xxx.xxxxxx.entry.vo.TrendInfoVO;import lombok.extern.slf4j.Slf4j;import org.apache.commons.lang3.StringUtils;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Value;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.kafka.annotation.KafkaListener;import org.springframework.kafka.listener.KafkaDataListener;import org.springframework.kafka.listener.MessageListener;import org.springframework.kafka.support.KafkaHeaders;import org.springframework.messaging.handler.annotation.Header;import org.springframework.messaging.handler.annotation.Payload;import org.springframework.scheduling.annotation.Async;import org.springframework.scheduling.annotation.EnableAsync;import org.springframework.stereotype.Service;import java.util.Calendar;import java.util.List;/** * @Author ：yaxuSong * @Description: * @Date: 17:09 2018/4/24 * @Modified by: */@Service@EnableAsync@Slf4jpublic class TrendConsumer&#123; @Autowired private RedisTemplate&lt;String, String&gt; redisTemplate; static Long timestart = System.currentTimeMillis(); static int count = 0; @Async(\"kafkaExecutor\") @KafkaListener(id = \"list\", topics = \"$&#123;app.topic.test&#125;\") public void receive(@Payload List&lt;String&gt; messages, @Header(KafkaHeaders.RECEIVED_PARTITION_ID) List&lt;Integer&gt; partitions, @Header(KafkaHeaders.OFFSET) List&lt;Long&gt; offsets) &#123; for (String str :messages) &#123; doRedisTask(str); count++; if (System.currentTimeMillis()-timestart&gt;=10000) &#123; log.info(\"时间：&#123;&#125;kafka消费数量：&#123;&#125;\", Calendar.getInstance().getTime(),count); count = 0; timestart = System.currentTimeMillis(); &#125; &#125; &#125; @Async(\"redisExecutor\") public void doRedisTask(String message)&#123; JSONObject json = JSONObject.parseObject(message); String exchange = json.getString(\"exchange\"); String symbol = json.getString(\"symbol\"); redisTemplate.opsForValue().set(Constants.KAFKA_TRADE_PREFIX+exchange+symbol,message); &#125;&#125; 这里使用了异步线程去消费。提高消费的速度。 四、特别注意 注意以下几点配置 123456789props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,false);props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG,1000);props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG,50);props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,\"latest\");factory.setBatchListener(true);actory.setConcurrency(1); 五、参考地址 阿里云kafka订阅者最佳实践:https://help.aliyun.com/document_detail/68166.html?spm=a2c4g.11186623.6.566.nnWMLo Spring KAFKA地址：https://docs.spring.io/spring-kafka/docs/2.1.5.RELEASE/reference/html/","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://weafteam.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://weafteam.github.io/tags/JAVA/"}]},{"title":"chapter-08-AIR","slug":"2018-04-30/chapter-08-AIR","date":"2018-05-06T03:30:48.000Z","updated":"2018-05-14T07:52:18.874Z","comments":true,"path":"posts/839e2740/","link":"","permalink":"http://weafteam.github.io/posts/839e2740/","excerpt":"","text":"TensorFlow Linear Regression 2 回顾一下线性回归的loss函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsdef run(): ops.reset_default_graph() sess = tf.Session() batch_size = 100 x_vals = np.linspace(0, 10, 100) y_vals = x_vals + np.random.normal(0, 1, 100) x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32) y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32) # Create variables for linear regression A = tf.Variable(tf.random_normal(shape=[1, 1])) b = tf.Variable(tf.random_normal(shape=[1, 1])) # Declare model operations model_output = tf.add(tf.matmul(x_data, A), b) loss_l1 = tf.reduce_mean(tf.abs(y_target - model_output)) # Declare optimizers my_opt_l1 = tf.train.GradientDescentOptimizer(0.0001) train_step_l1 = my_opt_l1.minimize(loss_l1) # Initialize variables init = tf.global_variables_initializer() sess.run(init) loss_vec_l1 = [] for i in range(1000): rand_index = np.random.choice(len(x_vals), size=batch_size) rand_x = np.transpose([x_vals[rand_index]]) rand_y = np.transpose([y_vals[rand_index]]) sess.run(train_step_l1, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) temp_loss_l1 = sess.run(loss_l1, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) loss_vec_l1.append(temp_loss_l1) if (i + 1) % 25 == 0: print('Step #' + str(i + 1) + ' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b))) ops.reset_default_graph() # Create graph sess = tf.Session() x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32) y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32) # Create variables for linear regression A = tf.Variable(tf.random_normal(shape=[1, 1])) b = tf.Variable(tf.random_normal(shape=[1, 1])) # Declare model operations model_output = tf.add(tf.matmul(x_data, A), b) loss_l2 = tf.reduce_mean(tf.square(y_target - model_output)) # Declare optimizers my_opt_l2 = tf.train.GradientDescentOptimizer(0.0001) train_step_l2 = my_opt_l2.minimize(loss_l2) # Initialize variables init = tf.global_variables_initializer() sess.run(init) loss_vec_l2 = [] for i in range(1000): rand_index = np.random.choice(len(x_vals), size=batch_size) rand_x = np.transpose([x_vals[rand_index]]) rand_y = np.transpose([y_vals[rand_index]]) sess.run(train_step_l2, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) temp_loss_l2 = sess.run(loss_l2, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) loss_vec_l2.append(temp_loss_l2) if (i + 1) % 25 == 0: print('Step #' + str(i + 1) + ' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b))) plt.plot(loss_vec_l1, 'k-', label='L1 Loss') plt.plot(loss_vec_l2, 'r--', label='L2 Loss') plt.title('L1 and L2 Loss per Generation') plt.xlabel('Generation') plt.ylabel('L1 Loss') plt.legend(loc='upper right') plt.show()def main(_): run()if __name__ == '__main__': tf.app.run() 戴明回归: 看图：找不同： loss函数是关键:下面是deming regression的loss函数 \\[ \\frac{\\mid{A*x+b-y}\\mid}{\\sqrt{A^2 + 1}} \\] 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2018/5/6 14:32# @Author : milittle# @Site : www.weaf.top# @File : deming_lr.py# @Software: PyCharmimport matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()def run(): x_vals = np.linspace(0, 10, 100) y_vals = x_vals + np.random.normal(0, 1, 100) batch_size = 125 x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32) y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32) A = tf.Variable(tf.random_normal(shape=[1, 1])) b = tf.Variable(tf.random_normal(shape=[1, 1])) model_output = tf.add(tf.matmul(x_data, A), b) # 注意这里的loss函数的求解 demming_numerator = tf.abs(tf.subtract(tf.add(tf.matmul(x_data, A), b), y_target)) demming_denominator = tf.sqrt(tf.add(tf.square(A), 1)) loss = tf.reduce_mean(tf.truediv(demming_numerator, demming_denominator)) my_opt = tf.train.GradientDescentOptimizer(0.01) train_step = my_opt.minimize(loss) # Initialize variables init = tf.global_variables_initializer() sess.run(init) loss_vec = [] for i in range(1500): rand_index = np.random.choice(len(x_vals), size=batch_size) rand_x = np.transpose([x_vals[rand_index]]) rand_y = np.transpose([y_vals[rand_index]]) sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) temp_loss = sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) loss_vec.append(temp_loss) if (i + 1) % 100 == 0: print(&apos;Step #&apos; + str(i + 1) + &apos; A = &apos; + str(sess.run(A)) + &apos; b = &apos; + str(sess.run(b))) print(&apos;Loss = &apos; + str(temp_loss)) [W] = sess.run(A) [bias] = sess.run(b) # Get best fit line best_fit = [] for i in x_vals: best_fit.append(W * i + bias) plt.plot(x_vals, y_vals, &apos;o&apos;, label=&apos;Data Points&apos;) plt.plot(x_vals, best_fit, &apos;r-&apos;, label=&apos;Best fit line&apos;, linewidth=3) plt.legend(loc=&apos;upper left&apos;) plt.title(&apos;Sepal Length vs Pedal Width&apos;) plt.xlabel(&apos;Pedal Width&apos;) plt.ylabel(&apos;Sepal Length&apos;) plt.show() # Plot loss over time plt.plot(loss_vec, &apos;k-&apos;) plt.title(&apos;Demming Loss per Generation&apos;) plt.xlabel(&apos;Iteration&apos;) plt.ylabel(&apos;Demming Loss&apos;) plt.show()def main(_): run()if __name__ == &apos;__main__&apos;: tf.app.run() LASSO and Ridge Regression(关键的地方还是loss函数的不同，其他步骤是一致的) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2018/5/6 14:46# @Author : milittle# @Site : www.weaf.top# @File : LASSO_Ridge_lr.py# @Software: PyCharmimport matplotlib.pyplot as pltimport sysimport numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()regression_type = 'LASSO'def run(): sess = tf.Session() x_vals = np.linspace(0, 10, 100) y_vals = x_vals + np.random.normal(0, 1, 100) batch_size = 125 x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32) y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32) seed = 13 np.random.seed(seed) tf.set_random_seed(seed) A = tf.Variable(tf.random_normal(shape=[1, 1])) b = tf.Variable(tf.random_normal(shape=[1, 1])) model_output = tf.add(tf.matmul(x_data, A), b) if regression_type == 'LASSO': lasso_param = tf.constant(0.9) heavyside_step = tf.truediv(1., tf.add(1., tf.exp(tf.multiply(-50., tf.subtract(A, lasso_param))))) regularization_param = tf.multiply(heavyside_step, 99.) loss = tf.add(tf.reduce_mean(tf.square(y_target - model_output)), regularization_param) elif regression_type == 'Ridge': ridge_param = tf.constant(1.) ridge_loss = tf.reduce_mean(tf.square(A)) loss = tf.expand_dims( tf.add(tf.reduce_mean(tf.square(y_target - model_output)), tf.multiply(ridge_param, ridge_loss)), 0) else: print('Invalid regression_type parameter value', file=sys.stderr) my_opt = tf.train.GradientDescentOptimizer(0.001) train_step = my_opt.minimize(loss) init = tf.global_variables_initializer() sess.run(init) # Training loop loss_vec = [] for i in range(1500): rand_index = np.random.choice(len(x_vals), size=batch_size) rand_x = np.transpose([x_vals[rand_index]]) rand_y = np.transpose([y_vals[rand_index]]) sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) temp_loss = sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) loss_vec.append(temp_loss[0]) if (i + 1) % 300 == 0: print('Step #' + str(i + 1) + ' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b))) print('Loss = ' + str(temp_loss)) print('\\n') [W] = sess.run(A) [bias] = sess.run(b) # Get best fit line best_fit = [] for i in x_vals: best_fit.append(W * i + bias) # Plot the result plt.plot(x_vals, y_vals, 'o', label='Data Points') plt.plot(x_vals, best_fit, 'r-', label='Best fit line', linewidth=3) plt.legend(loc='upper left') plt.title('Sepal Length vs Pedal Width') plt.xlabel('Pedal Width') plt.ylabel('Sepal Length') plt.show() # Plot loss over time plt.plot(loss_vec, 'k-') plt.title(regression_type + ' Loss per Generation') plt.xlabel('Generation') plt.ylabel('Loss') plt.show()def main(_): run()if __name__ == '__main__': tf.app.run() Elastic Net Regression(利用多个loss函数的叠加进行训练)弹性的方式 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2018/5/6 14:57# @Author : milittle# @Site : www.weaf.top# @File : Elastic_Net_Regression.py# @Software: PyCharmimport matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.python.framework import opssess = tf.Session()x_vals = np.linspace(0, 10, 100)y_vals = x_vals + np.random.normal(0, 1, 100)batch_size = 16seed = 13np.random.seed(seed)tf.set_random_seed(seed)x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32, name='input')y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32, name='output')A = tf.Variable(tf.random_normal(shape=[1, 1]))b = tf.Variable(tf.random_normal(shape=[1, 1]))model_output = tf.add(tf.matmul(x_data, A), b)elastic_param1 = tf.constant(1.)elastic_param2 = tf.constant(1.)l1_a_loss = tf.reduce_mean(tf.abs(A))l2_a_loss = tf.reduce_mean(tf.square(A))e1_term = tf.multiply(elastic_param1, l1_a_loss)e2_term = tf.multiply(elastic_param2, l2_a_loss)loss = tf.expand_dims(tf.add(tf.add(tf.reduce_mean(tf.square(y_target - model_output)), e1_term), e2_term), 0)my_opt = tf.train.GradientDescentOptimizer(0.001)train_step = my_opt.minimize(loss)init = tf.global_variables_initializer()sess.run(init)# Training looploss_vec = []for i in range(1000): rand_index = np.random.choice(len(x_vals), size=batch_size) rand_x = np.transpose([x_vals[rand_index]]) rand_y = np.transpose([y_vals[rand_index]]) sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) temp_loss = sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) loss_vec.append(temp_loss[0]) if (i + 1) % 250 == 0: print('Step #' + str(i + 1) + ' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b))) print('Loss = ' + str(temp_loss))W = sess.run(A)bias = sess.run(b)plt.plot(loss_vec, 'k-')plt.title('Loss per Generation')plt.xlabel('Generation')plt.ylabel('Loss')plt.show() 好，今天到此把一些线性模型的应用问题大体讲完了，还是有什么问题，可以积极讨论，可以给我发邮件air@weaf.top，上面的代码由于很简单，所以我想大家都可以看懂的。其实这么多类型的回归问题，总的来说就是loss函数不一样，只要将loss函数理解了，那么问题就迎刃而解。","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]},{"title":"chapter-07-AIR","slug":"2018-04-23/chapter-07-AIR","date":"2018-05-06T03:29:55.000Z","updated":"2018-05-14T07:52:18.874Z","comments":true,"path":"posts/1ceb091/","link":"","permalink":"http://weafteam.github.io/posts/1ceb091/","excerpt":"","text":"TensorFlow Linear Regression 1 亲爱的小伙伴们，上周又事情给耽搁了，这周将上周的内容一起补充一下。我们前面的内容将TensorFlow的基础内容都介绍了，所以接下来我们需要实现一些基本的算法，如果大家想跑一些现在主流的一些网络结构，大家可以移步到我的GitHub MLModel这个仓库MLModel，里面会定期更新一些主流的网络框架。喜欢的话，给个star可好。接下来呢我们开始我们今天的线性回归模型的各种求解方法。 在TensorFlow中使用矩阵的逆来解决线性模型，这个解决方案，相比学过线性代数的你，都会解。 大家可曾记得线性代数里面的线性方程组： \\[ A * x + b = y \\] x就是我们要解的未知解，那x该怎么解来着？ \\[ x = {(A^T * A)}^{-1} * A^T * y - b \\] 上面的公式一下就能看出来是怎么回事对不对？ 推导过程： \\[ 第一步:A^T * A * x = A^T*y-b\\\\第二步：{(A^T*A)}^{-1} *{(A^T*A)} * x = {(A^T*A)}^{-1} * A^T*y-b\\\\第三步：左边是不是就是一个单位矩阵了？\\\\得到最后的结果是：x = {(A^T * A)}^{-1} * A^T * y-b \\] 我们下面使用的A的特征维度是一维，(再加上b这个维度)这是为了可以可视化结果：在直接利用矩阵运算而得到的解对于这些数据来说，是最好的结果。也是唯一解。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()def run(): # 构造数据 x_vals = np.linspace(0, 10, 100) y_vals = x_vals + np.random.normal(0, 1, 100) x_vals_column = np.transpose(np.matrix(x_vals)) ones_column = np.transpose(np.matrix(np.repeat(1, 100))) A = np.column_stack((x_vals_column, ones_column)) y = np.transpose(np.matrix(y_vals)) A_tensor = tf.constant(A) y_tensor = tf.constant(y) #利用矩阵的逆解决这个线性问题。 t_A_A = tf.matmul(tf.transpose(A_tensor), A_tensor) #求矩阵转置和本身的乘积 t_A_A_inverse = tf.matrix_inverse(t_A_A) # 求矩阵的逆 product = tf.matmul(t_A_A_inverse, tf.transpose(A_tensor)) solution = tf.matmul(product, y_tensor) solution_eval = sess.run(solution) W = solution_eval[0][0] bias = solution_eval[1][0] print('W: ' + str(W)) print('bias: ' + str(bias)) # Get best fit line best_fit = [] for i in x_vals: best_fit.append(W * i + bias) plt.plot(x_vals, y_vals, 'o', label = 'data') plt.plot(x_vals, best_fit, 'r-', label = 'best fit line', linewidth = 3) plt.legend(loc='upper left') plt.show()def main(_): run()if __name__ == '__main__': tf.app.run() 其实大家也可以看出，我们使用矩阵的逆去求线性模型是又快又准，但是你们想过没有为啥神经网络里面在求解线性回归的时候还要用到反向传播梯度下降的方式呢，给你们举个例子，假设现在一个样本有上万或者十万个特征维度，你想想我们在求解矩阵逆的时候要花费多长时间，那是你意想不到的，所以才会使用方向传播算法去解决W的求解问题。详细需要花费的时间。 第二种求解方式：Cholesky Method \\[ A*x=y \\] 思路如下：在Cholesky method方法中，我们需要将A分解为L和L的转置的乘积，然后再进行x的求解，为什么要这么做呢?当然是为了避免求矩阵的逆，它很耗费时间。 因为A要求在求解Cholesky Depcomposition的时候是square也就是方阵。但是一般的A都不是方阵，那么我们就构造出来一个方阵： \\[ A^T*A \\] 它就是一个方阵，那么我们就求A转置和A乘积的Cholesky Decomposition。 首先明确分解步骤： \\[ 第一步：A^T*A=L^T*L\\\\第二步：L^T*L*x=A^T*Y\\\\第三步：L^T*z=A^T*Y\\\\where z = L*x \\] 其次明确求解步骤： \\[ 第一步：计算A的Cholesky Decomposition\\\\where A^T*A=L^T*L\\\\第二步：求解z，利用L^T*z=A^T*y这个公式\\\\第三步：求解x，利用L*x=z这个公式 \\] 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()def run(): # 构造数据 x_vals = np.linspace(0, 10, 100) y_vals = x_vals + np.random.normal(0, 1, 100) x_vals_column = np.transpose(np.matrix(x_vals)) ones_column = np.transpose(np.matrix(np.repeat(1, 100))) A = np.column_stack((x_vals_column, ones_column)) y = np.transpose(np.matrix(y_vals)) A_tensor = tf.constant(A) y_tensor = tf.constant(y) tA_A = tf.matmul(tf.transpose(A_tensor), A_tensor) L = tf.cholesky(tA_A) tA_y = tf.matmul(tf.transpose(A_tensor), y) sol1 = tf.matrix_solve(L, tA_y) sol2 = tf.matrix_solve(tf.transpose(L), sol1) solution_eval = sess.run(sol2) W = solution_eval[0][0] bias = solution_eval[1][0] print('slope: ' + str(W)) print('y_intercept: ' + str(bias)) best_fit = [] for i in x_vals: best_fit.append(W * i + bias) plt.plot(x_vals, y_vals, 'o', label='Data') plt.plot(x_vals, best_fit, 'r-', label='Best fit line', linewidth=3) plt.legend(loc='upper left') plt.show()def main(_): run()if __name__ == '__main__': tf.app.run() 使用反向传播梯度下降的方式求解线性模型。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()def run(): # 构造数据 batch_size = 32 x_vals = np.linspace(0, 10, 100) y_vals = x_vals + np.random.normal(0, 1, 100) x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32) y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32) A = tf.Variable(tf.random_normal(shape=[1, 1])) b = tf.Variable(tf.random_normal(shape=[1, 1])) model_output = tf.add(tf.matmul(x_data, A), b) loss = tf.reduce_mean(tf.square(y_target - model_output)) my_opt = tf.train.GradientDescentOptimizer(0.00001) train_step = my_opt.minimize(loss) sess.run(tf.global_variables_initializer()) loss_vec = [] for i in range(10000): rand_index = np.random.choice(len(x_vals), size = batch_size) rand_x = np.transpose([x_vals[rand_index]]) rand_y = np.transpose([y_vals[rand_index]]) sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) temp_loss = sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) loss_vec.append(temp_loss) if (i + 1) % 25 == 0: print('Step #' + str(i + 1) + ' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b))) print('Loss = ' + str(temp_loss)) [W] = sess.run(A) [b] = sess.run(b) best_fit = [] for i in x_vals: best_fit.append(W * i + b) plt.plot(x_vals, y_vals, 'o', label='Data Points') plt.plot(x_vals, best_fit, 'r-', label='Best fit line', linewidth=3) plt.legend(loc='upper left') plt.title('Sepal Length vs Pedal Width') plt.xlabel('Pedal Width') plt.ylabel('Sepal Length') plt.show() plt.plot(loss_vec, 'b-') plt.title('L2 Loss per Generation') plt.xlabel('Generation') plt.ylabel('L2 Loss') plt.show()def main(_): run()if __name__ == '__main__': tf.app.run() 这三种方式求解的线性模型，大家都熟悉一下。有什么不确定的地方，可以发邮件给我air@weaf.top。上面的方式其实在现实里面都可以使用，只不过前两种方法，在数据维度较大的时候，计算耗时。所以才会使用梯度下降的去求最优解。这次的文章就到这里。如果哪里表述不清的地方或者错误的地方，还请大家指出。","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]},{"title":"asyncio 不完全指北（四）","slug":"2018-04-30/guide-to-asyncio-4","date":"2018-05-04T14:32:16.000Z","updated":"2018-08-07T13:19:42.441Z","comments":true,"path":"posts/7eb3a479/","link":"","permalink":"http://weafteam.github.io/posts/7eb3a479/","excerpt":"","text":"书接上文。 同步原语 虽然使用 asyncio 的程序通常都以单线程运行，但仍然可以作为并发程序。每个协程或任务可以根据来自 I / O 或其他外部事件的延迟和中断以不可预测的顺序执行。为了支持安全并发，和 threading和 multiprocessing 模块一样，asyncio 包含了一些相同的低级原语的实现。 锁 锁对共享资源的访问提供了保护。只有锁的持有者才能使用资源。第二次及以上获取锁的尝试将被阻止，因此每次只有一个持有者： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import asyncioimport functoolsdef unlock(lock): print('callback releasing lock') lock.release()async def coro1(lock): print('`coro1` waiting for the lock') with await lock: print('`coro1` acquired lock') print('`coro1` released lock')async def coro2(lock): print('`coro2` waiting for the lock') await lock try: print('`coro2` acquired lock') finally: print('`coro2` released lock') lock.release()async def main(loop): # 创建并持有一个锁 lock = asyncio.Lock() print('acquiring the lock before starting coroutines') await lock.acquire() print(f'lock acquired: &#123;lock.locked()&#125;') # 安排一个回调释放锁 loop.call_later(0.1, functools.partial(unlock, lock)) # 运行希望持有锁的协程 print('waiting for coroutines') await asyncio.wait([coro1(lock), coro2(lock)])event_loop = asyncio.get_event_loop()try: event_loop.run_until_complete(main(event_loop))finally: event_loop.close() 可以使用 await 持有一个锁，并用 release() 释放，就像coro2() 的做法一样；同时也可以像 coro1() 一样，用带有 await 的异步上下文处理器来持有并释放一个锁： 12345678910acquiring the lock before starting coroutineslock acquired: Truewaiting for coroutines`coro1` waiting for the lock`coro2` waiting for the lockcallback releasing lock`coro1` acquired lock`coro1` released lock`coro2` acquired lock`coro2` released lock Event asyncio.Event 与 threading.Event 类似，用于允许多个协程等待某个事件发生，而不需要监听一个特定值来实现类似通知的功能： 123456789101112131415161718192021222324252627282930313233343536import asyncioimport functoolsdef set_event(event): print('setting event in callback') event.set()async def coro1(event): print('coro1 waiting for event') await event.wait() print('coro1 triggered')async def coro2(event): print('coro2 waiting for event') await event.wait() print('coro2 triggered')async def main(loop): event = asyncio.Event() print(f'event start state: &#123;event.is_set()&#125;') loop.call_later(0.1, functools.partial(set_event, event)) await asyncio.gather(coro1(event), coro2(event)) print(f'event end state: &#123;event.is_set()&#125;')event_loop = asyncio.get_event_loop()try: event_loop.run_until_complete(main(event_loop))finally: event_loop.close() 与锁一样，coro1() 和 coro2() 都会等待 event 被设置。不同之处在于，它们可以在 event 状态发生变化时立即启动，并且它们不需要获取 event 对象的唯一使用权： 1234567event start state: Falsecoro2 waiting for eventcoro1 waiting for eventsetting event in callbackcoro2 triggeredcoro1 triggeredevent end state: True Condition Condition的作用类似于 Event，不同之处在于，Condition 不会唤醒所有等待中的协程，唤醒的数量由 notify() 的参数控制： 1234567891011121314151617181920212223242526272829303132333435363738394041424344import asyncioasync def consumer(condition, n): with await condition: print(f'consumer &#123;n&#125; is waiting') await condition.wait() print(f'consumer &#123;n&#125; triggered') print(f'ending consumer &#123;n&#125;')async def manipulate_condition(condition): print('starting manipulate_condition') await asyncio.sleep(0.1) for i in range(1, 3): with await condition: print(f'notifying &#123;i&#125; consumers') condition.notify(n=i) await asyncio.sleep(0.1) with await condition: print('notifying remaining consumers') condition.notify_all() print('ending manipulate_condition')async def main(loop): condition = asyncio.Condition() consumers = [consumer(condition, i) for i in range(5)] loop.create_task(manipulate_condition(condition)) await asyncio.gather(*consumers)event_loop = asyncio.get_event_loop()try: result = event_loop.run_until_complete(main(event_loop))finally: event_loop.close() 这个例子中启动了五个 Condition 的消费者。每个都使用 wait() 方法等待通知它们继续的消息。manipulate_condition() 通知一个消费者，然后通知两个消费者，最后通知所有剩余的消费者： 1234567891011121314151617181920starting manipulate_conditionnotifying 1 consumersconsumer 2 is waitingconsumer 3 is waitingconsumer 0 is waitingconsumer 4 is waitingconsumer 1 is waitingnotifying 2 consumersconsumer 2 triggeredending consumer 2consumer 3 triggeredending consumer 3notifying remaining consumersending manipulate_conditionconsumer 0 triggeredending consumer 0consumer 4 triggeredending consumer 4consumer 1 triggeredending consumer 1 Queue asyncio.Queue 为协程提供了一个先进先出的数据结构，类似于与多线程中的 queue.Queue，多进程中的 multiprocessing.Queue： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import asyncioasync def consumer(n, q): print(f'consumer &#123;n&#125;: starting') while True: print(f'consumer &#123;n&#125;: waiting for item') item = await q.get() print(f'consumer &#123;n&#125;: has item &#123;item&#125;') # None 表示终止信号 if item is None: q.task_done() break else: await asyncio.sleep(0.01 * item) q.task_done() print(f'consumer &#123;n&#125;: ending')async def producer(q, num_workers): print('producer: starting') # 向队列中添加一些数据 for i in range(num_workers * 3): await q.put(i) print(f'producer: added task &#123;i&#125; to the queue') # 传入终止信号 print('producer: adding stop signals to the queue') for i in range(num_workers): await q.put(None) print('producer: waiting for queue to empty') await q.join() print('producer: ending')async def main(loop, num_consumers): # 创建指定大小的队列 # 超过队列大小时生产者会阻塞，直到有消费者取出数据 q = asyncio.Queue(maxsize=num_consumers) # 调度消费者 consumers = [loop.create_task(consumer(i, q)) for i in range(num_consumers)] # 调度生产者 prod = loop.create_task(producer(q, num_consumers)) # 等待所有任务完成 await asyncio.gather(*consumers, prod)event_loop = asyncio.get_event_loop()try: event_loop.run_until_complete(main(event_loop, 2))finally: event_loop.close() 使用 put() 添加项或使用 get() 获取并删除项都是异步操作，因为队列大小可能是固定的（阻塞添加操作），或者队列可能是空的（阻塞获取项的操作）： 123456789101112131415161718192021222324252627282930consumer 0: startingconsumer 0: waiting for itemconsumer 1: startingconsumer 1: waiting for itemproducer: startingproducer: added task 0 to the queueproducer: added task 1 to the queueconsumer 0: has item 0consumer 1: has item 1producer: added task 2 to the queueproducer: added task 3 to the queueconsumer 0: waiting for itemconsumer 0: has item 2producer: added task 4 to the queueconsumer 1: waiting for itemconsumer 1: has item 3producer: added task 5 to the queueproducer: adding stop signals to the queueconsumer 0: waiting for itemconsumer 0: has item 4consumer 1: waiting for itemconsumer 1: has item 5producer: waiting for queue to emptyconsumer 0: waiting for itemconsumer 0: has item Noneconsumer 0: endingconsumer 1: waiting for itemconsumer 1: has item Noneconsumer 1: endingproducer: ending 用 Protocol 抽象类实现异步 I / O 到目前为止，这些示例都避免了将并发和 I / O 操作混合在一起，一次只关注一个概念。但是，在 I / O 阻塞时切换上下文是 asyncio 的主要使用情形之一。在已经介绍的并发概念的基础上，本节将实现简单的 echo 服务器程序和客户端程序。客户端可以连接到服务器，发送一些数据，然后接收与响应相同的数据。每次启动 I / O 操作时，执行代码都会放弃对事件循环的控制，从而允许其他任务运行，直到 I / O 操作就绪。 Echo 服务器 服务器首先导入所需的 asyncio 和 logging 模块，然后创建事件循环对象： 1234567891011121314import asyncioimport loggingimport sysSERVER_ADDRESS = ('localhost', 10000)logging.basicConfig( level=logging.DEBUG, format='%(name)s: %(message)s', stream=sys.stderr,)log = logging.getLogger('main')event_loop = asyncio.get_event_loop() 然后定义了一个 asyncio.Protocol 的子类，用来处理与客户端的通信。Protocol 对象的方法是基于与服务器 socket 关联的事件调用的： 1class EchoServer(asyncio.Protocol): 每个新的客户端连接都会触发对 connection_made() 的调用。transport 参数是asyncio.Transport 的实例，它提供了使用 socket 进行异步 I / O 的抽象。不同类型的通信提供不同的 tansport 实现，所有这些实现都具有相同的 API。例如，有单独的 transport 类用于与 socket 通信、与子进程通过管道通信。传入客户端的地址可以通过 transport 的 get_extra_info() 获取，这是一种特定于实现的方法： 12345def connection_made(self, transport): self.transport = transport self.address = transport.get_extra_info('peername') self.log = logging.getLogger('EchoServer_&#123;&#125;_&#123;&#125;'.format(*self.address)) self.log.debug('connection accepted') 建立连接后，当数据从客户端发送到服务器时，将调用协议的 data_received() 方法将数据传入以进行处理。数据以字节串的形式传递，由应用程序以适当的方式对其进行解码。在这里记录结果，然后通过调用 transport.write() 立即将响应发送回客户端： 1234def data_received(self, data): self.log.debug('received &#123;!r&#125;'.format(data)) self.transport.write(data) self.log.debug('sent &#123;!r&#125;'.format(data)) 某些 transport 支持特殊的文件结束标识符（EOF）。遇到 EOF 时，将调用 eof_received() 方法。在这个实现中，EOF 被发送回客户端来表示它已被接收。由于并非所有 transport 都支持显式 EOF，因此 protocol 首先询问 transport 发送 EOF 是否安全： 1234def eof_received(self): self.log.debug('received EOF') if self.transport.can_write_eof(): self.transport.write_eof() 当连接关闭时，无论是正常关闭还是错误关闭，都会调用 protocol 的 connection_lost() 方法。如果发生错误，参数会包含适当的异常对象，否则为 None： 123456def connection_lost(self, error): if error: self.log.error(f'ERROR: &#123;error&#125;') else: self.log.debug('closing') super().connection_lost(error) 启动服务器有两个步骤。首先，应用程序告诉事件循环要使用的 protocol 类以及要侦听的主机名和 socket，用来创建新的服务器对象。create_server() 方法是协程，因此必须由事件循环处理结果，才能真正的启动服务器。然后，协程完成后产生了一个绑定到事件循环的 asyncio.Server 实例： 123factory = event_loop.create_server(EchoServer, *SERVER_ADDRESS)server = event_loop.run_until_complete(factory)log.debug('starting up on &#123;&#125; port &#123;&#125;'.format(*SERVER_ADDRESS)) 然后，需要运行事件循环以处理事件和客户端请求。对于长期运行的服务，run_forever() 方法是最简单的方法。当事件循环停止时，无论是通过应用程序代码还是通过发信号通知进程，服务器都可以关闭，以便正确清理 socket，然后可以关闭事件循环，以便在程序退出之前完成对任何其他事务的处理： 12345678try: event_loop.run_forever()finally: log.debug('closing server') server.close() event_loop.run_until_complete(server.wait_closed()) log.debug('closing event loop') event_loop.close() Echo 客户端 使用 protocol 类构造客户端非常类似于构造服务器。首先导入所需的 asyncio 和 logging 模块，然后创建事件循环对象： 1234567891011121314151617181920import asyncioimport functoolsimport loggingimport sysMESSAGES = [ b'This is the message. ', b'It will be sent ', b'in parts.',]SERVER_ADDRESS = ('localhost', 10000)logging.basicConfig( level=logging.DEBUG, format='%(name)s: %(message)s', stream=sys.stderr,)log = logging.getLogger('main')event_loop = asyncio.get_event_loop() 客户端 protocol 类定义了与服务器相同的方法，但实现方式不同。类构造函数接受两个参数，一个是要发送的消息列表，另一个是 future 的实例，用于通过接收来自服务器的响应来表明客户端已经完成了一个工作周期： 1234567class EchoClient(asyncio.Protocol): def __init__(self, messages, future): super().__init__() self.messages = messages self.log = logging.getLogger('EchoClient') self.f = future 当客户端成功连接到服务器时，它将立即开始通信。消息序列一次发送一条，尽管底层网络代码可以将多个消息组合成一个传输。当所有消息都用尽时，将发送 EOF。 虽然看起来数据都是立即发送的，但实际上 transport 对象缓冲传出的数据，并在当 socket 的缓冲区准备好接收数据时设置回调来进行实际的传输。所有这些都是透明处理的，因此可以编写应用程序代码，就好像 I / O 操作正在立即发生一样： 123456789101112def connection_made(self, transport): self.transport = transport self.address = transport.get_extra_info('peername') self.log.debug('connecting to &#123;&#125; port &#123;&#125;'.format(*self.address)) # 这里可以是 transport.writelines() # 但这会使显示要发送的消息的每个部分变得更加困难 for msg in self.messages: transport.write(msg) self.log.debug(f'sending &#123;msg!r&#125;') if transport.can_write_eof(): transport.write_eof() 收到来自服务器的响应时，将记录该响应： 12def data_received(self, data): self.log.debug(f'received &#123;data!r&#125;') 当从服务器端接收到 EOF 或者连接被关闭时，本地 transport 对象被关闭，并通过设置结果将 future 对象标记为完成： 123456789101112def eof_received(self): self.log.debug('received EOF') self.transport.close() if not self.f.done(): self.f.set_result(True)def connection_lost(self, exc): self.log.debug('server closed connection') self.transport.close() if not self.f.done(): self.f.set_result(True) super().connection_lost(exc) 通常，protocol 类被传递到事件循环以创建连接。在这种情况下，由于事件循环没有向 protocol 构造函数传递额外参数的工具，因此需要 functools.partial() 来包装客户端类，并传递要发送的消息列表和 future 的实例。然后，在调用 create_connection() 建立客户端连接时，将使用该新的可调用对象代替 protocol 类： 1234567891011client_completed = asyncio.Future()client_factory = functools.partial( EchoClient, messages=MESSAGES, future=client_completed,)factory_coroutine = event_loop.create_connection( client_factory, *SERVER_ADDRESS,) 为了触发客户端运行，事件循环将调用一次创建客户端的协程，然后再调用一次指定给客户端的 future 实例，以便在完成后进行通信。使用这样的两个调用避免了客户端程序中的无限循环，客户端程序可能希望在完成与服务器的通信后退出。如果仅使用第一个调用来等待协程创建客户端，那它可能无法处理所有响应数据并正确清理与服务器的连接： 1234567log.debug('waiting for client to complete')try: event_loop.run_until_complete(factory_coroutine) event_loop.run_until_complete(client_completed)finally: log.debug('closing event loop') event_loop.close() 输出 在一个窗口中运行服务器而在另一个窗口中运行客户端。 客户端将产生以下输出： 12345678910asyncio: Using selector: KqueueSelectormain: waiting for client to completeEchoClient: connecting to ::1 port 10000EchoClient: sending b'This is the message. 'EchoClient: sending b'It will be sent 'EchoClient: sending b'in parts.'EchoClient: received b'This is the message. It will be sent in parts.'EchoClient: received EOFEchoClient: server closed connectionmain: closing event loop 虽然客户端总是单独发送消息，但客户端第一次运行时，服务器会收到一条大消息，并将该消息返回给客户端。根据网络的繁忙程度以及是否在准备所有数据之前刷新网络缓冲区，这些结果在后续运行中会有所不同： 12345678asyncio: Using selector: KqueueSelectormain: starting up on localhost port 10000EchoServer_::1_55307: connection acceptedEchoServer_::1_55307: received b'This is the message. It will be sent in parts.'EchoServer_::1_55307: sent b'This is the message. It will be sent in parts.'EchoServer_::1_55307: received EOFEchoServer_::1_55307: closing 1234567EchoServer_::1_55309: connection acceptedEchoServer_::1_55309: received b'This is the message. 'EchoServer_::1_55309: sent b'This is the message. 'EchoServer_::1_55309: received b'It will be sent in parts.'EchoServer_::1_55309: sent b'It will be sent in parts.'EchoServer_::1_55309: received EOFEchoServer_::1_55309: closing","categories":[],"tags":[]},{"title":"Java中String、StringBuffer与StringBuilder的区别","slug":"2018-04-23/Java中String、StringBuffer与StringBuilder的区别","date":"2018-05-03T07:39:02.000Z","updated":"2018-05-06T02:35:14.424Z","comments":true,"path":"posts/2870b42a/","link":"","permalink":"http://weafteam.github.io/posts/2870b42a/","excerpt":"","text":"最近准备开始刷牛客网上的题目，为找工作面试做准备，今后我会将其中感觉比较不错的知识点总结出来形成博客，贴出来与大家共同学习，如果其中存在什么问题，也希望大家不吝指教，邮箱地址为well@weaf.top。 下面开始本次的内容整理吧~ String 首先说一下String类的声明,通过查阅源码可知它的声明是public final，所以也就是说我们的字符串的值一旦改变，我们就得再向内存重新申请一块地方存放改变之后的字符串。很显然，String是字符串常量，字符长度不可变。其实这是一件很恐怖的事情，当你需要多次改变时，多次的改变产生的代价不言而喻。 StringBuffer StringBuffer是字符串变量，支持多线程进行字符操作，而且是线程安全的，适合在多线程中使用。原因是在源代码中StringBuilder的很多方法都被关键字synchronized修饰了，这也是StringBuffer和StringBuilder的最大区别的地方。 StringBuilder StringBuilder也是字符串变量，但是区别于StringBuffer，它并不支持并发操作，非线程安全，不适合在多线程中使用，但是其在单线程中的性能比StringBuffer高。 测试 代码 123456789101112131415161718192021222324252627282930313233343536373839public class StringTest &#123; public static final int EchoNum = 10000; public static void TestStr()&#123; String A = new String(\"AAA\"); long StartTime = System.currentTimeMillis(); for(int i = 0;i&lt;EchoNum;i++)&#123; A+=\"A\"; &#125; long EndTime = System.currentTimeMillis(); System.out.println(\"String: \"+ (EndTime-StartTime)+\" millis has been used\"); &#125; public static void TestStrBuffer()&#123; StringBuffer A = new StringBuffer(\"AAA\"); long StartTime = System.currentTimeMillis(); for(int i = 0;i&lt;EchoNum;i++)&#123; A = A.append(\"A\"); &#125; long EndTime = System.currentTimeMillis(); System.out.println(\"StringBuffer: \"+ (EndTime-StartTime)+\" millis has been used\"); &#125; public static void TestStrBuilder()&#123; StringBuilder A = new StringBuilder(\"AAA\"); long StartTime = System.currentTimeMillis(); for(int i = 0;i&lt;EchoNum;i++)&#123; A = A.append(\"A\"); &#125; long EndTime = System.currentTimeMillis(); System.out.println(\"StringBuilder: \"+ (EndTime-StartTime)+\" millis has been used\"); &#125; public static void main(String[] args) &#123; // TODO Auto-generated method stub TestStr(); TestStrBuffer(); TestStrBuilder(); &#125;&#125; 结果 结果就很明显了，String的效率最低下，只考虑单线程的话，推荐使用StringBuilder；但是如果是要保证线程安全的话，肯定要使用StringBuffer。 稍微总结一下： 如果我们需要一个在整个程序中不需要修改的字符串，String是首选。 如果我们需要一个在多线程之间共享的动态字符串，StringBuffer是我们的选择。 如果以上两种情况都不存在，StringBuilder是一个不错的选择。 练手 然后来看一道阿里巴巴的面试题吧，题目内容如下： 题目： (单选题)Java中，StringBuilder和StringBuffer的区别，下面说法错误的是？ A： StringBuffer是线程安全的。 B： StringBuilder是非线程安全的。 C： StringBuffer对String类型进行改变的时候，其实都等同于生成了一个新的String对象，然后将指针指向新的String对象。 D： 效率比较 String &lt; StringBuffer &lt; StringBuilder，但是在 String S1 = “This is only a” + &quot; simple&quot; + &quot; test“时，String的效率最高。 分析： A，B就不用多说了，肯定是正确的。C项一眼其实就能看出来是错误的，StringBuffer和StringBuilder都是字符串变量，并不需要重新申请内存存放新的对象。D项中的后一部分String S1 = “This is only a”+ “Simple”+“Test”，其实这个地方并没有字符串操作处理，只是简单的String S1 = “This is only a simple test”，所以效率最高。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://weafteam.github.io/categories/JAVA/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://weafteam.github.io/tags/面试/"}]},{"title":"微信公众号后台在SpringBoot2.0中的实现（上）","slug":"2018-04-30/wechat-for-java-backend","date":"2018-05-01T10:13:02.000Z","updated":"2018-06-23T11:04:29.817Z","comments":true,"path":"posts/637facd6/","link":"","permalink":"http://weafteam.github.io/posts/637facd6/","excerpt":"","text":"现在微信在中国作为最重要的社交软件，相信很多人都使用微信公众号。 而且微信公众号也成为一些企业传播资讯最好的平台。 那么今天就来讲下微信公众号后台具体如何来实现。 一、申请公众号 ——- 首先我们根据自己活着企业的所需的东西，选定好一下几种类别的账号。 不同的账户拥有的权限不尽相同。 这个我们可以根据权限文档进行查看！ 接口权限：https://mp.weixin.qq.com/advanced/advanced?action=table&amp;token=1776791094&amp;lang=zh_CN 然后就是根据自己的接口的要求，去审批。 二、设置基本配置 ——— 这里需要我们先开启自己的开发者密码。（这个是我们在开发过程中需要使用的） 服务器地址（URL）:这里需要设置公网可访问的接口 令牌（Token）：这块我们可以设置为一个32为UUID的字符串。 更多细节请参考官方文档：https://mp.weixin.qq.com/wiki?t=resource/res_main&amp;id=mp1421135319 三、代码的实现 其他的也就不多说了，直接上代码。 我们首先创建一个介入指南中，微信需要验证的接口。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpSession;import java.io.UnsupportedEncodingException;import java.security.MessageDigest;import java.security.NoSuchAlgorithmException;import java.util.*;/**---------*/@RequestMapping(\"verification\")public String weChatVerification(String signature, String timestamp, String nonce, String echostr, HttpServletRequest request) throws AesException &#123; log.info(\"微信校验参数：signature=&#123;&#125;，timestamp=&#123;&#125;，nonce=&#123;&#125;，echostr=&#123;&#125;\",signature,timestamp,nonce,echostr); if(signature.equals(getSHA1(token,timestamp,nonce))) return echostr; else &#123; return \"\"; &#125;&#125;private String getSHA1(String token, String timestamp, String nonce) throws AesException &#123; try &#123; String[] array = new String[] &#123; token, timestamp, nonce &#125;; StringBuffer sb = new StringBuffer(); // 字符串排序 Arrays.sort(array); for (int i = 0; i &lt; 3; i++) &#123; sb.append(array[i]); &#125; String str = sb.toString(); // SHA1签名生成 MessageDigest md = MessageDigest.getInstance(\"SHA-1\"); md.update(str.getBytes()); byte[] digest = md.digest(); StringBuffer hexstr = new StringBuffer(); String shaHex = \"\"; for (int i = 0; i &lt; digest.length; i++) &#123; shaHex = Integer.toHexString(digest[i] &amp; 0xFF); if (shaHex.length() &lt; 2) &#123; hexstr.append(0); &#125; hexstr.append(shaHex); &#125; return hexstr.toString(); &#125; catch (Exception e) &#123; e.printStackTrace(); throw new AesException(AesException.ComputeSignatureError); &#125; &#125; 现在就已经接入了微信公众平台。","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://weafteam.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://weafteam.github.io/tags/JAVA/"}]},{"title":"asyncio 不完全指北（三）","slug":"2018-04-23/guide-to-asyncio-3","date":"2018-05-01T03:58:22.000Z","updated":"2018-08-07T13:19:42.440Z","comments":true,"path":"posts/c99ebd1c/","link":"","permalink":"http://weafteam.github.io/posts/c99ebd1c/","excerpt":"","text":"书接上文。 并行执行任务 任务是与事件循环交互的主要方式之一。任务包装协程并跟踪它们完成的时间。任务是 future 的子类，因此其它协程可以等待任务，并且每个任务都有一个结果，可以在任务完成后获取。 启动任务 使用 create_task() 创建任务实例。只要事件循环正在运行且协程不返回，生成的任务将作为事件循环管理的并发操作的一部分运行： 12345678910111213141516171819202122import asyncioasync def task_func(): print('in task_func') return 'the result'async def main(loop): print('creating task') task = loop.create_task(task_func()) print(f'waiting for &#123;task!r&#125;') return_value = await task print(f'task completed &#123;task!r&#125;') print(f'return value: &#123;return_value!r&#125;')event_loop = asyncio.get_event_loop()try: event_loop.run_until_complete(main(event_loop))finally: event_loop.close() main() 函数在退出前等待任务返回结果： 12345creating taskwaiting for &lt;Task pending coro=&lt;task_func() running at *.py:4&gt;&gt;in task_functask completed &lt;Task finished coro=&lt;task_func() done, defined at *.py:4&gt; result='the result'&gt;return value: 'the result' 取消任务 通过保留 create_task() 返回的任务对象，可以在任务完成之前取消其操作： 1234567891011121314151617181920212223242526272829import asyncioasync def task_func(): print('in task_func') return 'the result'async def main(loop): print('creating task') task = loop.create_task(task_func()) print('canceling task') task.cancel() print(f'canceled task &#123;task!r&#125;') try: await task except asyncio.CancelledError: print('caught error from canceled task') else: print(f'task result: &#123;task.result()!r&#125;')event_loop = asyncio.get_event_loop()try: event_loop.run_until_complete(main(event_loop))finally: event_loop.close() 在启动事件循环之前取消任务时，await task 会抛出 CancelledError 异常： 1234creating taskcanceling taskcanceled task &lt;Task cancelling coro=&lt;task_func() running at *.py:4&gt;&gt;caught error from canceled task 如果某个任务在等待另一个并发操作时被取消，则会通过在其等待的位置抛出 CancelledError 异常来通知该任务： 12345678910111213141516171819202122232425262728293031323334import asyncioasync def task_func(): print('in task_func, sleeping') try: await asyncio.sleep(1) except asyncio.CancelledError: print('task_func was canceled') raise return 'the result'def task_canceller(t): print('in task_canceller') t.cancel() print('canceled the task')async def main(loop): print('creating task') task = loop.create_task(task_func()) loop.call_soon(task_canceller, task) try: await task except asyncio.CancelledError: print('main() also sees task as canceled')event_loop = asyncio.get_event_loop()try: event_loop.run_until_complete(main(event_loop))finally: event_loop.close() 捕获该异常可以清理已完成工作： 123456creating taskin task_func, sleepingin task_cancellercanceled the tasktask_func was canceledmain() also sees task as canceled 从协程创建任务 ensure_future() 返回一个与协程的执行相关联的任务。然后，可以将该任务实例传递给其他代码，这些代码可以在不知道原始的协程是如何构造或调用的情况下等待它： 1234567891011121314151617181920212223242526272829import asyncioasync def wrapped(): print('wrapped') return 'result'async def inner(task): print('inner: starting') print(f'inner: waiting for &#123;task!r&#125;') result = await task print(f'inner: task returned &#123;result!r&#125;')async def starter(): print('starter: creating task') task = asyncio.ensure_future(wrapped()) print('starter: waiting for inner') await inner(task) print('starter: inner returned')event_loop = asyncio.get_event_loop()try: print('entering event loop') result = event_loop.run_until_complete(starter())finally: event_loop.close() 可以注意到传入 ensure_future() 的协程不会马上启动，而是直到某个地方用 await 调用了用它创建的任务： 12345678entering event loopstarter: creating taskstarter: waiting for innerinner: startinginner: waiting for &lt;Task pending coro=&lt;wrapped() running at *.py:4&gt;&gt;wrappedinner: task returned 'result'starter: inner returned 用控制结构组合协程 一系列线性执行的协程可以很方便的使用关键字 await 管理。对于复杂的控制结构，例如一个协程等待其他几个协程并行完成，也可以用 asyncio 中的工具实现。 等待多个协程 将一个操作分成许多部分并分别执行它们是很常见的场景。例如，下载多个远程资源，或查询远程 API。在执行顺序无关紧要，并且可能存在任意数量的操作的情况下，wait() 可以用于暂停一个协程，直到其他后台操作完成： 123456789101112131415161718192021222324import asyncioasync def phase(i): print(f'in phase &#123;i&#125;') await asyncio.sleep(0.1 * i) print(f'done with phase &#123;i&#125;') return f'phase &#123;i&#125; result'async def main(num_phases): print('starting main') phases = [phase(i) for i in range(num_phases)] print('waiting for phases to complete') completed, pending = await asyncio.wait(phases) results = [t.result() for t in completed] print(f'results: &#123;results!r&#125;')event_loop = asyncio.get_event_loop()try: event_loop.run_until_complete(main(3))finally: event_loop.close() 在内部，wait() 使用一个集合来保存它创建的任务实例，所以任务的执行顺序是无序的。wait() 的返回值是一个包含两个集合的元组，第一个保存了状态为 done 的任务，第二个保存了状态为 pending 的任务。 123456789starting mainwaiting for phases to completein phase 1in phase 0in phase 2done with phase 0done with phase 1done with phase 2results: ['phase 1 result', 'phase 0 result', 'phase 2 result'] 调用 wait() 时如果指定了 timeout 参数，才会出现状态为 pending 的任务： 12345678910111213141516171819202122232425262728293031323334import asyncioasync def phase(i): print(f'in phase &#123;i&#125;') try: await asyncio.sleep(0.1 * i) except asyncio.CancelledError: print(f'phase &#123;i&#125; canceled') raise else: print(f'done with phase &#123;i&#125;') return f'phase &#123;i&#125; result'async def main(num_phases): print('starting main') phases = [phase(i) for i in range(num_phases)] print('waiting 0.1 for phases to complete') completed, pending = await asyncio.wait(phases, timeout=0.1) print(f'&#123;len(completed)&#125; completed and &#123;len(pending)&#125; pending') # 取消状态为 pending 的任务 if pending: print('canceling tasks') for t in pending: t.cancel() print('exiting main')event_loop = asyncio.get_event_loop()try: event_loop.run_until_complete(main(3))finally: event_loop.close() 这些状态为 pending 的任务应被取消或者继续等待它们完成。事件循环继续运行时这些任务将继续执行，如果wait() 函数的完成被认为所有操作都已经终止了，那这样的结果是不正确的；如果在事件循环结束时仍未完成这些任务，则会生成警告。所以有必要在 wait() 函数结束后取消所有状态为 pending 的任务。 1234567891011starting mainwaiting 0.1 for phases to completein phase 0in phase 1in phase 2done with phase 01 completed and 2 pendingcanceling tasksexiting mainphase 1 canceledphase 2 canceled 收集协程的结果 如果要执行的多个协程已经被定义好，并且只关心它们的结果，那么 gather() 是一种比较好的收集结果的方法： 1234567891011121314151617181920212223242526272829303132import asyncioasync def phase1(): print('in phase1') await asyncio.sleep(2) print('done with phase1') return 'phase1 result'async def phase2(): print('in phase2') await asyncio.sleep(1) print('done with phase2') return 'phase2 result'async def main(): print('starting main') print('waiting for phases to complete') results = await asyncio.gather( phase1(), phase2(), ) print(f'results: &#123;results!r&#125;')event_loop = asyncio.get_event_loop()try: event_loop.run_until_complete(main())finally: event_loop.close() gather() 创建的任务不会公开，因此无法取消。返回值是一个结果列表，顺序与传递给 gather() 的参数的顺序相同，与实际完成的顺序无关： 1234567starting mainwaiting for phases to completein phase2in phase1done with phase2done with phase1results: [&apos;phase1 result&apos;, &apos;phase2 result&apos;] 在任务完成后做一些事 as_completed() 是一个生成器，它管理当作参数传递给它的协程列表的执行，每次迭代都会产生一个执行完的协程。与 wait() 一样，as_completed() 也不保证顺序；与 wait() 不同的是它不会等到所有后台操作完成后才可以执行其它操作： 1234567891011121314151617181920212223242526272829import asyncioasync def phase(i): print(f'in phase &#123;i&#125;') await asyncio.sleep(0.5 - (0.1 * i)) print(f'done with phase &#123;i&#125;') return f'phase &#123;i&#125; result'async def main(num_phases): print('starting main') phases = [phase(i) for i in range(num_phases)] print('waiting for phases to complete') results = [] for next_to_complete in asyncio.as_completed(phases): print(f'start &#123;next_to_complete&#125;') answer = await next_to_complete print(f'received answer &#123;answer!r&#125;') results.append(answer) print(f'results: &#123;results!r&#125;') return resultsevent_loop = asyncio.get_event_loop()try: event_loop.run_until_complete(main(3))finally: event_loop.close() 这个例子启动了几个协程，这些协程以它们开始顺序的相反顺序结束。当生成器被消耗时，循环使用 await等待协程的结果： 123456789101112starting mainwaiting for phases to completein phase 0in phase 1in phase 2done with phase 2received answer 'phase 2 result'done with phase 1received answer 'phase 1 result'done with phase 0received answer 'phase 0 result'results: ['phase 2 result', 'phase 1 result', 'phase 0 result'] 参考资料 Executing Tasks Concurrently Composing Coroutines with Control Structures","categories":[],"tags":[]},{"title":"asyncio 不完全指北（二）","slug":"2018-04-16/guide-to-asyncio-2","date":"2018-04-24T15:43:20.000Z","updated":"2018-08-07T13:19:42.439Z","comments":true,"path":"posts/84801e4e/","link":"","permalink":"http://weafteam.github.io/posts/84801e4e/","excerpt":"","text":"书接上文。 调度常规函数 除了管理协程和 I / O 回调之外，asyncio 事件循环还可以根据循环中的计时器调度常规函数。 立即调度 如果函数执行的时机无关紧要，call_soon() 可以用于在事件循环的下一次迭代中调度函数。 123456789101112131415161718192021222324import asyncioimport functoolsdef callback(arg, *, kwarg='default'): print(f'callback invoked with &#123;arg&#125; and &#123;kwarg&#125;')async def main(loop): print('registering callbacks') loop.call_soon(callback, 1) wrapped = functools.partial(callback, kwarg='not default') loop.call_soon(wrapped, 2) await asyncio.sleep(0.1)event_loop = asyncio.get_event_loop()try: print('entering event loop') event_loop.run_until_complete(main(event_loop))finally: print('closing event loop') event_loop.close() call_soon() 的第一个参数为回调函数，剩下的位置参数都会被传递给回调函数。如果想传入关键字参数，就需要用到 functools.partical()。 回调函数按照调度顺序被依次调用： 12345entering event loopregistering callbackscallback invoked with 1 and defaultcallback invoked with 2 and not defaultclosing event loop 有延迟的调度 要将回调函数的执行推迟到将来的某个时间，可以使用 call_later()。它的第一个参数是以秒为单位的延迟，第二个参数是回调函数。 1234567891011121314151617181920212223import asynciodef callback(n): print(f'callback &#123;n&#125; invoked')async def main(loop): print('registering callbacks') loop.call_later(0.2, callback, 1) loop.call_later(0.1, callback, 2) loop.call_soon(callback, 3) await asyncio.sleep(0.4)event_loop = asyncio.get_event_loop()try: print('entering event loop') event_loop.run_until_complete(main(event_loop))finally: print('closing event loop') event_loop.close() 在这个例子中，相同的回调函数参与了三次调度，分别使用了几个不同的延迟和不同的参数。最后使用了 call_soon()，它使回调函数在所有延迟调度之前调用，这表明 call_soon() 通常意味着最小延迟： 123456entering event loopregistering callbackscallback 3 invokedcallback 2 invokedcallback 1 invokedclosing event loop 在特定时间调度 还可以在特定时间调度函数。事件循环使用单调时钟，而不是 Unix 时钟，以确保当前的值永不回归。要在特定的时间调度，必须使用事件循环的time()方法。 12345678910111213141516171819202122232425262728import asyncioimport timedef callback(n, loop): print(f'callback &#123;n&#125; invoked at &#123;loop.time()&#125;')async def main(loop): now = loop.time() print(f'clock time: &#123;time.time()&#125;') print(f'loop time: &#123;now&#125;') print('registering callbacks') loop.call_at(now + 0.2, callback, 1, loop) loop.call_at(now + 0.1, callback, 2, loop) loop.call_soon(callback, 3, loop) await asyncio.sleep(1)event_loop = asyncio.get_event_loop()try: print('entering event loop') event_loop.run_until_complete(main(event_loop))finally: print('closing event loop') event_loop.close() 可以注意到事件循环内的时间与 time.time() 不一致： 12345678entering event loopclock time: 1524502404.7036376loop time: 4562.515registering callbackscallback 3 invoked at 4562.515callback 2 invoked at 4562.625callback 1 invoked at 4562.718closing event loop 异步产生结果 future是尚未完成的工作的结果。事件循环可以监视future对象的状态直到它完成，从而允许应用程序的一部分等待另一部分完成某些工作。 等待 future future就像协程，所以任何用于处理协程的方法也可以用来处理future。这个例子将future传递给事件循环的run_until_complete()方法。记住我们在上一篇中提到的，通常我们不应该自行创建 future 对象，这里只为演示。 1234567891011121314151617181920212223import asynciodef mark_done(future, result): print(f'setting future result to &#123;result!r&#125;') future.set_result(result)event_loop = asyncio.get_event_loop()try: all_done = asyncio.Future() print('scheduling mark_done') event_loop.call_soon(mark_done, all_done, 'the result') print('entering event loop') result = event_loop.run_until_complete(all_done) print(f'returned result: &#123;result!r&#125;')finally: print('closing event loop') event_loop.close()print(f'future result: &#123;all_done.result()!r&#125;') 调用 set_result() 时，future 的状态会被更改为已完成，future的实例将保存结果，并返回： 123456scheduling mark_doneentering event loopsetting future result to 'the result'returned result: 'the result'closing event loopfuture result: 'the result' future 也可以与 await 一起使用： 1234567891011121314151617181920212223import asynciodef mark_done(future, result): print(f'setting future result to &#123;result!r&#125;') future.set_result(result)async def main(loop): all_done = asyncio.Future() print('scheduling mark_done') loop.call_soon(mark_done, all_done, 'the result') result = await all_done print(f'returned result: &#123;result!r&#125;')event_loop = asyncio.get_event_loop()try: event_loop.run_until_complete(main(event_loop))finally: event_loop.close() future 的结果由 await 返回： 123scheduling mark_donesetting future result to 'the result'returned result: 'the result' future 的回调 除了像协程一样工作之外，future还可以在完成时调用回调函数。 1234567891011121314151617181920212223242526import asyncioimport functoolsdef callback(future, n): print(f'&#123;n&#125;: future done: &#123;future.result()&#125;')async def register_callbacks(all_done): print('registering callbacks on future') all_done.add_done_callback(functools.partial(callback, n=1)) all_done.add_done_callback(functools.partial(callback, n=2))async def main(all_done): await register_callbacks(all_done) print('setting result of future') all_done.set_result('the result')event_loop = asyncio.get_event_loop()try: all_done = asyncio.Future() event_loop.run_until_complete(main(all_done))finally: event_loop.close() 添加回调的函数只需要一个参数，即回调函数；回调函数也只接受一个参数，即 future 实例。若要传递其他参数给回调函数，要使用 functools.partical() 。回调函数按注册顺序被调用： 1234registering callbacks on futuresetting result of future1: future done: the result2: future done: the result 参考资料 Scheduling Calls to Regular Functions Producing Results Asynchronously","categories":[],"tags":[]},{"title":"Kafka在SpringBoot 2.0中的整合","slug":"2018-04-23/SpringBoot-integration-with-Kafka","date":"2018-04-23T10:13:02.000Z","updated":"2018-05-03T07:33:58.508Z","comments":true,"path":"posts/23949c22/","link":"","permalink":"http://weafteam.github.io/posts/23949c22/","excerpt":"","text":"一、Windows平台Kafka的环境搭建 注意：确保JAVA环境变量的正确 1.ZooKeeper的安装 Kafka的运行依赖于Zookeeper，所以需要先安装Zookeeper. Zookeeper下载地址：Zookeeper 解压出来，放在指定位置。 在conf文件夹下修改zoo_sample.cfg名为zoo.cfg 然后打开zoo.cfg 添加一下变量（如果没有请添加，存在请修改） 12dataDir=D:\\data\\logs\\zookeeper dataLogDir=D:\\data\\logs\\zookeeper 然后进入bin目录双击zkServer.cmd运行。如下图： 2.Kafka的安装 Kafka下载地址：Kafka 解压文件到指定地方 打开config下的server.properties 修改以下变量 1log.dirs=D:\\data\\logs\\kafka 我们可以看到bin目录下的是linux的启动脚本，然后有个单独的文件夹装着windows的脚本。 我们在根目录下打开命令行，运行以下命令启动Kafka。 我们在运行前需要注意以下几点 确认JAVA环境变量没有问题 路径不能有空格，不然可能会出现无法加载主类的错误。 出现无法加载主类错误，可修改bin-run-class.bat中 set COMMAND=%JAVA% %KAFKA_HEAP_OPTS% %KAFKA_JVM_PERFORMANCE_OPTS% %KAFKA_JMX_OPTS% %KAFKA_LOG4J_OPTS% -cp %CLASSPATH% %KAFKA_OPTS% %* 中“%CLASSPATH%”加上双引号 1.\\bin\\windows\\kafka-server-start.bat .\\config\\server.properties 二、SpringBoot2.0相关配置 pom文件加入以下依赖 pom.xml 123456&lt;!-- kafka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt; &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt; &lt;version&gt;2.1.5.RELEASE&lt;/version&gt; &lt;/dependency&gt; 我这里SpringBoot的配置文件使用的是YAML。 在相应环境中配置Kafka ##### application-local.yml 12345678910111213141516171819202122232425262728293031323334server: port: 7777spring: datasource: name: test driverClassName: com.mysql.jdbc.Driver url: jdbc:mysql://..... username: ... password: .... redis: database: 0 host: localhost port: 6379 jedis: pool: min-idle: 0 max-idle: 8 max-active: 8 max-wait: -1ms password: 123456 kafka: consumer: group-id: foo auto-offset-reset: earliest key-deserializer: org.apache.kafka.common.serialization.StringDeserializer value-deserializer: org.apache.kafka.common.serialization.StringDeserializer producer: key-serializer: org.apache.kafka.common.serialization.StringSerializer value-serializer: org.apache.kafka.common.serialization.StringSerializer bootstrap-servers: localhost:9092app: topic: foo: foo.t 可以仅关注spring.kafka和app.topic节点 更多spring.kafka配置信息请查看官网文档 三、代码 主要代码结构 消费者代码 12345678910111213141516171819202122package com.xxx.kafka.consumer;import lombok.extern.slf4j.Slf4j;import org.springframework.kafka.annotation.KafkaListener;import org.springframework.messaging.handler.annotation.Payload;import org.springframework.stereotype.Service;/** * @Author ：yaxuSong * @Description: * @Date: 17:56 2018/4/23 * @Modified by: */@Slf4j@Servicepublic class Receiver &#123; @KafkaListener(topics = \"$&#123;app.topic.foo&#125;\") public void listen(@Payload String message) &#123; log.info(\"received message='&#123;&#125;'\", message); &#125;&#125; 生产者代码 1234567891011121314151617181920212223242526272829package com.xxx.kafka.producer;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Value;import org.springframework.kafka.core.KafkaTemplate;import org.springframework.stereotype.Service;/** * @Author ：yaxuSong * @Description: * @Date: 17:57 2018/4/23 * @Modified by: */@Service@Slf4jpublic class Sender &#123; @Autowired private KafkaTemplate&lt;String, String&gt; kafkaTemplate; @Value(\"$&#123;app.topic.foo&#125;\") private String topic; public void send(String message)&#123; log.info(\"sending message='&#123;&#125;' to topic='&#123;&#125;'\", message, topic); kafkaTemplate.send(topic, message); &#125;&#125; 测试代码 1234567891011121314151617181920212223242526272829package com.xxx.controller;import com.xxx.controller.entry.ResMsg;import com.xxx.Sender;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;/** * @Author ：yaxuSong * @Description: * @Date: 11:21 2018/4/21 * @Modified by: */@Slf4j@RequestMapping(\"test\")@RestControllerpublic class TestController &#123; @Autowired private Sender sender; @RequestMapping(\"send\") public ResMsg send(String content)&#123; sender.send(\"Spring Kafka and Spring Boot Send Message:\"+content); return ResMsg.success(); &#125;&#125; 四、简单的测试 运行项目 测试发送 查看接收结果： 五、SpringBoot-Demo 本人最近使用阿里云的Kafka发现没有SpringBoot的Demo便写了一个。 代码地址：https://github.com/songyaxu/kafka-springboot-demo 本文参考地址https://docs.spring.io/spring-kafka/docs/2.1.5.RELEASE/reference/html/","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://weafteam.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://weafteam.github.io/tags/JAVA/"}]},{"title":"chapter-06-AIR","slug":"2018-04-16/chapter-06-AIR","date":"2018-04-22T06:31:24.000Z","updated":"2018-05-14T07:52:18.873Z","comments":true,"path":"posts/3cae9921/","link":"","permalink":"http://weafteam.github.io/posts/3cae9921/","excerpt":"","text":"TensorFlow 基础（3） hello,大家好，几天我们继续学习基础知识，为我们以后建立模型打下基础。主要是最近有点忙，所以每一周的内容会少一些，请大家谅解，随后慢慢加快进度。 这一次我们讲一下batch的概念，以及一些基本的操作，在之前的文章中，我们也讲过batch这个概念的。大家应该不会陌生 Working with Batch and Stochastic Training 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091import tensorflow as tfimport matplotlib.pyplot as pltimport numpy as npfrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()# 随机梯度训练# 生成数据x_vals = np.random.normal(1., 0.1, 100)y_vals = np.repeat(10., 100)x_data = tf.placeholder(shape = [1], dtype = tf.float32)y_target = tf.placeholder(shape = [1], dtype = tf.float32)# A就相当于权重咯A = tf.Variable(tf.random_normal(shape = [1]))my_output = tf.multiply(x_data, A)# 注意我们上一次的loss函数哦loss = tf.square(my_output - y_target)my_opt = tf.train.GradientDescentOptimizer(0.02) # 0.02就是学习率train_step = my_opt.minimize(loss)init = tf.global_variables_initializer()sess.run(init)# 开始训练模型loss_stochastic = []for i in range(100): rand_index = np.random.choice(100) # 随机选取一个样本进行训练 rand_x = [x_vals[rand_index]] rand_y = [y_vals[rand_index]] sess.run(train_step, feed_dict = &#123;x_data: rand_x, y_target: rand_y&#125;) if (i + 1) % 5 == 0: print('Step #' + str(i + 1) + ' A = ' + str(sess.run(A))) temp_loss = sess.run(loss, feed_dict = &#123;x_data: rand_x, y_target: rand_y&#125;) print('Loss = ' + str(temp_loss)) loss_stochastic.append(temp_loss) # batch train ops.reset_default_graph()sess = tf.Session()batch_size = 25x_vals = np.random.normal(1, 0.1, 100)y_vals = np.repeat(10., 100)x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32) # 看出来变化了吧？y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32) # 这里也是A = tf.Variable(tf.random_normal(shape=[1,1]))my_output = tf.matmul(x_data, A)# 是不是l2lossloss = tf.reduce_mean(tf.square(my_output - y_target))# 这里已经强调过很多遍了init = tf.global_variables_initializer()sess.run(init)# 这里是优化器my_opt = tf.train.GradientDescentOptimizer(0.02)train_step = my_opt.minimize(loss)loss_batch = []# Run Loopfor i in range(100): rand_index = np.random.choice(100, size=batch_size) # 看出来区别了么？ rand_x = np.transpose([x_vals[rand_index]]) rand_y = np.transpose([y_vals[rand_index]]) sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) if (i+1)%5==0: print('Step #' + str(i+1) + ' A = ' + str(sess.run(A))) temp_loss = sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) print('Loss = ' + str(temp_loss)) loss_batch.append(temp_loss)plt.plot(range(0, 100, 5), loss_stochastic, 'b-', label='Stochastic Loss')plt.plot(range(0, 100, 5), loss_batch, 'r--', label='Batch Loss, size=20')plt.legend(loc='upper right', prop=&#123;'size': 11&#125;)plt.show() 让你感受一些，batch训练的loss收敛： 你就说上面的你理解没理解，没理解要好好理解理解了~！！！ 这个就是结合了，把所有上面介绍的基础结合在一起，你说说是不是很棒 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# 联合所有的基础操作，弄一个分类的例子，期不期待import matplotlib.pyplot as pltimport numpy as npfrom sklearn import datasetsimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()iris = datasets.load_iris() # 将鸢尾花数据集下下来 总共是四个属性的数据集，根据四个种类，预测种类，总共三类binary_target = np.array([1. if x==0 else 0. for x in iris.target]) # 将label作为二分类问题iris_2d = np.array([[x[2], x[3]] for x in iris.data]) # 将后两个属性取出来用作种类预测batch_size = 20sess = tf.Session()x1_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)x2_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)A = tf.Variable(tf.random_normal(shape=[1, 1]))b = tf.Variable(tf.random_normal(shape=[1, 1]))my_mult = tf.matmul(x2_data, A)my_add = tf.add(my_mult, b)my_output = tf.subtract(x1_data, my_add) # 你能不能自己使用公式写出我们预测种类的公式呢？xentropy = tf.nn.sigmoid_cross_entropy_with_logits(logits = my_output, labels = y_target)my_opt = tf.train.GradientDescentOptimizer(0.05)train_step = my_opt.minimize(xentropy)init = tf.global_variables_initializer()sess.run(init)for i in range(1000): rand_index = np.random.choice(len(iris_2d), size=batch_size) #rand_x = np.transpose([iris_2d[rand_index]]) rand_x = iris_2d[rand_index] rand_x1 = np.array([[x[0]] for x in rand_x]) rand_x2 = np.array([[x[1]] for x in rand_x]) #rand_y = np.transpose([binary_target[rand_index]]) rand_y = np.array([[y] for y in binary_target[rand_index]]) sess.run(train_step, feed_dict=&#123;x1_data: rand_x1, x2_data: rand_x2, y_target: rand_y&#125;) if (i+1)%200==0: print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)) + ', b = ' + str(sess.run(b)))# Pull out slope/intercept[[slope]] = sess.run(A)[[intercept]] = sess.run(b)# Create fitted linex = np.linspace(0, 3, num=50)ablineValues = []for i in x: ablineValues.append(slope*i+intercept)# Plot the fitted line over the datasetosa_x = [a[1] for i,a in enumerate(iris_2d) if binary_target[i]==1]setosa_y = [a[0] for i,a in enumerate(iris_2d) if binary_target[i]==1]non_setosa_x = [a[1] for i,a in enumerate(iris_2d) if binary_target[i]==0]non_setosa_y = [a[0] for i,a in enumerate(iris_2d) if binary_target[i]==0]plt.plot(setosa_x, setosa_y, 'rx', ms=10, mew=2, label='setosa')plt.plot(non_setosa_x, non_setosa_y, 'ro', label='Non-setosa')plt.plot(x, ablineValues, 'b-')plt.xlim([0.0, 2.7])plt.ylim([0.0, 7.1])plt.suptitle('Linear Separator For I.setosa', fontsize=20)plt.xlabel('Petal Length')plt.ylabel('Petal Width')plt.legend(loc='lower right')plt.show() 上面是分类的结果图 验证模型： 接下来，我们会实现一个简单的回归模型和分类模型，分别做出他们的测试样例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113# 回归模型import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()batch_size = 25x_vals = np.random.normal(1, 0.1, 100)y_vals = np.repeat(10., 100)x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)# 将数据分为训练集80%和测试集20%train_indices = np.random.choice(len(x_vals), round(len(x_vals) * 0.8), replace = False)test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))x_vals_train = x_vals[train_indices]x_vals_test = x_vals[test_indices]y_vals_train = y_vals[train_indices]y_vals_test = y_vals[test_indices]# 下面这些我们是不是写了好多遍了！！！A = tf.Variable(tf.random_normal(shape=[1,1]))my_output = tf.matmul(x_data, A)# 还记得我么？你也见过我好多次了，为什么加reduce_mean? 你也知道batch可不是一个数据样本呀~loss = tf.reduce_mean(tf.square(my_output - y_target))# 创建优化器my_opt = tf.train.GradientDescentOptimizer(0.02)train_step = my_opt.minimize(loss)init = tf.global_variables_initializer()sess.run(init)# 算了，我都不想写了，你说说我们写了多少遍了，该会了for i in range(100): rand_index = np.random.choice(len(x_vals_train), size=batch_size) rand_x = np.transpose([x_vals_train[rand_index]]) rand_y = np.transpose([y_vals_train[rand_index]]) sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) if (i+1)%25==0: print('Step #' + str(i+1) + ' A = ' + str(sess.run(A))) print('Loss = ' + str(sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)))# 验证呀，一般训练出来的模型，我们要在测试集上面跑的很好，在测试集上面的准确率好了，没用，因为有过拟合的嫌疑，就是太认真了。泛化能力太差。# 验证回归模型，那就要使用loss去衡量这个模型的好坏了mse_test = sess.run(loss, feed_dict=&#123;x_data: np.transpose([x_vals_test]), y_target: np.transpose([y_vals_test])&#125;)mse_train = sess.run(loss, feed_dict=&#123;x_data: np.transpose([x_vals_train]), y_target: np.transpose([y_vals_train])&#125;)print('MSE on test:' + str(np.round(mse_test, 2)))print('MSE on train:' + str(np.round(mse_train, 2)))# 分类工作，小伙伴是不是一直觉得分类工作怎么能做，那是因为使用概率做的，也就是说，比如概率大于某一个阈值0.5 我们就分为某一类，如果小于我们就分为另一种，这是二分类，那么多分类怎么办，那么就要引入one-hot编码，这个我们后来慢慢深入。先来看例子ops.reset_default_graph()sess = tf.Session()batch_size = 25# 一毛一样x_vals = np.concatenate((np.random.normal(-1, 1, 50), np.random.normal(2, 1, 50)))y_vals = np.concatenate((np.repeat(0., 50), np.repeat(1., 50)))x_data = tf.placeholder(shape=[1, None], dtype=tf.float32)y_target = tf.placeholder(shape=[1, None], dtype=tf.float32)train_indices = np.random.choice(len(x_vals), round(len(x_vals)*0.8), replace=False)test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))x_vals_train = x_vals[train_indices]x_vals_test = x_vals[test_indices]y_vals_train = y_vals[train_indices]y_vals_test = y_vals[test_indices]A = tf.Variable(tf.random_normal(mean=10, shape=[1]))my_output = tf.add(x_data, A) # 我们直接使用了一个加法操作# 关键还是loss函数xentropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = my_output, labels = y_target))my_opt = tf.train.GradientDescentOptimizer(0.05)train_step = my_opt.minimize(xentropy)init = tf.global_variables_initializer()sess.run(init)# Run loopfor i in range(1800): rand_index = np.random.choice(len(x_vals_train), size=batch_size) rand_x = [x_vals_train[rand_index]] rand_y = [y_vals_train[rand_index]] sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) if (i + 1) % 200 == 0: print('Step #' + str(i+1) + ' A = ' + str(sess.run(A))) print('Loss = ' + str(sess.run(xentropy, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)))# 在测试集上面做测试y_prediction = tf.squeeze(tf.round(tf.nn.sigmoid(tf.add(x_data, A))))correct_prediction = tf.equal(y_prediction, y_target)accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))acc_value_test = sess.run(accuracy, feed_dict=&#123;x_data: [x_vals_test], y_target: [y_vals_test]&#125;)acc_value_train = sess.run(accuracy, feed_dict=&#123;x_data: [x_vals_train], y_target: [y_vals_train]&#125;)print('Accuracy on train set: ' + str(acc_value_train))print('Accuracy on test set: ' + str(acc_value_test))# 画出分类结果A_result = -sess.run(A)bins = np.linspace(-5, 5, 50)plt.hist(x_vals[0:50], bins, alpha=0.5, label='N(-1,1)', color='white')plt.hist(x_vals[50:100], bins[0:50], alpha=0.5, label='N(2,1)', color='red')plt.plot((A_result, A_result), (0, 8), 'k--', linewidth=3, label='A = '+ str(np.round(A_result, 2)))plt.legend(loc='upper right')plt.title('Binary Classifier, Accuracy=' + str(np.round(acc_value_test, 2)))plt.show() 总结：这次下来我们就把TensorFlow的所有基础内容讲完了，有什么问题，可以给我发邮件：air@weaf.top 希望大家把这些基础知识好好稳固一下，务必牢记于心，随后我们就会很顺利。 下一次我们就会开始一些基本算法~，基础学完不得好好练习一下么？","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]},{"title":"唯密文解密（针对Vigenere加密）","slug":"2018-04-16/唯密文解密（针对Vigenere加密）","date":"2018-04-21T01:38:12.000Z","updated":"2018-04-22T07:32:03.948Z","comments":true,"path":"posts/899ccb0/","link":"","permalink":"http://weafteam.github.io/posts/899ccb0/","excerpt":"","text":"上次说到了Vigenere加密以及解密的算法，但是如何破译这样的密码，也是很有意思的，这篇博客就是实现一个这样的破译，主要针对的是通过Vigenere加密的密文，那么就开始吧~ 任务要求： a.编程实现Vigenere加密/解密系统，并分析和评估该算法的安全性。 b.编程实现唯密文破译系统，能够破译密钥为2到4个字符的Vigenere密文，并分析如何加快破译速度。 时间要求： 布置任务后，在3周之内完成。 提交结果：已设计并测试好的程序，包括源码、可执行程序、测试数据集、实验报告。 原理介绍： 按照我们之前的说法，我们先介绍一下Caesar加密的缺点。对于一个稍微有点点密码学功底的人来说，Caesar密码的安全强度几乎为零，正如我们是上篇博客所讲的Caesar加密，加密的密钥充其量也就24个，也就是说，不管移动多少个字符，最多进行24次猜解就可以破译出来。 当然，这只是一种解密方法，也是比较笨的一种方法，而且这种方法并不适用于我们的Vigenere密码破解，因为我们没办法列举出所有的情况。 这里我们介绍破解Caesar密码的另一类方法，称为（字母）频度分析法。 假设大家都知道，英语中的字母出现概率是有差别的，其实对于一种特定的自然语言，如果文本足够长，那么各个字母出现的概率就是相对稳定的，具体的概率统计如下图所示： 这样我们根据以上的频度表，以及根据我们的Caesar密文中的统计出来的各个词的拼读，对应一下就可以找到密文对应的明文，再然后对应密文与明文就可以找到相应的加密的密钥。很简单吧，其实能想到这个想法并不简单的。 由上一篇的博客介绍，你应该知道了Vigenere密码分解之后其实就是多个Caesar密码。所以我们如果知道密钥的长度，每隔这个长度将原来的Vigenere密文分解为多个Caesar密文，再做上述的工作，是不是就完成了我们的破译工作？思路就是这样，但是怎么确定我们的密钥长度？那么我们接下来就讲讲怎么解决这个问题吧。 确定密钥长度 这个在网上搜集到的资料其实是有两种方法的。分别称为Kasiski测试法和Friedman测试法，但是本文给到的代码是基于第二个方法的，不过在此之前，我们还是先讲讲这两个方法的思路吧。 Kasiski测试法 Kasiski测试法是由Friedrich Kasiski于1863年给出了其描述，然而早在约1854年这一方法就由Charles Babbage首先发现。 它的思想是基于这样的一个事实：两个相同的明文段加密成相同的密文段，它们之间的距离为Length，那么密钥的长度就是距离Length的约数。 而当密文的长度很长时，我们便可以多找几组这样拥有重复密文段，找出他们间距的相同约数就是密钥的长度。 关于这个方法的代码，本文并未涉及，大家有兴趣可自行查阅资料实现。 另：机智的你也许发现了，我们这种方法其实并没有涉及到我们刚才说的统计频度，所以我们的重点不是这个方法，接下来就是本文的重点了。 Friedman测试法 首先我们讲一个概念：重合指数（IC，index of coincidence）。百度一下这个概念的话，搜到的结果可能不是令人很满意，我也是找了很多资料，感觉如下的概念说的很清楚，分享给大家。 重合指数表示：两个随机选出的字母是相同的概率，对于我们的英文字母来说，即以上概率即为随机选出两个A的概率+随机选出两个B的概率+….+随机选出两个Z的概率。 前人也为我们统计出了这个数字，为0.65。 即 P(A)^2 + P(B)^2 + P(C)^2 + … + P(Z)^2 = 0.65. 而利用这一概念推测密钥长度的原理为：对于一个Caesar密码的序列，由于所有字母的位移程度是一样的，所以密文的重合指数等于原文的重合指数。 将这一概念迁移到我们的Vigenere密文上，我们只要计算不同密钥长度下的重合指数，只要重合指数接近期望的0.65时，我们便可以推测当前的长度就是我们的密钥长度。 举个例子： 密文为：AAABBCCDDDDEEEFG 首先我们测试密钥长度=1，首先统计上述密文中每个字母出现的次数（A-Z）： A：3 B：2 C：2 D：4 E：3 F:1 G:1 H:0 … Z：0 然后我们根据上述公式计算重合指数P，如果 P ！= 0.65，我们就尝试密钥长度2。 假设为2的话，将上述的密文分成两组： 组1：A A B C D D E F 组2：A B C D D E E G 再分别计算重合指数，如果这两个的重合指数都接近于0.65，那么我们就可以基本确定密钥的长度为2了。如果不是，那么继续往下分。 理论上来说，我们得到的密文长度越长，通过这个方法分析得到的效果会更好，实际上在我测试的结果中，也确实符合刚才的说法。 实现 Friedman测试法确定密钥长度 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859// Friedman测试法确定密钥长度 public int Friedman(String ciphertext) &#123; int keyLength = 1; // 猜测密钥长度 double[] IC; // 重合指数 double average; // 平均重合指数 ArrayList&lt;String&gt; cipherGroup; // 密文分组 while (true) &#123; IC = new double[keyLength]; cipherGroup = new ArrayList&lt;String&gt;(); average = 0; // 1 先根据密钥长度分组 for (int i = 0; i &lt; keyLength; ++i) &#123; StringBuffer temporaryGroup = new StringBuffer(); for (int j = 0; i + j * keyLength &lt; ciphertext.length(); ++j) &#123; temporaryGroup.append(ciphertext.charAt(i + j * keyLength)); &#125; cipherGroup.add(temporaryGroup.toString()); &#125; // 2 再计算每一组的重合指数 for (int i = 0; i &lt; keyLength; ++i) &#123; String subCipher = new String(cipherGroup.get(i)); // 子串 HashMap&lt;Character, Integer&gt; occurrenceNumber = new HashMap&lt;Character, Integer&gt;(); // 字母及其出现的次数 // 2.1 初始化字母及其次数键值对 for (int h = 0; h &lt; 26; ++h) &#123; occurrenceNumber.put((char) (h + 65), 0); &#125; // 2.2 统计每个字母出现的次数 for (int j = 0; j &lt; subCipher.length(); ++j) &#123; occurrenceNumber.put(subCipher.charAt(j), occurrenceNumber.get(subCipher.charAt(j)) + 1); &#125; // 2.3 计算重合指数 double denominator = Math.pow((double) subCipher.length(), 2); for (int k = 0; k &lt; 26; ++k) &#123; double o = (double) occurrenceNumber.get((char) (k + 65)); IC[i] += o * (o - 1); &#125; IC[i] /= denominator; &#125; // 3 判断退出条件,重合指数的平均值是否大于0.065 for (int i = 0; i &lt; keyLength; ++i) &#123; average += IC[i]; &#125; average /= (double) keyLength; if (average &gt;= 0.06) &#123; break; &#125; else &#123; keyLength++; &#125; &#125; // while--end return keyLength; &#125;// Friedman--end 破译密文 这里给出来的是打印出来了具体的密钥和明文，实际上可以直接写一个类，类中设计两个属性值，一个密钥属性，一个明文属性，直接赋值下就可以了。相信机智的你可以完成这个操作。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public void decryptCipher(int keyLength, String ciphertext) &#123; int[] key = new int[keyLength]; ArrayList&lt;String&gt; cipherGroup = new ArrayList&lt;String&gt;(); double[] probability = new double[] &#123; 0.082, 0.015, 0.028, 0.043, 0.127, 0.022, 0.02, 0.061, 0.07, 0.002, 0.008, 0.04, 0.024, 0.067, 0.075, 0.019, 0.001, 0.06, 0.063, 0.091, 0.028, 0.01, 0.023, 0.001, 0.02, 0.001 &#125;; // 1 先根据密钥长度分组 for (int i = 0; i &lt; keyLength; ++i) &#123; StringBuffer temporaryGroup = new StringBuffer(); for (int j = 0; i + j * keyLength &lt; ciphertext.length(); ++j) &#123; temporaryGroup.append(ciphertext.charAt(i + j * keyLength)); &#125; cipherGroup.add(temporaryGroup.toString()); &#125; // 2 确定密钥 for (int i = 0; i &lt; keyLength; ++i) &#123; double MG; // 重合指数 int flag; // 移动位置 int g = 0; // 密文移动g个位置 HashMap&lt;Character, Integer&gt; occurrenceNumber; // 字母出现次数 String subCipher; // 子串 while (true) &#123; MG = 0; flag = 65 + g; subCipher = new String(cipherGroup.get(i)); occurrenceNumber = new HashMap&lt;Character, Integer&gt;(); // 1.1 初始化字母及其次数 for (int h = 0; h &lt; 26; ++h) &#123; occurrenceNumber.put((char) (h + 65), 0); &#125; // 1.2 统计字母出现次数 for (int j = 0; j &lt; subCipher.length(); ++j) &#123; occurrenceNumber.put(subCipher.charAt(j), occurrenceNumber.get(subCipher.charAt(j)) + 1); &#125; // 1.3 计算重合指数 for (int k = 0; k &lt; 26; ++k, ++flag) &#123; double p = probability[k]; flag = (flag == 91) ? 65 : flag; double f = (double) occurrenceNumber.get((char) flag) / subCipher.length(); MG += p * f; &#125; // 1.4 判断退出条件 if (MG &gt;= 0.055) &#123; key[i] = g; break; &#125; else &#123; ++g; &#125; &#125; // while--end &#125; // for--end // 3 打印密钥 StringBuffer keyString = new StringBuffer(); for (int i = 0; i &lt; keyLength; ++i) &#123; keyString.append((char) (key[i] + 65)); &#125; System.out.println(&quot;\\n密钥为: &quot; + keyString.toString()); // 4 解密 StringBuffer plainBuffer = new StringBuffer(); for (int i = 0; i &lt; ciphertext.length(); ++i) &#123; int keyFlag = i % keyLength; int change = (int) ciphertext.charAt(i) - 65 - key[keyFlag]; char plainLetter = (char) ((change &lt; 0 ? (change + 26) : change) + 65); plainBuffer.append(plainLetter); &#125; System.out.println(&quot;\\n明文为：\\n&quot; + plainBuffer.toString().toLowerCase()); &#125; 参考资料： 维吉尼亚密码及其破解 【密码学】维吉尼亚密码加解密原理及其破解算法Java实现 以上是本次博客的全部内容，感谢驻足~","categories":[{"name":"密码学","slug":"密码学","permalink":"http://weafteam.github.io/categories/密码学/"}],"tags":[{"name":"密码学","slug":"密码学","permalink":"http://weafteam.github.io/tags/密码学/"}]},{"title":"Redis在SpringBoot 2.0中的整合","slug":"2018-04-16/SpringBoot-integration-with-Redis","date":"2018-04-18T09:57:12.000Z","updated":"2018-04-30T14:06:29.815Z","comments":true,"path":"posts/2b2e3e2f/","link":"","permalink":"http://weafteam.github.io/posts/2b2e3e2f/","excerpt":"今天开始给大家分享Java相关的技术开发知识，在以后的开发和学习中，还希望大家多多指教，对于我发表的相关内容，如有错误，请大家指出来，一起学习。 更要记住这句话：Stay Hungry, Stay Foolish. 一、Redis的安装 为了方便教程这里先简单介绍Redis的安装。 1. windows平台的安装 现在官网已经不提供windows平台的下载，所以只能去github上下载安装 github下载网址 进入之后选择好版本点击msi下载 然后双击安装。 默认是直接运行的。 可以通过控制台访问如 具体语法可以在相关网上查阅。 ### 2. Linux平台的安装 直接到官网下载 Redis.io 解压并安装 1234wget http://download.redis.io/releases/redis-4.0.9.tar.gztar xzf redis-4.0.9.tar.gzcd redis-4.0.9make","text":"今天开始给大家分享Java相关的技术开发知识，在以后的开发和学习中，还希望大家多多指教，对于我发表的相关内容，如有错误，请大家指出来，一起学习。 更要记住这句话：Stay Hungry, Stay Foolish. 一、Redis的安装 为了方便教程这里先简单介绍Redis的安装。 1. windows平台的安装 现在官网已经不提供windows平台的下载，所以只能去github上下载安装 github下载网址 进入之后选择好版本点击msi下载 然后双击安装。 默认是直接运行的。 可以通过控制台访问如 具体语法可以在相关网上查阅。 ### 2. Linux平台的安装 直接到官网下载 Redis.io 解压并安装 1234wget http://download.redis.io/releases/redis-4.0.9.tar.gztar xzf redis-4.0.9.tar.gzcd redis-4.0.9make 服务端运行脚本 1src/redis-server 客户端运行脚本 1src/redis-cli 3. SpringBoot2.0相关配置 pom文件加入以下依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 我这里SpringBoot的配置文件使用的是YAML。 在相应环境中配置Redis ##### application-local.yml 1234567891011121314151617spring: datasource: name: test driverClassName: com.mysql.jdbc.Driver url: jdbc:mysql://127.0.0.1:3306/local?useUnicode=true&amp;characterEncoding=UTF-8 username: root password: root redis: database: 0 host: localhost port: 6379 jedis: pool: min-idle: 0 max-idle: 8 max-active: 8 max-wait: -1ms 4.代码级别配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import com.xxx.controller.entry.entity.AccessToken;import org.springframework.cache.annotation.CachingConfigurerSupport;import org.springframework.cache.annotation.EnableCaching;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.redis.cache.RedisCacheManager;import org.springframework.data.redis.connection.RedisConnectionFactory;import org.springframework.data.redis.connection.jedis.JedisConnectionFactory;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.data.redis.core.convert.KeyspaceConfiguration;import org.springframework.data.redis.repository.configuration.EnableRedisRepositories;import org.springframework.data.redis.serializer.Jackson2JsonRedisSerializer;import org.springframework.data.redis.serializer.StringRedisSerializer;import java.util.Collections;/** * @Author ：yaxuSong * @Description: * @Date: 18:35 2018/4/17 * @Modified by: */@Configuration@EnableCaching//增加Respository支持，并使其支持@TimeToLive@EnableRedisRepositories(keyspaceConfiguration = RedisCacheConfig.MyKeyspaceConfiguration.class)public class RedisCacheConfig extends CachingConfigurerSupport&#123; @Bean public RedisCacheManager cacheManager(RedisConnectionFactory connectionFactory) &#123; return RedisCacheManager.builder(connectionFactory).build(); &#125;// @Bean// public RedisConnectionFactory connectionFactory() &#123;// return new JedisConnectionFactory();// &#125; @Bean public RedisTemplate&lt;?, ?&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) &#123; RedisTemplate&lt;byte[], byte[]&gt; template = new RedisTemplate&lt;byte[], byte[]&gt;(); template.setConnectionFactory(redisConnectionFactory); return template; &#125;// @Bean// public RedisTemplate&lt;Object, Object&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory)&#123;// RedisTemplate&lt;Object, Object&gt; redisTemplate = new RedisTemplate&lt;Object, Object&gt;();// redisTemplate.setConnectionFactory(redisConnectionFactory);// redisTemplate.setKeySerializer(new StringRedisSerializer());//key序列化// redisTemplate.setValueSerializer(new Jackson2JsonRedisSerializer(Object.class)); //value序列化// redisTemplate.afterPropertiesSet();// return redisTemplate;// &#125; public static class MyKeyspaceConfiguration extends KeyspaceConfiguration &#123; @Override protected Iterable&lt;KeyspaceSettings&gt; initialConfiguration() &#123; return Collections.singleton(new KeyspaceSettings(AccessToken.class, \"accessToken\")); &#125; &#125;&#125; 缓存对象AccessToken 1234567891011121314151617181920import lombok.Data;import org.springframework.data.annotation.Id;import org.springframework.data.redis.core.RedisHash;import org.springframework.data.redis.core.TimeToLive;/** * @Author ：yaxuSong * @Description: * @Date: 14:26 2018/4/18 * @Modified by: */@RedisHash(\"accessToken\")@Datapublic class AccessToken &#123; @Id String id; String accessToken; @TimeToLive Long expire;&#125; 创建Respository 123456789101112131415import com.xxx.controller.entry.entity.AccessToken;import org.springframework.data.repository.CrudRepository;import org.springframework.stereotype.Repository;/** * @Author ：yaxuSong * @Description: * @Date: 14:34 2018/4/18 * @Modified by: */@Repository// 继承自CURD，里边有最基本的方法public interface AccessTokenRepository extends CrudRepository&lt;AccessToken, String&gt; &#123;&#125; 接下来完成自己的业务服务类 12345678910111213141516import com.xxx.controller.entry.entity.AccessToken;/** * @Author ：yaxuSong * @Description: * @Date: 14:50 2018/4/18 * @Modified by: */public interface AccessTokenService &#123; AccessToken save(AccessToken accessToken); void delete(AccessToken accessToken); AccessToken get(String id);&#125; 业务服务类的实现 123456789101112131415161718192021222324252627282930313233343536import com.xxx.controller.entry.entity.AccessToken;import com.xxx.dao.repository.AccessTokenRepository;import com.xxx.service.AccessTokenService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import java.util.Optional;/** * @Author ：yaxuSong * @Description: * @Date: 14:52 2018/4/18 * @Modified by: */@Service(\"accessTokenService\")public class AccessTokenServiceImpl implements AccessTokenService &#123; @Autowired private AccessTokenRepository repo; @Override public AccessToken save(AccessToken accessToken) &#123; return repo.save(accessToken); &#125; @Override public void delete(AccessToken accessToken) &#123; repo.delete(accessToken); &#125; @Override public AccessToken get(String id) &#123; Optional&lt;AccessToken&gt; accessToken = repo.findById(id); return accessToken.orElse(null); &#125;&#125; 以上完成了整个整合过程。 ### 5. 简单的测试 12345678910111213141516171819202122232425262728293031323334import com.xxx.controller.entry.entity.AccessToken;import com.xxx.service.AccessTokenService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;/** * @Author ：yaxuSong * @Description: * @Date: 15:16 2018/4/18 * @Modified by: */@RestController@RequestMapping(\"test\")public class TestController &#123; @Autowired private AccessTokenService accessTokenService; @RequestMapping(\"add\") public String test()&#123; AccessToken accessToken = new AccessToken(); accessToken.setAccessToken(\"dadaadadsdadewqeqfskksdbfdbkfsdkdajdhwke2elhsbcslc/DNDAWDAWWAFEWFSD23E2342\"); accessToken.setExpire(60L); //单位 秒 AccessToken at = accessTokenService.save(accessToken); return \"成功\"+\"键值为：\"+at.getId(); &#125; @RequestMapping(\"get\") public String get(String id)&#123; AccessToken accessToken = accessTokenService.get(id); return accessToken==null?\"已过期\":accessToken.toString(); &#125;&#125; 测试结果： 我这里添加了一个过期时间为60s的token。 我们通过查看可以看到时间的变化 第一次查询： 第二次查询： 第三次查询： 我们查看下本地Rdis所有键值情况： 过一段时间后查询： 我们发现之前还存在键值id为c07cde6a-aec7-40f3-ad39-41862209bc9f的，但是内容没有了。 后来查询的就被删除了（过期后不会直接删除，会稍有延迟，只有id存在，其他都已被删除） 我们看到键值为：d2b97d54-1c8b-4803-8f60-6aaf3384fc32的是我之前存的TTL=7200s的。 至此所有相关的内容就介绍完了。 本文参考地址：Spring-data-redis","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://weafteam.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://weafteam.github.io/tags/JAVA/"}]},{"title":"Linux下MySQL安装","slug":"2018-04-09/three-method-for-install-mysql","date":"2018-04-17T04:07:12.000Z","updated":"2018-04-22T07:28:18.414Z","comments":true,"path":"posts/2cd32dfc/","link":"","permalink":"http://weafteam.github.io/posts/2cd32dfc/","excerpt":"接下来我将介绍3种方法安装MySQL 第一种 一、查看是否安装了MySQL 使用命令： 1rpm -qa|grep -i mysql 如果使用centos，可能会出现冲突，解决冲突需要卸载mariadb 首先查看是否安装了Mariadb 1rpm -qa|grep mariadb","text":"接下来我将介绍3种方法安装MySQL 第一种 一、查看是否安装了MySQL 使用命令： 1rpm -qa|grep -i mysql 如果使用centos，可能会出现冲突，解决冲突需要卸载mariadb 首先查看是否安装了Mariadb 1rpm -qa|grep mariadb 然后卸载 1rpm -e mariadb-libs-5.5.56-2.el7.x86_64 强制卸载(可选): 1rpm -e --nodeps mariadb-libs-5.5.56-2.el7.x86_64 二、如果安装了需删除已安装版本 —— 删除命令： 12rpm -e --nodeps 包名( rpm -ev mysql-4.1.12-3.RHEL4.1 ) 删除老版本mysql的开发头文件和库 命令： 12rm -fr /usr/lib/mysqlrm -fr /usr/include/mysql 注意：卸载后/var/lib/mysql中的数据及/etc/my.cnf不会删除，如果确定没用后就手工删除 12rm -f /etc/my.cnfrm -fr /var/lib/mysql 三、安装mysql准备环境 我自mysql官网下载通用的Linux版本安装包 mysql-5.7.20-linux-glibc2.12-x86_64.tar.gz 将下载好的包放在 /usr/local 目录下，或者执行命令： 12cd /usr/localwget https://cdn.mysql.com//Downloads/MySQL-5.7/mysql-5.7.20-linux-glibc2.12-x86_64.tar.gz 解压下载的文件 1tar -zxvf mysql-5.7.20-linux-glibc2.12-x86_64.tar.gz 将解压之后的所有文件移动到/usr/local/mysql 1mv ./mysql-5.7.20-linux-glibc2.12-x86_64/* ./mysql 为mysql创建系统用户(可选，新版本会自动创建相应用户) 12groupadd mysqluseradd -r -g mysql mysql //-r参数表示mysql用户是系统用户，不可用于登录系统 并变更mysql安装目录的所属用户和用户组 12chown -R mysql:mysql mysql// -R 迭代处理 四、 安装和初始化 —— 初始化数据库 1./bin/mysqld --initialize --user=mysql --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data --lc_messages_dir=/usr/local/mysql/share --lc_messages=en_US 记录刚刚输出的密码： jk9tEao&lt;94MC 配置/etc/my.cnf my.cnf 五、 启动登录和设置密码 切换到mysql安装目录的bin下启动 1./mysqld_safe --user=mysql 启动后可能无法使用当前窗口 登录进去设置新的密码： 1./mysql -u root -p 12set password=password(\"root\");flush privileges; 六、 添加到服务 切换到 support-files目录下，并执行以下命令 1cp mysql.server /etc/init.d/mysql 然后停止当前进程，使用服务启动mysql 1service mysql start 并添加mysql环境变量 在 /etc/profile 的文件末尾追加： export PATH=$PATH:/usr/local/mysql/bin 保存后执行 1source /etc/profile 最后使用新密码登录到mysql 第二种 一、查看是否安装了MySQL 使用命令： 1rpm -qa|grep -i mysql 如果使用centos，可能会出现冲突，解决冲突需要卸载mariadb 首先查看是否安装了Mariadb 1rpm -qa|grep mariadb 然后卸载 1rpm -e mariadb-libs-5.5.56-2.el7.x86_64 强制卸载(可选): 1rpm -e --nodeps mariadb-libs-5.5.56-2.el7.x86_64 二、如果安装了需删除已安装版本 —— 删除命令： 12rpm -e --nodeps 包名( rpm -ev mysql-4.1.12-3.RHEL4.1 ) 删除老版本mysql的开发头文件和库 命令： 12rm -fr /usr/lib/mysqlrm -fr /usr/include/mysql 注意：卸载后/var/lib/mysql中的数据及/etc/my.cnf不会删除，如果确定没用后就手工删除 12rm -f /etc/my.cnfrm -fr /var/lib/mysql 三、准备安装的环境 wget https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-5.7.20-1.el7.x86_64.rpm-bundle.tar 或者用自己准备好的包 mysql-5.7.20-1.el7.x86_64.rpm-bundle.tar 解压包 1tar -xvf mysql-5.7.20-1.el7.x86_64.rpm-bundle.tar 然后按照以下顺序安装 1234rpm -ivh mysql-community-common-5.7.20-1.el7.x86_64.rpm rpm -ivh mysql-community-libs-5.7.20-1.el7.x86_64.rpm rpm -ivh mysql-community-client-5.7.20-1.el7.x86_64.rpm rpm -ivh mysql-community-server-5.7.20-1.el7.x86_64.rpm 四、启动并修改密码 安装完成后就可以启动服务了 1service mysqld start 启动后查看配置文件 1vi /etc/my.cnf 找打log文件 进入查找默认root密码 1cat /var/log/mysqld.log 使用一下命令登录并修改密码 1mysql -uroot -p 修改密码 12SET PASSWORD FOR &apos;root&apos;@&apos;localhost&apos; = PASSWORD(&apos;newpass&apos;);FLUSH PRIVILEGES; 第三种 一、查看是否安装了MySQL数据库 1rpm -qa|grep mysql 卸载 1rpm -e --nodeps mysql-libs-5.1.71-1.el6.x86_64 二、安装 安装一下包 123456rpm -ivh MySQL-devel-5.6.23-1.linux_glibc2.5.x86_64.rpmrpm -ivh MySQL-client-5.6.23-1.linux_glibc2.5.x86_64.rpmrpm -ivh MySQL-server-5.6.23-1.linux_glibc2.5.x86_64.rpmrpm -ivh MySQL-embedded-5.6.23-1.linux_glibc2.5.x86_64.rpmrpm -ivh MySQL-shared-5.6.23-1.linux_glibc2.5.x86_64.rpmrpm -ivh MySQL-shared-compat-5.6.23-1.linux_glibc2.5.x86_64.rpm 三、启动登录设置密码 使用以下命令开启服务 service mysql start 获取初始密码： 使用root登录 mysql -uroot -p 然后试用一下命令设置密码 SET PASSWORD = PASSWORD(&#39;123456&#39;);","categories":[{"name":"Linux","slug":"Linux","permalink":"http://weafteam.github.io/categories/Linux/"}],"tags":[{"name":"Linux运维","slug":"Linux运维","permalink":"http://weafteam.github.io/tags/Linux运维/"}]},{"title":"chapter-05-AIR","slug":"2018-04-09/chapter-05-AIR","date":"2018-04-15T14:14:23.000Z","updated":"2018-04-22T07:28:18.413Z","comments":true,"path":"posts/7b0ee3f1/","link":"","permalink":"http://weafteam.github.io/posts/7b0ee3f1/","excerpt":"","text":"TensorFlow 基础（2） 今天有和大家见面了，今天的文章可能内容有点少，这周有很多事情，所以少写点。下一周我尽量多写点。弥补大家。那么我们今天闲话少说，直接开始今天的TensorFlow的基础介绍。接着上一节继续讲起。 Loss Functions 今天这个开头就是最常用的损失函数的实现，使用。主要涉及到两种损失函数的设计，数值预测的回归损失函数，还有分类的损失函数设计。那么我们直接开始我们的实现，有什么难点我会注释。 12345678910111213141516171819202122232425262728293031323334353637383940# 首先我们像往常一场导入我们需要的模块import tensorflow as tfimport matplotlib.pyplot as pltfrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()x_vals = tf.linspace(-1., 1., 500)target = tf.constant(0.)# l2 loss 和l2范数差一个平方根l2_y_vals = tf.square(target - x_vals)l2_y_out = sess.run(l2_y_vals)# l1 loss 就是l1范数l1_y_vals = tf.abs(target - x_vals)l1_y_out = sess.run(l1_y_vals)# Pseudo-Huber loss 为了让loss更加的光滑一些# 具体看公式一delta = tf.constant(0.25)phuber1_y_vals = tf.multiply(tf.square(delta), tf.sqrt(1. + tf.square((target - x_vals) / delta)) - 1.)phuber1_y_out = sess.run(phuber1_y_vals)delta2 = tf.constant(5.)phuber2_y_vals = tf.multiply(tf.square(delta2), tf.sqrt(1. + tf.square((target - x_vals)/delta2)) - 1.)phuber2_y_out = sess.run(phuber2_y_vals)# 画出这些回归损失函数x_array = sess.run(x_vals)plt.plot(x_array, l2_y_out, 'b-', label='L2 Loss')plt.plot(x_array, l1_y_out, 'r--', label='L1 Loss')plt.plot(x_array, phuber1_y_out, 'k-.', label='P-Huber Loss (0.25)')plt.plot(x_array, phuber2_y_out, 'g:', label='P-Huber Loss (5.0)')plt.ylim(-0.2, 0.4)plt.legend(loc='lower right', prop=&#123;'size': 11&#125;)plt.show()# 你能从后面的两个损失函数中得到什么规律呢？ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import tensorflow as tffrom tensorflow.python.framework import opsimport matplotlib.pyplot as pltops.reset_default_graph()sess = tf.Session()# Various predicted X valuesx_vals = tf.linspace(-3., 5., 500)# Target of 1.0target = tf.constant(1.)targets = tf.fill([500,], 1.)# 分类损失函数# Hinge Loss 合页损失函数# 具体请见公式二hinge_y_vals = tf.maximum(0., 1. - tf.multiply(target, x_vals))hinge_y_out = sess.run(hinge_y_vals)# 交叉熵损失xentropy_y_vals = - tf.multiply(target, tf.log(x_vals)) - tf.multiply((1. - target), tf.log(1. - x_vals))xentropy_y_out = sess.run(xentropy_y_vals)# sigmoid 交叉熵x_val_input = tf.expand_dims(x_vals, 1)target_input = tf.expand_dims(targets, 1)xentropy_sigmoid_y_vals = tf.nn.softmax_cross_entropy_with_logits(logits = x_val_input, labels = target_input)xentropy_sigmoid_y_out = sess.run(xentropy_sigmoid_y_vals)# 权重softmax 交叉熵损失函数weight = tf.constant(0.5)xentropy_weighted_y_vals = tf.nn.weighted_cross_entropy_with_logits(x_vals, targets, weight)xentropy_weighted_y_out = sess.run(xentropy_weighted_y_vals)# 画出这些损失函数x_array = sess.run(x_vals)plt.plot(x_array, hinge_y_out, 'b-', label='Hinge Loss')plt.plot(x_array, xentropy_y_out, 'r--', label='Cross Entropy Loss')plt.plot(x_array, xentropy_sigmoid_y_out, 'k-.', label='Cross Entropy Sigmoid Loss')plt.plot(x_array, xentropy_weighted_y_out, 'g:', label='Weighted Cross Entropy Loss (x0.5)')plt.ylim(-1.5, 3)#plt.xlim(-1, 3)plt.legend(loc='lower right', prop=&#123;'size': 11&#125;)plt.show()# 具体损失函数是干嘛用的，那就是为了具体的数据预测给提供一个最优化的目标，为了让每一类任务有一个最小化目标而构造出来的loss函数，在机器学习里面最重要的其实有一项就是损失函数的设计，设计一个好的损失函数，会让我们的网络更加的稳定，更加容易收敛和收敛到一个相对最优值# 没有掌握这些基本概念的，希望自己先找一些这方面的知识来看一看，然后再理解的写代码，这样会事半功倍。 公式一： \\[ L_{\\delta}(i) = {\\delta}^2 (\\sqrt{1 + (a/{\\delta})^2} - 1) \\] 公式二： \\[ max(0, 1 - (pre - y)) \\] 公式三： \\[ L = -actual * (log(pre)) - (1- actual)(log(1-pre)) \\] 公式四： \\[ L = -actual * (log(sigmoid(pre))) - (1- actual)(log(1- sigmoid(pre))) \\] 公式五： \\[ L = -actual * (log(pre)) * weights - (1-actual)(log(1-pre)) \\] Back Propagation 这个地方不要紧张，我这里给你推荐一个网站，上面有很好理解这个算法的解释。 机器学习基础以及反向传播算法介绍 12345678910111213141516171819202122232425262728293031323334353637383940# 下面是一个回归的例子# 老样子，我们创建tensorflow的会话，使用默认的计算图import tensorflow as tfimport numpy as npimport matplotlib.pyplot as pltfrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()# 一个回归的例子# 创建数据x_vals = np.random.normal(1, 0.1, 100) # x数据y_vals = np.repeat(10., 100) # y 数据x_data = tf.placeholder(shape=[1], dtype=tf.float32) # 占位符y_target = tf.placeholder(shape=[1], dtype=tf.float32) # label（真值）A = tf.Variable(tf.random_normal(shape=[1]))my_output = tf.multiply(x_data, A)# 使用l2 lossloss = tf.square(my_output - y_target)init = tf.global_variables_initializer()sess.run(init)# 创建了一个反向传播优化器my_opt = tf.train.GradientDescentOptimizer(0.02)train_step = my_opt.minimize(loss) # 最小化loss# 开始我们的迭代训练for i in range(100): rand_index = np.random.choice(100) rand_x = [x_vals[rand_index]] rand_y = [y_vals[rand_index]] sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) if (i+1)%25==0: print('Step #' + str(i+1) + ' A = ' + str(sess.run(A))) print('Loss = ' + str(sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;))) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# 下面是一个分类的例子import tensorflow as tfimport numpy as npimport matplotlib.pyplot as pltfrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()# 分类的例子# 创建数据x_vals = np.concatenate((np.random.normal(-1, 1, 50), np.random.normal(3, 1, 50)))y_vals = np.concatenate((np.repeat(0., 50), np.repeat(1., 50)))# 创建占位符x_data = tf.placeholder(shape=[1], dtype=tf.float32)y_target = tf.placeholder(shape=[1], dtype=tf.float32)A = tf.Variable(tf.random_normal(mean=10, shape=[1]))my_output = tf.add(x_data, A)my_output_expanded = tf.expand_dims(my_output, 0)y_target_expanded = tf.expand_dims(y_target, 0)# 是不是使用的是对应的分类损失函数呀 sigmoid cross entropyxentropy = tf.nn.sigmoid_cross_entropy_with_logits(logits = my_output_expanded, labels = y_target_expanded)my_opt = tf.train.GradientDescentOptimizer(0.05)train_step = my_opt.minimize(xentropy)init = tf.global_variables_initializer()sess.run(init)for i in range(1400): rand_index = np.random.choice(100) rand_x = [x_vals[rand_index]] rand_y = [y_vals[rand_index]] sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) if (i+1)%200==0: print('Step #' + str(i+1) + ' A = ' + str(sess.run(A))) print('Loss = ' + str(sess.run(xentropy, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)))# 测试 predictions = []for i in range(len(x_vals)): x_val = [x_vals[i]] prediction = sess.run(tf.round(tf.sigmoid(my_output)), feed_dict=&#123;x_data: x_val&#125;) predictions.append(prediction[0]) accuracy = sum(x==y for x,y in zip(predictions, y_vals))/100.print('Ending Accuracy = ' + str(np.round(accuracy, 2))) o^o,今天我们就讲到这里，下节我们再见，总的来说，就是在回归和分类问题中，设计相对应的loss函数，然后使用反向传播优化器起优化loss，使得loss逐渐减小","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]},{"title":"asyncio 不完全指北（一）","slug":"2018-04-09/guide-to-asyncio-1","date":"2018-04-14T19:37:56.000Z","updated":"2018-08-07T13:19:42.438Z","comments":true,"path":"posts/b496f296/","link":"","permalink":"http://weafteam.github.io/posts/b496f296/","excerpt":"","text":"前言 众所周知，Python 的并发编程主要由线程、进程和协程三个组件组成，我们可以使用 Python 模块 threading、multiprocessing 和 yield 句法去操纵它们。后来，又有了更高层的封装：concurrent.futures 和 asyncio 模块。concurrent.futures是对 threading 和 multiprocessing 的封装，不是这篇文章的重点；asyncio 是 Python 中的重大变化，也代表了未来的发展趋势，所以这篇文章打算讲讲 asyncio。 什么是 asyncio asyncio 一开始是 Python 的作者 Guido van Rossum 在 Python 仓库之外开发的，代号为“Tulip”，在 Python 3.4 时加入标准库。asyncio模块使用事件循环驱动的协程实现并发，提供了基于协程来构建并发程序的工具。作为对比，threading 模块通过应用级线程实现并发；multiprocessing 模块使用系统级进程实现并发；而 asyncio使用单线程、单进程，其中应用程序的各个部分在事件循环的驱动下进行协作，在最佳时间显式切换任务。asyncio 不但支持通常情况下出现阻塞型 IO 时的上下文切换，还支持调度，来让代码在指定的将来时间运行，并且还可以让一个协程等待另一个协程完成。 asyncio 中的几个概念 事件循环：asyncio 提供的框架以事件循环为中心，它是负责有效处理 I / O 事件、系统事件和应用程序上下文切换的第一类对象。Python 提供了几个循环实现，通常会自动选择合理的默认值，但也可以选择特定的事件循环实现。同样也有一些第三方的实现，例如 uvloop。应用程序将要执行的代码注册到事件循环中，代表允许事件循环在必要时对代码进行调用。当调用结束，或无法继续时，应用程序会让出控制权，交还给事件循环。 协程（coroutine ）：将控制权交还给事件循环的机制来自于 Python 的协程，这是一种特殊的函数，它将控制权交还给调用方而不会丢失本身状态。协程类似于生成器函数，事实上可以在 Python 3.5 之前的版本中用生成器实现协程。asyncio 还为 Protocols 和 Transports 提供了基于类的抽象层，使用回调的代码风格。在基于类的模型和协程模型中，通过重新进入事件循环来显式更改上下文将取代 Python 线程实现中的隐式上下文更改。 future：future是一种对象，表示待完成的操作的结果。事件循环可以监视 future 对象直到它完成，从而允许应用程序的一部分等待另一部分完成某些工作。除了 future，asyncio 还包括其他并发原语，例如锁和信号量。通常情况下，我们不应该自行创建 future，只能通过并发框架（例如 asyncio）来实例化。原因是 future代表终将发生的事情，而某件事的发生，是通过安排好这件事的执行时间来确定的。只有当我们把某件事交给事件循环处理时，事件循环才会给这件事排期，从而创建一个 future对象。 任务（Task）：任务是 future的子类，它包装并管理协程的执行。任务可以通过事件循环进行调度，以便在它们需要的资源可用时运行，并生成可由其他协程使用的结果。 使用协程处理多任务协作 协程是为并发设计的语言概念。协程函数在调用时创建协程对象，然后调用方可以使用协程的 send () 方法运行函数。协程可以在另一个协程中使用 await关键字暂停执行。当它被暂停时，协程的状态被保持，允许它在下次被唤醒时恢复到它停止的位置。 启动协程 启动一个协程最简单的方式是将一个协程传递给事件循环的 run_until_complete() 方法： 123456789101112131415import asyncioasync def coroutine(): print('in coroutine')event_loop = asyncio.get_event_loop()try: print('starting coroutine') print('entering event loop') event_loop.run_until_complete(coroutine())finally: print('closing event loop') event_loop.close() 1234starting coroutineentering event loopin coroutineclosing event loop 首先，我们通过 asyncio.get_event_loop() 获取了一个默认事件循环的引用。run_until_complete() 方法接受一个协程对象，并用它启动事件循环，然后在协程通过 return 结束时停止事件循环。 协程的返回值 协程的返回值返回给启动并等待它的程序： 1234567891011121314import asyncioasync def coroutine(): print('in coroutine') return 'result'event_loop = asyncio.get_event_loop()try: return_value = event_loop.run_until_complete(coroutine()) print(f'it returned: &#123;return_value!r&#125;')finally: event_loop.close() 12in coroutineit returned: 'result' 链式调用协程 一个协程可以启动另一个协程并等待它返回结果。下面的示例包含两个阶段，它们必须按顺序执行，但可以与另外的操作同时运行： 12345678910111213141516171819202122232425262728import asyncioasync def phase1(): print('in phase1') return 'result1'async def phase2(arg): print('in phase2') return f'result2 derived from &#123;arg&#125;'async def main(): print('in main') print('waiting for result1') result1 = await phase1() print('waiting for result2') result2 = await phase2(result1) return (result1, result2)event_loop = asyncio.get_event_loop()try: return_value = event_loop.run_until_complete(main()) print(f'return value: &#123;return_value!r&#125;')finally: event_loop.close() 123456in mainwaiting for result1in phase1waiting for result2in phase2return value: ('result1', 'result2 derived from result1') 在这里使用了 await 关键字，并没有将新的协程添加到事件循环中。因为控制流已经在由事件循环管理的协程内部，所以不需要通知事件循环管理新的协程。 使用生成器语法 async 和 await 关键字出现于 Python 3.5，对于 Python 3.5 之前的版本，可以使用 asyncio.coroutine 装饰器和 yield from 来实现相同的功能： 12345678910111213141516171819202122232425262728293031import asyncio@asyncio.coroutinedef phase1(): print('in phase1') return 'result1'@asyncio.coroutinedef phase2(arg): print('in phase2') return f'result2 derived from &#123;arg&#125;'@asyncio.coroutinedef main(): print('in main') print('waiting for result1') result1 = yield from phase1() print('waiting for result2') result2 = yield from phase2(result1) return (result1, result2)event_loop = asyncio.get_event_loop()try: return_value = event_loop.run_until_complete(main()) print(f'return value: &#123;return_value!r&#125;')finally: event_loop.close() 参考资料 Asynchronous Concurrency Concepts Cooperative Multitasking with Coroutines","categories":[],"tags":[]},{"title":"Vigenere密码加密解密","slug":"2018-04-09/Vigenere密码加密解密","date":"2018-04-14T14:11:24.000Z","updated":"2018-04-15T12:46:07.825Z","comments":true,"path":"posts/8b092926/","link":"","permalink":"http://weafteam.github.io/posts/8b092926/","excerpt":"","text":"今天换个口味，写点原来从没接触过的东西–密码学。前一阵信息安全课上留了一个作业，实现Vigenere加密解密，借着机会写篇博客。这次博客由于比较仓促，这次只写加密解密系统的实现，不涉及唯密文破解。 任务要求： a.编程实现Vigenere加密/解密系统，并分析和评估该算法的安全性。 b.编程实现唯密文破译系统，能够破译密钥为2到4个字符的Vigenere密文，并分析如何加快破译速度。 时间要求： 布置任务后，在3周之内完成。 提交结果：已设计并测试好的程序，包括源码、可执行程序、测试数据集、实验报告。 原理介绍： 先普及下Caesar密码，作为单密码简单替换密码届的扛把子，他有着不可动摇的地位，它的原理很简单，对于需要加密的每个字符都进行相同大小的平移。先给出Caesar密码加密的字符对应表，如下： 举个例子吧：明文为China，它对应的数字应为2 7 8 13 0.比如我们平移距离为3，那么加密之后的密文应该为FKLQD，简单到炸。 我们先不谈上述加密方法的缺点，这些我们放到唯密文解密中聊。有了以上的基础之后我们再聊Vigenere加密以及解密，它是使用一系列凯撒密码组成密码字母表的加密算法，属于多表密码的一种简单形式。同样先给出它的密码加密字符对应表（自己画太麻烦了，我就在百科上扒了一个图，溜。。）： 上图中的维吉尼亚表的第一列代表着密钥字母（这是有别于Caesar密码的地方），第一行代表着明文字母，行列分别使用当前需要加密字符和当前的密钥字符确定当前明文字符对应着的密文字符。 这里我们说一下，一般情况下，我们给出的密钥是短于我们的明文长度的。所以我们做Vigenere加密的时候第一步做的就是对照明文长度，补齐密钥字串。 说了这么多，举个例子说下吧： 例如我们的明文字串为：data security 密钥：best 按照上述的规则，第b行，第d列，对应的字符为E，….. 加密之后的密文应为：EELTTIUNSMLR（不区分大小写）。 实现 第一步：使得密钥字符串长度与明文长度相同 1234567891011121314151617181920public String dealKey(String str,String Key)&#123; Key=Key.toUpperCase();// 将密钥转换成大写 Key=Key.replaceAll(\"[^A-Z]\", \"\");//去除所有非字母的字符 StringBuilder stringBuilder = new StringBuilder(Key); String newKey=\"\"; if(sstringBuilder.length()!=str.length())&#123; //如果密钥长度与str不同，则需要生成密钥字符串 if(stringBuilder.length()&lt;str.length())&#123; //如果密钥长度比str短，则以不断重复密钥的方式生成密钥字符串 while(stringBuilder.length()&lt;str.length())&#123; stringBuilder.append(Key); &#125; &#125; //此时，密钥字符串的长度大于或等于str长度 //将密钥字符串截取为与str等长的字符串 newKey=stringBuilder.substring(0, str.length()); &#125; return newKey; &#125; 第二步：加密 其实我们不用将上述的Vigenere密码表列出来，一是列出来费时费力费空间，二是一个简单的取余操作就能解决这个事情。 12345678910111213141516private String PwTable = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\";public String Encryption(String P,String K)&#123; P = P.toUpperCase();// 将明文转换成大写 P = P.replaceAll(\"[^A-Z]\", \"\");//去除所有非字母的字符 K = dealKey(P,K); int len = K.length(); StringBuilder stringBuilder = new StringBuilder(); for(int i = 0;i &lt; len;i++)&#123; int row=PwTable.indexOf(K.charAt(i));//行号 int col=PwTable.indexOf(P.charAt(i));//列号 int index = (row+col)%26; stringBuilder.append(PwTable.charAt(index)); &#125; return stringBuilder.toString(); &#125; 第三步：解密 这个过程其实是比较有趣的，我们取密钥所在行为解密的行号，密文所在列作为我们的列号，我们需要分两种情况考虑： 首先说一下第一种情况，将上述的密码表从主对角线分开，一是密文在我们的密码表的右上部，这种情况比较简单，其实就是一个加密过程的逆过程，我们此时的密文字符在密码表中的位置肯定是不比其对应的明文靠后的（理解这句话，这种情况其实也就明白了，再简单点讲就是PwTable.indexof(密文)&gt;PwTable.indexof(对应的明文)），此时我们只要让列号减去行号，然后将结果做indexof操作就能得到相应的明文。 如果你理解了第一种情况，第二种情况也就好理解了，此时我们的密文字符在我们的密码表中的位置肯定是不比其对应的明文的位置靠前的，我们需要将列号加一圈字符表再去减行号。 理解之后下面的这些代码应该就不难理解了。 12345678910111213141516171819public String Decryption(String C,String K)&#123; C=C.toUpperCase();// 将密文转换成大写 C=C.replaceAll(\"[^A-Z]\", \"\");//去除所有非字母的字符 K=dealKey(C,K); int len = K.length(); StringBuilder stringBuilder=new StringBuilder(); for(int i = 0;i&lt;len;i++)&#123; int row = PwTable.indexOf(K.charAt(i));//行号 int col = PwTable.indexOf(C.charAt(i));//列号 int index; if(row&gt;col)&#123; index=col+26-row; &#125;else&#123; index=col-row; &#125; stringBuilder.append(PwTable.charAt(index)); &#125; return sb.toString(); &#125; 以上是本篇博客的全部内容，希望对你有所帮助，感谢驻足~","categories":[{"name":"密码学","slug":"密码学","permalink":"http://weafteam.github.io/categories/密码学/"}],"tags":[{"name":"密码学","slug":"密码学","permalink":"http://weafteam.github.io/tags/密码学/"}]},{"title":"如何理解丘奇计数","slug":"2018-04-02/how-to-understand-church-numerals","date":"2018-04-07T19:39:29.000Z","updated":"2018-08-07T13:19:42.437Z","comments":true,"path":"posts/cd89a3b2/","link":"","permalink":"http://weafteam.github.io/posts/cd89a3b2/","excerpt":"","text":"前言 不想写 Python 了，这次换个主题：丘奇计数，又名 lambda 演算的自然数表示法。 什么是 lambda 演算 lambda 演算（也称为 λ 演算）是数学逻辑中的一种形式系统，它基于函数抽象和应用，使用变量绑定和替换来表示计算。 没错，上面这句话来自维基百科，基本上是一句正确的废话，看完了也不知道什么是 lambda 演算。不过这篇文章的重点不在 lambda 演算上，希望你已经了解了一些关于 lambda 演算的知识。如果有机会下一篇再展开说（可能 什么是自然数 在计算机科学和集合论中，我们把非负整数 \\((0, 1, 2, 3, 4...)\\) 称为自然数。皮亚诺给出了自然数的严格定义： \\(0\\) 是自然数； 如果 \\(n\\) 是自然数，那么 \\(n+1\\) 也是自然数（\\(n+1\\) 代表 \\(n\\) 的后继）； \\(0\\) 不是任何一个数的后继； 如果 \\(m\\) 与 \\(n\\) 都是自然数且 \\(m\\neq n\\)，那么 \\(n+1 \\neq m+1\\)； 设 \\(P(n)\\) 为关于自然数 \\(n\\) 的一个性质，如果 \\(P(0)\\) 正确， 且假设 \\(P(n)\\) 正确，则 \\(P(n+1)\\) 亦正确。那么 \\(P(n)\\) 对一切自然数 \\(n\\) 都正确。 存在一个集合 \\(N\\)，称其元素为自然数，当且仅当这些元素满足公理 1 - 5（也就是皮亚诺公理）。 在自然数集合上可以定义一组运算：加法、乘法等等，这里用加法举个例子： 1234def add(m, n): if n == 0: return m return add(m, n - 1) + 1 可以看出加法是由两条规则递归定义的： $ m + 0 = m$ \\(m + (n + 1) = (m + n) + 1\\) lambda 演算的自然数表示法 自然数当然不止可以用皮亚诺公理定义，丘奇首先把自然数和自然数上的运算定义在了 lambda 演算上，所以称之为丘奇计数。 下面就要开始丘奇计数的定义了，由于 lambda 演算的样子不太友好，所以还是用 Python 表示。 首先定义 0： 1zero = lambda f: lambda x: x 先不管它为什么是 0，让我们看看这个语句。它定义了一个接受一个参数 f 的函数，返回一个函数，这个函数接受一个参数 x，然后返回它。似乎很简单，但是它为什么是 0？ 把它放在一边，看看 1 的定义： 1one = lambda f: lambda x: f(x) 这个语句是什么意思呢？定义了一个接受一个参数 f 的函数，返回一个函数，这个函数接受一个参数 x，然后返回 f(x) 的调用结果。好像有些规律了，再看看 2： 1two = lambda f: lambda x: f(f(x)) 定义了一个接受一个参数 f 的函数，返回一个函数，这个函数接受一个参数 x，然后返回 f(f(x)) 的调用结果。 现在可以清楚的看到，每个自然数的后继都多调用了一次 f，自然数被表示为 f 的调用次数。 于是，我们可以很轻易的写出后继函数： 1succ = lambda n: lambda f: lambda x: f(n(f)(x)) 这个函数接受一个参数 n（也就是上面被定义的 0，1，2 等等），返回一个函数，这个函数在 n 的基础上多执行了一次 f，达到了求 n 的后继的目的。 现在让我们忘记 1 的定义，用 0 和后继重新定义一次： 12345678zero = lambda f: lambda x: xsucc = lambda n: lambda f: lambda x: f(n(f)(x))one = succ(zero) = (lambda n: lambda f: lambda x: f(n(f)(x)))(lambda f: lambda x: x) = lambda f: lambda x: f(((lambda f: lambda x: x)(f))(x)) = lambda f: lambda x: f((lambda x: x)(x)) = lambda f: lambda x: f(x) 和我们预想的完全一致。 加法 接下来试着定义一下加法，加法是两个数相加返回一个数（也就是说，加法是定义在自然数上的幺半群），所以签名长这样： 1add = lambda m: lambda n: lambda f: lambda x: ... 函数体呢？我们推广一下后继函数：后继函数在n的基础上多调用了一次 f，相当于 +1；那加法相当于 +m，也就是多调用 m 次 f，于是可以得出： 1add = lambda m: lambda n: lambda f: lambda x: m(f)(n(f)(x)) 乘法 乘法的签名也是一样： 1mul = lambda m: lambda n: lambda f: lambda x: ... 我们都知道乘法是从加法推广而来的：m * n 相当于加 m 次 n，所以可以使用加法的定义： 1mul = lambda m: lambda n: lambda f: lambda x: m(add(n))(zero)(f)(x) 上面的写法正确，不过太丑了，可以化简为： 1mul = lambda m: lambda n: lambda f: lambda x: m(n(f))(x) 求幂 求幂的签名也一样： 1pow = lambda m: lambda n: lambda f: lambda x: ... 求幂是由乘法推广而来的：mn 相当于乘 n 次 m，所以可以使用乘法的定义： 1pow = lambda m: lambda n: lambda f: lambda x: n(mul(m))(one)(f)(x) 同样可以化简为： 1pow = lambda m: lambda n: lambda f: lambda x: n(m)(f)(x) 结语 机智的同学一定发现我们并没有实现减法，这是因为减法的实现太复杂了。至于为什么减法的实现很复杂，以及如何实现减法，这里有一篇参考资料 ，有兴趣的话可以自行了解一下。","categories":[],"tags":[]},{"title":"chapter-04-AIR","slug":"2018-04-02/chapter-04-AIR","date":"2018-04-05T02:13:07.000Z","updated":"2018-04-22T07:10:52.883Z","comments":true,"path":"posts/466eca41/","link":"","permalink":"http://weafteam.github.io/posts/466eca41/","excerpt":"","text":"TensorFLow 基础（1） hi,又和大家见面了，上一次我们讲了建立模型步骤和一些基础的概念（Tensor、Placeholder），那么我们这次就继续我们的矩阵操作，因为在TensorFlow处理一些数学问题的时候，往往都是通过矩阵来存储数据，通过特定的矩阵运算，我们实现数据的处理，从而得到一些数据的特性。还有一些其他的Tensorflow的概念，我希望大家能坚持下去，只要将这些基础的概念学会，那么以后运用TensorFlow就会得心应手。 和TensorFlow一起工作的Matrices 12345678910111213141516171819202122232425262728293031# 矩阵和矩阵操作import tensorflow as tfimport numpy as npfrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()identity_matrix = tf.diag([1., 1., 1])print(sess.run(identity_matrix)) #注意这个地方，如果是TensorFlow的Tensor，# 那么使用sess的run方法才能将结果显示# 或者下面这种方式print(identity_matrix.eval(session = sess))A = tf.truncated_normal([2, 3]) # 2 * 3 大小 均值0 方差为1.print(sess.run(A))B = tf.fill([2, 3], 5.) # 2 * 3 使用5.填充print(sess.run(B))C = tf.random_uniform([3, 2]) # 3 * 2 随机初始化print(sess.run(C))D = tf.convert_to_tensor(np.array([[1., 2., 3.], [-3., -7., -1.], [0., 5., -2.]]))print(sess.run(D))print(sess.run(A+B)) # 加法和 tf.add() 一样print(sess.run(B-B)) # 减法和 tf.subtract() 一样print(sess.run(tf.matmul(B, identity_matrix))) # 矩阵乘法print(sess.run(tf.transpose(C))) # 矩阵转置print(sess.run(tf.matrix_determinant(D))) # 计算行列式print(sess.run(tf.matrix_inverse(D))) # 矩阵的逆print(sess.run(tf.cholesky(identity_matrix))) # cholesk分解（平方根分解）eigenvalues, eigenvectors = sess.run(tf.self_adjoint_eig(D)) # 求特征向量和特征值print(eigenvalues)print(eigenvectors) Math Operation（数学操作） 123456789101112131415161718192021222324252627282930import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()# math operationprint(sess.run(tf.div(3, 4)))print(sess.run(tf.truediv(3, 4)))print(sess.run(tf.floordiv(3., 4.)))print(sess.run(tf.mod(22., 5.)))print(sess.run(tf.cross([1., 0., 0.], [0., 1., 0.])))# Trig operationprint(sess.run(tf.sin(3.1416)))print(sess.run(tf.cos(3.1416)))print(sess.run(tf.div(tf.sin(3.1416 / 4.), tf.cos(3.1416 / 4.))))# custom operation# f(x) = 3 * x^2 - X + 10test_nums = range(15) # 生成一个listdef custom_polynomial(x_val): return (tf.subtract(3 * tf.square(x_val), x_val) + 10)print(sess.run(custom_polynomial(11)))# list expendexpected_output = [3 * x * x - x + 10 for x in test_nums]print(expected_output)for num in test_nums: print(sess.run(custom_polynomial(num))) Activation Function（激活函数） 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 激活函数主要是为了让神经网络模型具有非线性的特性import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()x_vals = np.linspace(start = -10, stop = 10, num = 100)# Relu Activation-&gt; max(0, x)print(sess.run(tf.nn.relu([-3., 3., 10.])))y_relu = sess.run(tf.nn.relu(x_vals))# Relu6 Activation-&gt; min(max(x, 0), 6)print(sess.run(tf.nn.relu6([-3., 3., 10.])))y_relu6 = sess.run(tf.nn.relu6(x_vals))# Sigmoidactivation-&gt; 见公式1print(sess.run(tf.nn.sigmoid([-1., 0., 1.])))y_sigmoid = sess.run(tf.nn.sigmoid(x_vals))# Hyper Tangent activation-&gt;见公式2print(sess.run(tf.nn.tanh([-1., 0., 1.])))y_tanh = sess.run(tf.nn.tanh(x_vals))# softsign activation-&gt;见公式3print(sess.run(tf.nn.softsign([-1., 0., 1.])))y_softsign = sess.run(tf.nn.softsign(x_vals))# softplus activation-&gt;见公式4print(sess.run(tf.nn.softplus([-1., 0., 1.])))y_softplus = sess.run(tf.nn.softplus(x_vals))# Exponential linear activation-&gt;见公式5print(sess.run(tf.nn.elu([-1., 0., 1.])))y_elu = sess.run(tf.nn.elu(x_vals))plt.plot(x_vals, y_softplus, 'r--', label='Softplus', linewidth=2)plt.plot(x_vals, y_relu, 'b:', label='ReLU', linewidth=2)plt.plot(x_vals, y_relu6, 'g-.', label='ReLU6', linewidth=2)plt.plot(x_vals, y_elu, 'k-', label='ExpLU', linewidth=0.5)plt.ylim([-1.5,7])plt.legend(loc='upper left')plt.show()plt.plot(x_vals, y_sigmoid, 'r--', label='Sigmoid', linewidth=2)plt.plot(x_vals, y_tanh, 'b:', label='Tanh', linewidth=2)plt.plot(x_vals, y_softsign, 'g-.', label='Softsign', linewidth=2)plt.ylim([-2,2])plt.legend(loc='upper left')plt.show()下图给出激活函数的曲线图 公式1： \\[ \\sigma(x)=\\frac{1}{1+e^{-x}} \\] 公式2： \\[ f(x)=\\frac{e^x-e^{-x}}{e^x + e^{-x}} \\] 公式3： \\[ f(x) = \\frac{1}{1 + |x|} \\] 公式4 \\[ f(x) = \\log(1 + e^x) \\] 公式5： \\[ elu(x) = \\begin{cases} x &amp; x &gt; 0 \\\\ \\alpha(exp(x) - 1) &amp; x \\leq 0 \\end{cases} \\] Operations on a Computational Graph 1234567891011121314151617181920212223242526272829import osimport matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()# 创建的数据是要喂给下面的placeholder的x_vals = np.array([1., 3., 5., 7., 9.])# 创建placeholderx_data = tf.placeholder(tf.float32)# 创建一个乘数m = tf.constant(3.)# 乘法prod = tf.multiply(x_data, m)for x_val in x_vals: print(sess.run(prod, feed_dict = &#123;x_data: x_val&#125;))#下面将数据计算图输出到文件里面，供我们后来启动tensorboard使用merged = tf.summary.merge_all(key = 'summary')if not os.path.exists('tensorboard_logs/'): os.makedirs('tensorboard_logs/')my_writer = tf.summary.FileWriter('./tensorboard_logs/', sess.graph) Layering Nested Operations 123456789101112131415161718192021222324252627282930313233343536373839404142import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tfimport osfrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()# 创建数据为了feedmy_array = np.array([[1., 3., 5., 7., 9.], [-2., 0., 2., 4., 6.], [-6., -3., 0., 3., 6.]])# 复制x_vals = np.array([my_array, my_array + 1])# 声明placeholderx_data = tf.placeholder(tf.float32, shape = [3, 5])# 声明常数来操作m1 = tf.constant([[1.], [0.], [-1.], [2.], [4]])m2 = tf.constant([[2.]])a1 = tf.constant([[10.]])# 声明操作prod1 = tf.matmul(x_data, m1)prod2 = tf.matmul(prod1, m2)add1 = tf.matmul(prod2, a1)# 打印验证结果for x_val in x_vals: print(sess.run(add1, feed_dict = &#123;x_data: x_val&#125;)) #下面将数据计算图输出到文件里面，供我们后来启动tensorboard使用merged = tf.summary.merge_all(key = 'summaries')if not os.path.exists('tensorboard_logs/'): os.makedirs('tensorflow_logs/')my_writer = tf.summary.FileWriter('tensorboard_logs/', sess.graph)#下图就是在操作过程中，tensorflow建立的图运算模型 Working With Multiple Layers 123456789101112131415161718192021222324252627282930313233343536373839404142434445import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tfimport osfrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()x_shape = [1, 4, 4, 1]# 定义一个4 * 4 大小的随机矩阵x_val = np.randim.uniform(size = x_shape)x_data = tf.placeholder(tf.float32, shape = x_shape)# 定义一个空间移动窗口，也就是卷积操作的卷积核# 大小是2 * 2， 步长是 2# filter的值是一个固定的值0.25my_filter = tf.constant(0.25, shape = [1, 2, 2, 1])my_strides = [1, 2, 2, 1]mov_avg_layer = tf.nn.conv2d(x_data, my_filter, my_strides, padding = 'SAME', name = 'Moving_Avg_Window')# 第二层def custom_layer(input_matrix): input_matrix_sqeezed = tf.squeeze(input_matrix) A = tf.constant([1., 2.], [-1., 3.]) b = tf.constant(1., shape = [2, 2]) output = tf.add(tf.matmul(A, input_matrix_sqeezed), b) return tf.nn.relu(output)with tf.name_scope('custom_layer') as scope: custom_layer1 = custom_layer(mov_avg_layer)# 运行结果print(sess.run(mov_avg_layer, feed_dict = &#123;x_data: x_val&#125;))print(sess.run(custom_layer1, feed_dict = &#123;x_data: x_val&#125;))#下面将数据计算图输出到文件里面，供我们后来启动tensorboard使用merged = tf.summary.merge_all(key = 'summaries')if not os.path.exists('tensorboard_logs/'): os.makedirs('tensorboard_logs/')my_writer = tf.summary.FileWriter('tensorboard_logs', sess.graph)# 下图是计算图 总结：这一次，一开始主要讲了矩阵的一些操作，后续又进行了数学操作，激活函数，运算图、层内元素嵌套运算还有最好的多层运算，并给出了tensorboard的计算图结构。大家不仅仅要看一看，也要动手做一做哦。 一如既往的有什么问题可以直接联系milittle，air@weaf.top邮箱","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]},{"title":"Nginx和Git的离线安装","slug":"2018-03-26/offline-install-git-and-nginx","date":"2018-04-01T04:07:12.000Z","updated":"2018-04-22T07:28:18.410Z","comments":true,"path":"posts/a86291b8/","link":"","permalink":"http://weafteam.github.io/posts/a86291b8/","excerpt":"一、准备工作 一般情况下为了确保安装没有任何问题，我们先使用有网络环境的下安装的方法，去检测当前机器具体需要安装什么冬天链接库，然后按照提示缺失的库去下载相应的库。 我们按照正常的流程，去解压nginx 1tar -zxvf nginx-1.13.8.tar.gz 进入解压后的目录执行 12cd nginx-1.13.8./configure 出现以下错误： 我们按照有网络环境的方法去检测缺失的库及其版本。 1yum -y install gcc gcc-c++ autoconf automake make","text":"一、准备工作 一般情况下为了确保安装没有任何问题，我们先使用有网络环境的下安装的方法，去检测当前机器具体需要安装什么冬天链接库，然后按照提示缺失的库去下载相应的库。 我们按照正常的流程，去解压nginx 1tar -zxvf nginx-1.13.8.tar.gz 进入解压后的目录执行 12cd nginx-1.13.8./configure 出现以下错误： 我们按照有网络环境的方法去检测缺失的库及其版本。 1yum -y install gcc gcc-c++ autoconf automake make 显示如下： 我们下载号相应的库 下边提供几个下载的网址： http://mirrors.163.com/centos/6/os/x86_64/Packages/ http://rpmfind.net/ https://pkgs.org 下边是下载好的库 安装相应的库（集体安装情况具体分析）： 12345678910111213rpm -ivh mpfr-2.4.1-6.el6.x86_64.rpmrpm -ivh cpp-4.4.7-18.el6.x86_64.rpmrpm -Uvh tzdata-2016j-1.el6.noarch.rpmrpm -Uvh glibc-common-2.12-1.209.el6.x86_64.rpm glibc-2.12-1.209.el6.x86_64.rpm glibc-headers-2.12-1.209.el6.x86_64.rpm glibc-devel-2.12-1.209.el6.x86_64.rpm kernel-headers-2.6.32-696.el6.x86_64.rpmrpm -ivh libgomp-4.4.7-18.el6.x86_64.rpmrpm -Uvh libstdc++-4.4.7-18.el6.x86_64.rpmrpm -ivh libstdc++-devel-4.4.7-18.el6.x86_64.rpmrpm -ivh ppl-0.10.2-11.el6.x86_64.rpmrpm -ivh cloog-ppl-0.15.7-1.2.el6.x86_64.rpmrpm -Uvh libgcc-4.4.7-18.el6.x86_64.rpmrpm -ivh gcc-4.4.7-18.el6.x86_64.rpmrpm -ivh gcc-c++-4.4.7-18.el6.x86_64.rpmrpm -ivh automake-1.11.1-4.el6.noarch.rpm autoconf-2.63-5.1.el6.noarch.rpm 然后执行./configure 发现错误： 然后安装一下库 1234567rpm -Uvh pcre-7.8-7.el6.x86_64.rpmrpm -ivh pcre-devel-7.8-7.el6.x86_64.rpmrpm -Uvh zlib-1.2.3-29.el6.x86_64.rpmrpm -ivh zlib-devel-1.2.3-29.el6.x86_64.rpmrpm -i --force --nodeps krb5-devel-1.10.3-65.el6.x86_64.rpmrpm -Uvh openssl-1.0.1e-57.el6.x86_64.rpmrpm -ivh openssl-devel-1.0.1e-57.el6.x86_64.rpm 然后执行 123./configuremakemake install 安装完成 二、查看版本信息 ===== 根据安装完成的信息查看nginx. 三、简介 —– 不同操作系统的Linux的安装可能不太一样。 本教程使用的是CentOS或者RHEL。 四、准备环境 —- 如果有网络的情况下肯定相当容易： 123yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel gcc perl-ExtUtils-MakeMakeryum install git 若果yum源安装的版本较低，不执行yum install git命令，并按照四、五步骤操作。 如果是离线需要完成后续操作： 首先我们需要到官网下载相应的安装包 https://www.kernel.org/pub/software/scm/git/ 选择tar.gz 三、下载和安装依赖 —— 然后最麻烦的地方就是依赖动态链接库的下载。 需要下载的库可以到这两个网站上去找： fr2.rpmfind.net Linux Packages Search-pkgs.org 我这里提供了一些 1234567891011121314151617181920212223cpio-2.11-24.el7.x86_64.rpmcurl-7.29.0-42.el7.x86_64.rpmexpat-2.1.0-10.el7_3.x86_64.rpmexpat-devel-2.1.0-10.el7_3.x86_64.rpmgdbm-devel-1.10-8.el7.x86_64.rpmgettext-0.19.8.1-2.el7.x86_64.rpmgettext-devel-0.19.8.1-2.el7.x86_64.rpmkrb5-devel-1.15.1-8.el7.x86_64.rpmlibcurl-7.29.0-42.el7.x86_64.rpmlibcurl-devel-7.29.0-42.el7.x86_64.rpmlibdb-devel-5.3.21-20.el7.x86_64.rpmopenssl-1.0.2k-8.el7.x86_64.rpmopenssl-devel-1.0.2k-8.el7.x86_64.rpmperl-5.16.3-292.el7.x86_64.rpmperl-devel-5.16.3-292.el7.x86_64.rpmperl-ExtUtils-CBuilder-0.28.2.6-292.el7.noarch.rpmperl-ExtUtils-Install-1.58-292.el7.noarch.rpmperl-ExtUtils-MakeMaker-6.68-3.el7.noarch.rpmperl-ExtUtils-Manifest-1.61-244.el7.noarch.rpmperl-ExtUtils-ParseXS-3.18-3.el7.noarch.rpmsystemtap-sdt-devel-3.1-3.el7.x86_64.rpmzlib-1.2.7-17.el7.x86_64.rpmzlib-devel-1.2.7-17.el7.x86_64.rpm 这是git需要的一些库，需要安装的不是很多，但是安装的库也需要依赖。 123456789rpm -ivh perl-5.16.3-292.el7.x86_64.rpmrpm -ivh perl-devel-5.16.3-292.el7.x86_64.rpmrpm -ivh zlib-devel-1.2.7-17.el7.x86_64.rpmrpm -ivh libcurl-devel-7.29.0-42.el7.x86_64.rpmrpm -ivh curl-7.29.0-42.el7.x86_64.rpmrpm -ivh zlib-devel-1.2.7-17.el7.x86_64.rpmrpm -ivh openssl-devel-1.0.2k-8.el7.x86_64.rpmrpm -ivh perl-ExtUtils-MakeMaker-6.68-3.el7.noarch.rpmrpm -ivh gettext-devel-0.19.8.1-2.el7.x86_64.rpm 以上库需要安装，并需要安装对应依赖。 有时候有些包可能互相依赖，安装时可使用一下命令 1rpm -ivh perl-ExtUtils-MakeMaker-6.68-3.el7.noarch.rpm perl-ExtUtils-Install-1.58-292.el7.noarch.rpm zlib-devel-1.2.7-17.el7.x86_64.rpm 编译安装可能需要的包 12345678910rpm -ivh cloog-ppl-0.15.7-1.2.el6.x86_64.rpmrpm -ivh cpp-4.4.7-18.el6.x86_64.rpmrpm -ivh gcc-4.4.7-18.el6.x86_64.rpmrpm -ivh gcc-c++-4.4.7-18.el6.x86_64.rpmrpm -ivh libgcc-4.4.7-18.el6.x86_64.rpmrpm -ivh libgomp-4.4.7-18.el6.x86_64.rpmrpm -ivh libstdc++-4.4.7-18.el6.x86_64.rpmrpm -ivh libstdc++-devel-4.4.7-18.el6.x86_64.rpmrpm -ivh mpfr-2.4.1-6.el6.x86_64.rpmrpm -ivh ppl-0.10.2-11.el6.x86_64.rpm 有时会发生冲突可以使用枪支卸载，或者不考虑依赖安装。 12345rpm -e --nodeps mariadb-libs-5.5.56-2.el7.x86_64 //强制卸载rpm -i --force --nodeps krb5-devel-1.15.1-8.el7.x86_64.rpm //强制安装 --force可选 五、解压和安装 解压安装包 1tar -zxvf git-2.15.1.tar.gz 进入解压后的文件夹 执行一下命令 1./configure 检查没有任何出错 然后执行以下命令进行编译 1make 检查没有问题执行安装 1make install 检查没有任何出错 六、查看安装结果 —- 1git --version","categories":[{"name":"Linux","slug":"Linux","permalink":"http://weafteam.github.io/categories/Linux/"}],"tags":[{"name":"Linux运维","slug":"Linux运维","permalink":"http://weafteam.github.io/tags/Linux运维/"}]},{"title":"TensorFlow 建立网络模型","slug":"2018-03-26/chapter-03-AIR","date":"2018-03-31T10:23:35.000Z","updated":"2018-04-02T14:12:49.208Z","comments":true,"path":"posts/5b7df854/","link":"","permalink":"http://weafteam.github.io/posts/5b7df854/","excerpt":"","text":"TensorFlow 建立网络模型 上次一我们在fashion-mnist上面体验了一把，但是里面有一些建立模型和一些TensorFlow的基础概念都没有给大家讲，所以这节决定将这方面的知识介绍一些，上节是为了引起大家的注意，TensorFlow具有很强大的功能，我们只能后续慢慢的学习。 其实在上一次的实例中，有很多地方确实是很困惑的，如果没有接触过机器学习的小伙伴可能理解起来会有一些问题，那么我开头就稍微讲一下，机器学习有一些什么？就我现在了解的一些内容给大家介绍，有可能有一些不到位的地方，还请多多包涵： 其实机器学习，总的宗旨就是利用数据的特征来做识别和分类等任务 第一大类是分类工作，假设有一百类，经典的做法，就是使用神经网络提取一些数据的特征，然后利用softmax输出层进行不同种类概率的预测： \\[ softmax(i) = \\frac{X_i}{\\sum_{i=0,99}X_i} \\] 上面是softmax层计算的公式，从一百类里面找出每一类的概率值，然后按照概率值来预测输入数据是哪一种类型，就像上一次文章里面的fashion-mnist的数据一样，会预测出输出的类别。softmax(i)代表的就是这个种类的概率值，取最大值作为预测类别。 你可以把一个矩阵看成一个数据集合，一行是一个数据信息，就和我们的关系型数据一样，一行代表一个表的一条信息，那么每列就是每一行数据的一个属性，那么在机器学习里面就是数据的特征了，因为在网络模型中，每个特征都有对应的权重，那么，对于每个特征来说，对于最后的分类，识别等工作起的重要程度是不一样的。这也和我们的数据库信息差不多，有一些信息也是无关紧要的。有些信息可以主要决定这一行数据。 第二大类就是回归，回归可以看作是一个连续的分类，对于二维数据来说，其实就是根据你给出的数据来拟合一条线。对于三维来讲就是拟合一个平面。再高维就是超平面。 最近，也就是2018年3月31在加利福尼亚州山景城的计算机历史博物馆举办了第二届TensorFlow开发者峰会，会上有超过500名使用TensorFlow的用户，还有一些观众，大家有兴趣的话可以关注youtube的TensorFlow官方频道。可以查看开会的视频。 TensorFlow应用广泛，其中有使用TensorFlow来做开普勒任务分析的 也有使用TensorFlow预测心脏发作和中风概率 还有一些应用在现实当中的项目。 这让我们认识到TensorFlow对于实际领域中应用的越来越广泛，所以我们不学习是不是有点亏。这么好的开源项目。 上一次我们既然做过了一次服装类别识别，那么这次我主要从TensorFlow建立模型的步骤讲起：让大家再深入理解一下TensorFlow。 第一步也是很重要的一步，那就是导入数据。 第二步一般就是对数据进行的预处理，一般包括归一化数据，转换数据等操作。 第三步设置算法的超参数，一般也就是学习率，batch_size(批处理个数)，epoch(轮次)。这里举一个例子，假如你有10000条训练数据。那么，batch_size设置为100，那么你的一个epoch就迭代100次才能将所有数据训练一遍，每次输入数据是100条，因为一个epoch的意思就是训练完一次训练数据，所以一个epoch是迭代100次就可以结束一轮了。learning_rate一般设置为0.1-0.0001之间，但是也不排除一些特殊情况，主要是learning_rate设置的过小，反向传播更新参数的时候速度会很慢，设置的过大，会出现无法收敛的情况。 第四步设置变量和placeholders，变量是记录权重和偏置项信息的，一般在最小化loss函数的时候，反向传播算法会更新权重和偏置项，TensorFlow导入数据是通过placeholders来实现的，大家还记得我们上次的fashion-mnist识别，我的数据就是通过先定义placeholders，最后在Session运行的时候，在feed_dict这个字典参数里面将训练数据喂进去的。 1234a_var = tf.constant(42)x_input = tf.placeholder(tf.float32, [n_x, None], name=\"X\")x_output = tf.placeholder(tf.float32, [n_y, None], name=\"y\")# 定义输入数据的一些方式 第五步定义图模型，我们有了数据，初始化了变量和placeholders，那我们就需要定义一个图模型，来生成TensorFlow的图模型（计算图）我们必须告诉TensorFlow对我们的数据进行哪些操作，来让我们的模型具有预测能力（更加深入的运算我们在后续的博客里面会陆续讲到） 1h_pre_output = tf.add(tf.matmul(W, x_input) + B) 第六步声明loss函数，在上面计算图中我们定义了一些对我们数据的操作。那么我们需要验证我们预测的输出，和我们真实之间的差距，一般对于回归任务来讲的话，就是平方误差：这样就求得了平方误差。但是对于分类任务，那就是交叉熵误差。就像上一节我们用到的loss生成函数就是softmax这种方式。，交叉熵的公式后续用到再给大家介绍。 \\[ loss(i)=\\frac{1}{N}\\sum{_i}(y\\_pre_i-y\\_true_i)^2 \\] 12TensorFlow求法：loss = tf.reduce_mean(tf.square(y_pre - y_true)) 第七步声明了loss函数以后，我们需要使用BP算法也就是反向传播算法来更新权重和偏置项。在TenorFlow框架里面有好多这样的优化器，都在 tf.train这个模块里面。 12optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)这个就是我们上次使用的优化器，来优化我们的loss 最后一步那就是初始化会话Session()，开始训练模型 123with tf.Session() as session: session.run(init) ..... 由上面的步骤，大家再结合上一次的网络代码，是不是可以理解了TensorFlow在建立一个网络模型的时候的具体步骤。 其实在TensorFlow中还有一个很重要的概念，那就是Tensor，上次说过了它的概念，那么接下来我讲一下TensorFlow里面的Tensor。 1234567891011121314151617181920212223242526import tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()# 定义一个会话，记得，TensorFlow里面都是通过session来执行的sess = tf.Session()# 创建一个1 * 20的向量tensor_zeros = tf.zeros([1, 20])sess.run(tensor_zeros) # 你可以运行一下看看my_var = tf.Variable(tf.zeros([1, 20])) # 使用tenso来初始化变量sess.run(my_var.initializer) # 又一种运行变量初始化器的方式sess.run(my_var) #打印出来看看# tf.ones() 生成全是1# tf.zeros() 生成全是0# tf.constant() 生成一个常量Tensor# 如果我们想要通过一个已知的Tensor来创建另一个，则可以使用ones_like()和zeros_like()这两个函数zero_similar = tf.Variable(tf.zeros_like(tensor_zeros))sess.run(zero_similar.initializer)print(sess.run(zero_similar))# 注意上面的两个函数的参数是为了确定生成Tensor的大小，而产生的值是通过函数决定的tf.fill([row, col], -1) # 用具体的数字填充tf.linspace(start=0.0, stop=1.0, num=3) # 线性分布 包括endtf.range(start=6, limit=15, delta=3) # 也是线性均匀 不包括endtf.random_normal([row_dim, col_dim], mean=0.0, stddev=1.0) # 随机 均值0 方差1.0tf.random_uniform([row_dim, col_dim], minval=0, maxval=4) # 或者最小最大值随机初始化 1234567891011121314151617import tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()my_var = tf.Variable(tf.zeros([1,20]))merged = tf.summary.merge_all()writer = tf.summary.FileWriter(\"./tmp/variable_logs\", graph=sess.graph)initialize_op = tf.global_variables_initializer()sess.run(initialize_op)# 上面的就是一个Tensor放在一个变量里面，我们使用了一条语句 merged = tf.summary.merge_all() 还有writer = tf.summary.FileWriter(\"/tmp/variable_logs\", graph=sess.graph)，这两句这是为了将变量在TensorBoard里面显示出来，让我们更加了解TensorFLow的一些操作。# 上面的操作过程会在当前文件夹里面创建一个/tmp/variable_logs文件夹然后会将变量信息存储在一个文件里面 那怎么使用tensorboard 12#进去我们的环境变量，然后执行tensorboard --logdir=tmp的绝对路径 可以看到我上面执行的命令。然后在浏览器里面输入127.0.0.1:6006然后你就可以看到刚才那个变量的操作过程，这就是tensorboard的魅力 上面就是一个变量在进行初始化时候可视化显示 Placeholders使用(一样可以使用tensorboard来查看) 123456789101112131415import numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()# 定义一个placeholderx = tf.placeholder(tf.float32, shape = (4, 4))# 随机生成4 * 4的矩阵reand_array = np.random.rand(4, 4)y = tf.identity(x) # 返回与输入对象相同的内容和大小print(sess.run(y, feed_dict=&#123;x: rand_array&#125;))merged = tf.summary.merge_all()writer = tf.summary.FileWriter(&quot;./tmp/variable_logs&quot;, sess.graph) 总结 这次我们就TensorFlow的一些基础概念的介绍，也是为了让大家在以后的TensorFlow使用过程中少一些疑问，后面的章节，我们会慢慢深入。小伙伴们不要着急，我的邮箱是air@weaf.top，依旧是那个可以交流学习的milittle。谢谢大家的驻足。 第一篇 TensorFlow安装 第二篇 TensorFlow初体验（fasion-mnist识别） 修改pip全局镜像方法","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]},{"title":"文本聚类系列教程：（三）构建词袋空间VSM（Vector Space Model）","slug":"2018-03-26/文本聚类系列教程：（三）构建词袋空间VSM（Vector-Space-Model）","date":"2018-03-30T06:00:08.000Z","updated":"2018-04-14T05:59:22.791Z","comments":true,"path":"posts/a751f7e5/","link":"","permalink":"http://weafteam.github.io/posts/a751f7e5/","excerpt":"","text":"咱们今天先聊个概念吧，著名的聚类假设，这也是文本聚类的依据，内容如下：该假设认为，同类的文档相似度较大，而不同类的文档相似度较小。 概念： 对于上述概念，也就是做文本聚类的基础，如果不相关的文档反而相似度高，我们便无法做文本聚类。 接下来再说VSM(Vector Space Model),对于VSM的定义，我在网上搜罗了些资料，如下所示： Vector space model (or term vector model) is an algebraic model for representing text documents (and any objects, in general) as vectors of identifiers, such as, for example, index terms. It is used in information filtering, information retrieval, indexing and relevancy rankings. Its first use was in the SMART Information Retrieval System. A document is represented as a vector. Each dimension corresponds to a separate term. If a term occurs in the document, its value in the vector is non-zero. Several different ways of computing these values, also known as (term) weights, have been developed. One of the best known schemes is tf-idf weighting. The definition of term depends on the application. Typically terms are single words, keywords, or longer phrases. If the words are chosen to be the terms, the dimensionality of the vector is the number of words in the vocabulary (the number of distinct words occurring in the corpus). 拙劣的翻译： 向量空间模型是用来表示文本文档（通常也包含一些对象）的特征向量的代数模型，例如索引词项。它被应用于信息过滤、信息检索、索引和相关度计算。这个模型最早被应用于SMART信息检索系统。 一个文本文档表示一个向量。每一个维度相当于一个单独的词项（term）。如果一个词项（term）出现在一个文档中，那么它在表示该文档的向量中对应项不为0.有一些计算这些词项（term）权重的方法被逐渐提出来，其中最著名的方法就是tf-idf权重计算方法。 对于词项（term）的定义依赖于应用。一般而言，词项（term）可以是单词、关键字、或者长短语。如果单词作为词项（term），那么向量中的维度就是词汇表中的单词的个数（出现在文档全集中所有不同的单词的数量）。 小荔枝： 举个荔枝吧 ，方便理解上述的概念。首先假设有这样两个文本 1.我来到北京清华大学 2.他来到了网易杭研大厦 分词结果为：我/来到/北京/清华大学和他/来到/了/网易/杭研/大厦统计所有文档的词集合：我/来到/北京/清华大学/他/了/网易/杭研/大厦，按照1983停用词去除停用词后结果为：来到/北京/清华大学/网易/杭研/大厦 我们对这两个文本构建向量，结果如下 来到 北京 清华大学 网易 杭研 大厦 文本1 1 1 1 0 0 0 文本2 1 0 0 1 1 1 相信你已经对VSM的认识有了一个大致的轮廓，但是细心的你也可能发现了，我们在上述的例子中计算term值的方法仅仅只是计数，这样的term值是否有意义呢？我们是否能用这样的方法直接进行接下来的计算呢？对于前一个问题，答案是肯定的。不管在此基础上做什么样的改进，我们最基础的就是统计单词出现的次数，那就让我们先把上述的代码实现一下吧(与该文件同目录下有个名为txt1的文件夹，里面有1.txt和2.txt两个文件，内容分别是上述所说的两个文档，我们在上次RmStopWord.py的基础上再做修改)： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import sysimport jiebaimport osimport numpy as npfrom os import pathd = path.dirname(__file__) # 获取当前文件的dir路径stopwords_path = 'stopwords1893.txt' # 停用词表路径#text = open(path.join(d, text_path),'rb').read()def read_from_file(file_name): with open(file_name,\"r\") as fp: words = fp.read() return wordsdef RmStopWords(text): mywordlist = [] seg_list = jieba.cut(text, cut_all=False) liststr=\"/ \".join(seg_list) # 添加切分符 f_stop = open(stopwords_path) try: f_stop_text = f_stop.read() finally: f_stop.close( ) f_stop_seg_list=f_stop_text.split('\\n') # 停用词是每行一个，所以用/n分离 for myword in liststr.split('/'): #对于每个切分的词都去停用词表中对比 if not(myword.strip() in f_stop_seg_list) and len(myword.strip())&gt;1: mywordlist.append(myword) return mywordlistdef get_all_vector(file_path): names = [ os.path.join(file_path,f) for f in os.listdir(file_path) ] txts = [ open(name).read() for name in names] docs = [] word_set = set() for txt in txts: doc = RmStopWords(txt) docs.append(doc) word_set |= set(doc) word_set = list(word_set) docs_vsm = [] # 这里只是想显示有多少term for word in word_set[:30]: print(word) for doc in docs: temp_vector = [] for word in word_set: temp_vector.append(doc.count(word) * 1.0) docs_vsm.append(temp_vector) docs_matrix = np.array(docs_vsm) return docs_matrix # txt2 = RmStopWords(read_from_file(text1_path))# print(txt2)#文件路径为txt1/1.txt和2.txt，只不过我让程序循环扫描txt1下所有的文本文件txt3 = get_all_vector('txt1')print(txt3) 运行结果： 分析： 上述结果不言而喻，那么我们接着讨论，显而易见，我既然提出了第二个疑问就一定有它被提出的道理，仅仅只计算term值的方法显然存在问题，我们再随便举个例子，文本1中北京只出现了1次，但是文本1中只有3个单词，文本2中北京出现了10次但是文本2中有1000个单词，那我们用上述的方法显然不合适。所以接下来我们便要讲一个最著名的方法tf-idf计算权值的方法。 TF-IDF(term frequency–inverse document frequency) 维基百科和百度百科上的讲的很清楚，这里截取概念方便大家阅读，更详细的内容请参考前面所说的两个百科。 TF-IDF是一种统计方法，用以评估一个词(term)对于一个文件集或者一个语料库中的一份文件的重要程度。一个词(term)的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。 原理： TF-IDF的主要思想是：如果某个词或短语(term)在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语(term)具有很好的类别区分能力，适合用来分类。如果包含词条term的文档越少，也就是n越小，则IDF越大，则说明词条term也具有很好的类别区分能力。 思考： 现在想一下我们刚才提出的问题，针对我们上述的问题：同一词语在长文件里可能会比短文件有更高的词数，而不管该词重要与否。那么我们对词数做归一化就可以了，而TF就帮我们做了这样的事。那么我们就先给出TF的运算公式吧。 \\(tf_i,_j = \\frac{n_i,_j}{\\sum_k n_k,_j}\\) TF公式解读：上式中分子是该词在文件中出现的次数，而分母则是该词在文件中出现的词数之和。 我们再讲个小问题： 如果某一类文档C中包含词条t的文档数为m，而其他类包含t的文档总数为k，显然所有包含t的文档数n=m+k，而当m变大的时候，n也变大，这是后按照IDF的计算方法计算得到的IDF值会变小，也就相对应的说明该词条t类别区分能力不强。但是实际上，如果一个词条在一个类的文件中频繁出现，则说明该词条能够很好的代表这个类的文本的特征，这样的词条应该给它们赋予较高的权重，并选来作为该类文本的特征词以区别与其它类文档。其实这就是IDF的不足。 针对这个问题，我的想法是TF-IDF用来做信息检索和数据挖掘，为了获取更精准的效果，我们宁愿忽略这样不足来换取更加理想的效果（也就是TF-IDF计算出更大的权值）。（这里我的理解是这样的，如果有人有更好的解释，欢迎与我进行讨论，邮箱：well@weaf.top） 那么接下来就该给出IDF的计算公式了： \\(idf(t,D) = log(\\frac{N}{\\lvert {d \\in D, t \\in d}\\rvert})\\) IDF公式解读： |D|：语料库中文件的总数 分子为包含该词条t的文件数目，如果该词条不在语料库中，就会导致分母为零，因此一般使用1。 那就接着我们上述代码，运用TF-IDF，把对应的矩阵的单纯计数转换成权值计算吧： 123456789101112131415161718192021222324252627282930313233343536373839404142434445···def get_all_vector(file_path): names = [ os.path.join(file_path,f) for f in os.listdir(file_path) ] posts = [ open(name).read() for name in names] docs = [] word_set = set() for post in posts: doc = RmStopWords(post) docs.append(doc) word_set |= set(doc) word_set = list(word_set) docs_vsm = [] for word in word_set[:30]: print(word) for doc in docs: temp_vector = [] for word in word_set: temp_vector.append(doc.count(word) * 1.0) docs_vsm.append(temp_vector) docs_matrix = np.array(docs_vsm) #return docs_matrix column_sum = [ float(len(np.nonzero(docs_matrix[:,i])[0])) for i in range(docs_matrix.shape[1]) ] column_sum = np.array(column_sum) column_sum = docs_matrix.shape[0] / column_sum idf = np.log(column_sum) idf = np.diag(idf) i = 0 for doc_v in docs_matrix: if doc_v.sum() == 0: docs_matrix[i] = docs_matrix[i]/1 else: docs_matrix[i] = docs_matrix[i] / (doc_v.sum()) i+=1 tfidf = np.dot(docs_matrix,idf) return names,tfidftxt3 = get_all_vector(&apos;txt1&apos;)print(txt3) 结果： 本次的学习会用到很多numpy的知识，请大家自行查阅。如有兴趣，请思考为什么在新的权值矩阵中“来到”一词的权重变成了0。感谢大家的阅读~","categories":[{"name":"文本聚类","slug":"文本聚类","permalink":"http://weafteam.github.io/categories/文本聚类/"}],"tags":[{"name":"文本聚类","slug":"文本聚类","permalink":"http://weafteam.github.io/tags/文本聚类/"}]},{"title":"修改pip全局镜像","slug":"2018-03-26/alterM","date":"2018-03-27T12:29:36.000Z","updated":"2018-04-10T08:57:00.655Z","comments":true,"path":"posts/233074e6/","link":"","permalink":"http://weafteam.github.io/posts/233074e6/","excerpt":"","text":"修改pip全局镜像 第一次我们在windows上面安装了Anaconda，在使用pip安装Tensorflow中速度过慢，所以我为大家介绍一中修改全局pip源的方法（这样在使用pip下载依赖库的时候就会快一些）： 打开用户主目录：我的是C:\\Users\\milittle。 在里面新建pip文件夹，在pip文件夹中建立pip.ini文件。 在pip.ini文件中添加如下配置信息，我使用的豆瓣源： 123[global]timeout = 6000index-url = https://pypi.douban.com/simple 最后的目录结构就是：C:\\Users\\milittle\\pip\\pip.ini","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]},{"title":"如何理解描述符","slug":"2018-03-19/how-to-understand-descriptor","date":"2018-03-25T15:52:32.000Z","updated":"2018-08-07T13:19:42.436Z","comments":true,"path":"posts/5dd0238f/","link":"","permalink":"http://weafteam.github.io/posts/5dd0238f/","excerpt":"","text":"前言 上篇文章中挖了 property 和描述符的坑，这篇就把它填上好了_(:з)∠)_ property 是用描述符实现的，所以先说说 property。 property property 本身是一个实现了描述符协议的类，在不改变类接口的情况下，提供了一组对实例属性的读取、写入和删除操作。下面举个例子，一个银行账户的抽象，很容易实现： 12345class Account: def __init__(self, name, balance): self.name = name self.balance = balance 银行账户最常见的操作就是存款和取款了： 1234567891011121314In [1]: account = Account('zhang', 100) # 创建一个有 100 块存款的账户In [2]: account.balanceOut[2]: 100In [3]: account.balance -= 90 # 取 90 块In [4]: account.balance # 还剩 10 块Out[4]: 10In [5]: account.balance += 30 # 存 30 块In [6]: account.balance # 现在有 40 块Out[6]: 40 但是这里有个问题： 123456...In [7]: account.balance -= 50 # 再取 50 块In [8]: account.balance # 存款变成了负数！Out[8]: -10 当然这种操作是不该被允许的，我们需要对 balance 的写入做限制。Jawa 之类的语言会创建一组 getter、setter 方法来管理属性，但是这并不 Python，也对现有的代码不友好。正确的方式是使用 property。 12345678910111213141516class Account: def __init__(self, name, balance): self.name = name self.balance = balance @property def balance(self): return self._balance @balance.setter def balance(self, value): if value &lt; 0: raise ValueError('balance must greater than 0.') else: self._balance = value 现在 balance 被禁止设为小于 0 的数： 123456789101112131415161718192021In [1]: account = Account('zhang', 100)In [2]: account.balanceOut[2]: 100In [3]: account.balance += 40In [4]: account.balance -= 200---------------------------------------------------------------------------ValueError Traceback (most recent call last)...ValueError: balance must greater than 0.In [5]: account.balanceOut[5]: 140In [6]: account = Account('zhang', -1) # 初始化的时候也不行！---------------------------------------------------------------------------ValueError Traceback (most recent call last)...ValueError: balance must greater than 0. 可以看到我们使用 balance 的方式没有发生变化，但是对值的限制已经生效了。 property 还有一个 deleter 装饰器，处理应用于属性的 del；当然，del 本身用的也不多，大多数时候把销毁操作交给 Python 就可以了。不过如果涉及到复杂对象的引用，要做到 RAII（误，还是要手动实现的。 property 是类 property 本身是用 C 实现的，这里有一个纯 Python 的实现。正如上文所说，它本身是一个类，构造方法的签名如下： 123class property(object): def __init__(self, fget=None, fset=None, fdel=None, doc=None): pass 熟悉一点装饰器用法的话就可以看出上面的 123456class Account: ... @property def balance(self): pass 实际上就是 1234567class Account: ... def get_balance(self): pass balance = property(fget=get_balance) 如果不熟悉的话，下一篇就讲装饰器好了（误 property 的实例是类属性 上面的代码段同时展示了这样一个事实：property 的实例是类属性。这就涉及到了属性查找顺序的问题，简单试一下： 123456class Foo: data = 'data!' @property def bar(self): return 'bar!' 123456789101112In [1]: f = Foo()In [2]: f.dataOut[2]: 'data!'In [3]: f.data = 'f.data!'In [4]: f.dataOut[4]: 'f.data!'In [5]: Foo.dataOut[5]: 'data!' 实例属性覆盖了类属性，符合直觉。那么对 property 的实例来说呢？ 12345678In [6]: f.barOut[6]: 'bar!'In [7]: f.bar = 'bar'---------------------------------------------------------------------------AttributeError Traceback (most recent call last)...AttributeError: can't set attribute 尝试给 bar 赋值，失败了，也符合 property 的工作方式：执行赋值时，如果没有 setter 方法就抛出异常。那么直接修改 f.__dict__ 呢？ 1234In [8]: f.__dict__['bar'] = 'bar'In [9]: f.barOut[9]: 'bar!' 也不行，property 的实例完全覆盖了实例属性。但是，它是一个类属性，所以我们可以这样做： 123456789101112In [10]: Foo.barOut[10]: &lt;property at 0x29c44800408&gt;In [11]: Foo.bar = 'bar'In [12]: f.barOut[12]: 'bar'In [13]: f.bar = 'ba'In [14]: f.barOut[14]: 'ba' 对类属性的覆盖使 bar 不再是一个 property 的实例，所以也就不会覆盖后续的赋值了。 当然我们仍然可以用一个 property 的实例再次覆盖 Foo.bar： 1234In [15]: Foo.bar = property(fget=lambda self: 'bar!')In [16]: f.barOut[16]: 'bar!' 恢复原样。 property 的实例这种先从类中开始属性查找的方式，是一类描述符的工作模式。接下来就说说描述符。 描述符 描述符是指实现了描述符协议的类，这个协议包含四个方法，分别是 __get__，__set__，__delete__ 和 Python 3.6 新增的 __set_name__。通常，只要实现了 __get__ 或 __set__，就可以被称之为描述符。在某个角度上说，描述符的作用相当于抽象的 property，可以为一组属性提供相同的读取、写入和删除逻辑。接下来，还是从数据验证的例子开始。 下面是商店中一项商品的抽象，包含商品名、数量和单价： 12345678class Item: amount = Storage('amount') price = Storage('price') def __init__(self, name, amount, price): self.name = name self.amount = amount self.price = price 其中的 amount 和 price 都必须大于 0，所以可以用统一的描述符实现： 12345678910class Storage: def __init__(self, name): self.name = name def __set__(self, instance, value): if value &gt; 0: instance.__dict__[self.name] = value else: raise ValueError(f'&#123;self.name&#125; must greater than 0.') 由于我们并没有对读取方法有特别的需求，所以不用实现 __get__ 方法。 试一下： 1234567891011In [1]: item = Item('orange', 100, 0)---------------------------------------------------------------------------ValueError Traceback (most recent call last)...ValueError: price must greater than 0.In [2]: item = Item('orange', 0, 100)---------------------------------------------------------------------------ValueError Traceback (most recent call last)...ValueError: amount must greater than 0. 如果 amount 或 price 中的任何一个不大于 0，都会被禁止。 这里需要解释一下 __set__ 的签名中的 instance： 12def __set__(self, instance, value): pass instance 是 Item 的实例。因为描述符应该管理实例的属性，所以需要额外的参数提供相应的实例。这也是为什么我们不能这样写： 12def __set__(self, instance, value): self.__dict__[self.name] = value 这实际上是为描述符实例设置了值，而描述符实例是Item 类的类属性，所有的 Item 实例都共享相同的描述符实例。修改了某个描述符实例，相当于修改了所有的 Item 实例。 上面的例子有个缺点，初始化描述符实例的时候需要重复属性的名字。我们希望可以简单的写成： 1234class Item: amount = Storage() price = Storage() ... 而不需要在描述符的构造方法中重复属性名。这就是 Python 3.6 新增的 __set_name__ 方法的作用。只要实现 __set_name__ 方法： 12345class Storage: ... def __set_name__(self, owner, name): self.name = name 同样解释一下函数签名： 12def __set_name__(self, owner, name): pass owner 是 Item 类本身，name 是引用描述符实例的变量的名字。 如果使用的 Python 版本在 3.6 以下呢？有两个方法：第一个是用元类接管Item类的创建过程，这个不在这篇文章的内容之内（可能又挖了一个坑；第二个就是为每个描述符实例生成与属性名无关但是唯一字符串，用来代替属性名： 1234567891011121314151617class Storage: _counter = 0 def __init__(self): cls = self.__class__ self.name = f'_&#123;cls.__name__&#125;#&#123;cls._counter&#125;' cls._counter += 1 def __get__(self, instance, owner): return getattr(instance, self.name) def __set__(self, instance, value): if value &gt; 0: setattr(instance, self.name, value) else: raise ValueError('must greater than 0.') 由于 Item 中的属性名和我们实际保存的属性名不同，所以需要实现 __get__ 方法。与 __set_name__ 签名中的 owner 含义相同，__get__ 方法签名中的 owner 也是 Item 类本身。 现在，我们使用 _Storage#N 这样的名称在 Item 实例中保存属性。当然，这样的名称会让人有点困惑，特别是以类属性访问的时候： 12345In [1]: Item.amount---------------------------------------------------------------------------AttributeError Traceback (most recent call last)...AttributeError: 'NoneType' object has no attribute '_Storage#0' 为了避免在如此明显的地方暴露我们的实现细节，我们可以修改异常的错误消息，或者，内省描述符实例： 12345def __get__(self, instance, owner): if instance is None: return self else: return getattr(instance, self.name) 两类描述符 上述例子中对数据属性的控制和管理是描述符的典型用途之一。这种实现了 __set__ 方法，接管了设置属性行为的描述符，被称为覆盖型描述符，没有定义 __set__ 方法的描述符，被称为非覆盖型描述符。由于 Python 中对实例属性和类属性的处理方式不同，这两类描述符也有不同的行为。 覆盖型描述符 实现了 __set__ 方法的描述符就是覆盖型描述符。这类描述符虽然是类属性，但是会覆盖实例属性的赋值操作： 123456789101112class Override: def __get__(self, instance, owner): print('get!') def __set__(self, instance, value): print('set!')class Manager: override = Override() 下面做一些实验： 123456789101112131415161718In [1]: m = Manager()In [2]: m.overrideget!In [3]: m.override = 1set!In [4]: Manager.overrideget!In [5]: m.__dict__['override'] = 1In [6]: m.__dict__Out[6]: &#123;'override': 1&#125;In [7]: m.overrideget! 可以看出，无论以实例属性还是类属性访问 override，都会触发 __get__ 方法；为实例属性 override 赋值会触发 __set__ 方法；即使跳过描述符直接为 m.__dict__ 赋值，读取 override 的操作仍然会被描述符覆盖。 没有 __get__ 方法的覆盖型描述符 如果只实现了 __set__ 会发生什么呢？ 123456789class OverrideNoGet: def __set__(self, instance, value): print('set!')class Manager: override_no_get = OverrideNoGet() 123456789101112131415161718192021222324In [1]: m = Manager()In [2]: m.override_no_getOut[2]: &lt;__main__.OverrideNoGet at 0x29c44a97668&gt;In [3]: Manager.override_no_getOut[3]: &lt;__main__.OverrideNoGet at 0x29c44a97668&gt;In [4]: m.override_no_get = 1set!In [5]: m.override_no_getOut[5]: &lt;__main__.OverrideNoGet at 0x29c44a97668&gt;In [6]: m.__dict__['override_no_get'] = 1In [7]: m.override_no_getOut[7]: 1In [8]: m.override_no_get = 2set!In [9]: m.override_no_getOut[9]: 1 可以看到，没实现 __get__ 方法，无论以实例属性还是类属性访问 override_no_get，都会返回描述符实例；而赋值操作可以触发 __set__ 方法；由于我们的 __set__ 方法并没有真正修改实例属性，所以再次访问 override_no_get 仍然会得到描述符实例；通过 m.__dict__ 修改实例属性后，实例属性就会覆盖描述符；不过只有访问实例属性时才是如此，赋值仍然由 __set__ 处理。 非覆盖型描述符 没有实现 __set__ 方法的描述符就是非覆盖型描述符： 123456789class NonOverride: def __get__(self, instance, owner): print('get!')class Manager: non_override = NonOverride() 1234567891011121314151617181920In [1]: m = Manager()In [2]: m.non_overrideget!In [3]: Manager.non_overrideget!In [4]: m.non_override = 1In [5]: m.non_overrideOut[5]: 1In [6]: Manager.non_overrideget!In [7]: del m.non_overrideIn [8]: m.non_overrideget! 无论访问实例属性还是类属性，都会触发 __get__ 方法；由于没有 __set__ 方法，对属性的赋值不会被干涉；对属性复制之后，实例属性就会覆盖同名的描述符，但是访问类属性仍然可以触发 __get__ 方法；如果把 non_override 从实例中删除，访问 non_override 的操作又会交给 __get__。 当然，描述符都是定义在类上的，如果对同名的类属性进行赋值，就会完全替换掉描述符。这里表现出读、写属性时的不对等：对类属性的读操作可以被 __get__ 处理，但是写操作不会。当然，了解一些 Python 的话就会知道还存在着另一种不对等：读取实例属性时，会返回实例属性，如果实例属性不存在，会返回类属性；但是为实例属性赋值时，如果实例属性不存在，会在实例中创建属性，不会影响到类属性。 结语 描述符充斥在 Python 底层（举个例子：Python 中的方法是怎么实现的？）与各种框架中，理解描述符是体会 Python 世界工作原理和设计美学的重要方式。","categories":[],"tags":[]},{"title":"TensorFlow 初体验 （Fashion-mnist）","slug":"2018-03-19/chapter-02-AIR","date":"2018-03-25T13:18:37.000Z","updated":"2018-04-02T14:12:49.205Z","comments":true,"path":"posts/b0821049/","link":"","permalink":"http://weafteam.github.io/posts/b0821049/","excerpt":"","text":"TensorFlow 初体验（Fashion-mnist） 接着上一讲的内容，想必大家已经通过我的教程安装好了TensorFlow了吧，那我们这节课通过安装简单的跨平台的集成开发环境Spyder，在这个集成开发环境上面实现一些python程序。具体安装过程见如下阐述： 首先在应用程序里面找到Anaconda应用程序，打开里面的Anaconda Navigator，然后打开以后，选中我们上次建立好的环境tensorflow。 选中tensorflow这个环境变量以后，看到里面有一个集成开发环境叫spyder，这个工具就是今天我们要安装的，我的已经安装好了，所以是Launch，你们的没有安装好，所以是install状态，点解安装就好。（这个地方也可能需要翻墙）。 这个安装好以后，你就会在应用文件夹里面出现一个Spyder(tensorflow)这个应用程序，以后你就从应用文件夹启动就好。 那么启动以后：我也是启动了，出现了以下的情况：不慌，慢慢来。 看到上面的错误，这个错误提示是因为没有安装jedi这个依赖库，而且要求版本要大于0.9.0。那我们接下来解决一下这个问题。 小插曲，一下就可以解决，具体操作步骤: 还是打开上次那个AnacondaPrompt的命令行 进去以后，执行activate tensorflow 相当于你要在这个环境下面给这个spyder安装这个依赖 进去以后，执行pip install jedi==0.9.0 就可以了，然后重启spyder（可以直接在这个环境里面输入spyder命令就可以实现spyder的启动，你也可以在应用文件夹里面启动，性质是一样的） 不出什么意外的话，spyder使用就没有问题了，有什么问题可以发邮件给我！！！ 解决了上面的小插曲以后，我们在spyder中输入以下代码进行测试。 123456789101112import tensorflow as tfsess = tf.Session()init = tf.global_variables_initializer() # 此处的init是全局变量初始化器，# TensorFlow的session必须执行这个初始化器才能执行前面建立好的图，# 所以，这个是很重要的一点，后续也会强调#（也就是后续再网络中建立变量就是通过那个初始化器来进行初始化工作的）# 其实在没有变量的时候，这个初始化器是不需要的# 但是为了让大家形成习惯，还是写上sess.run(init)hello = tf.constant('hello world')print(sess.run(hello)) 上图中左面是代码书写区域，右面上半部分是变量查看区域，还有文件夹区域可以切换，右面下半部分是执行console区域，我输入上面的代码，执行以后console区域打出hello world字符串。 从上面的一些简单的测试以后，我们进入今天的主题，fashion-minist的识别，fashion-minist是一个服装识别的一个数据集，在这个数据集之前有一个mnist手写体识别数据集，这个手写数据集对应我们手写的十个数字，然后通过设计网络来识别手写体。但是今天我们不做手写体识别，直接来做fashion-minist识别。 闲话少说，上代码，边写边说。 首先目标是实现衣服种类的识别。 数据可以在 Zalando_Fashion_MNIST_repository这个Github仓库获取。 数据分为60000训练数据和10000测试数据，图片都是灰度图片，大小为28 X 28，总共也是由10类组成。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305# -*- coding: utf-8 -*-\"\"\"Created on Sun Mar 25 15:16:23 2018@author: milittle\"\"\"# 导入一些必要的库import numpy as np # 数学计算库import matplotlib.pyplot as plt # 画图的一个库import tensorflow as tf # TensorFlow的库from tensorflow.examples.tutorials.mnist import input_datafashion_mnist = input_data.read_data_sets('input/data', one_hot = True)# 定义一个服装对应表label_dict = &#123; 0: 'T-shirt/top', 1: 'Trouser', 2: 'Pullover', 3: 'Dress', 4: 'Coat', 5: 'Sandal', 6: 'Shirt', 7: 'Sneaker', 8: 'Bag', 9: 'Ankle boot'&#125;# 获取随机的数据和它的labelsample_1 = fashion_mnist.train.images[47].reshape(28,28)sample_label_1 = np.where(fashion_mnist.train.labels[47] == 1)[0][0]sample_2 = fashion_mnist.train.images[23].reshape(28,28)sample_label_2 = np.where(fashion_mnist.train.labels[23] == 1)[0][0]# 用matplot画出这个image和labelprint(\"y = &#123;label_index&#125; (&#123;label&#125;)\".format(label_index=sample_label_1, label=label_dict[sample_label_1]))plt.imshow(sample_1, cmap='Greys')plt.show()print(\"y = &#123;label_index&#125; (&#123;label&#125;)\".format(label_index=sample_label_2, label=label_dict[sample_label_2]))plt.imshow(sample_2, cmap='Greys')plt.show()# 接下来就是设计网络参数n_hidden_1 = 128 # 第一个隐藏层的单元个数n_hidden_2 = 128 # 第二个隐藏层的单元个数n_input = 784 # fashion mnist输入图片的维度（单元个数） (图片大小: 28*28)n_classes = 10 # fashion mnist的种类数目 (0-9 数字)# 创建 placeholdersdef create_placeholders(n_x, n_y): \"\"\" 为sess创建一个占位对象。 参数: n_x -- 向量, 图片大小 (28*28 = 784) n_y -- 向量, 种类数目 (从 0 到 9, 所以是 -&gt; 10种) 返回参数: X -- 为输入图片大小的placeholder shape是[784, None] Y -- 为输出种类大小的placeholder shape是[10, None] None在这里表示以后输入的数据可以任意多少 \"\"\" X = tf.placeholder(tf.float32, [n_x, None], name=\"X\") Y = tf.placeholder(tf.float32, [n_y, None], name=\"Y\") return X, Y# 测试上面的create_placeholders()X, Y = create_placeholders(n_input, n_classes)print(\"Shape of X: &#123;shape&#125;\".format(shape=X.shape))print(\"Shape of Y: &#123;shape&#125;\".format(shape=Y.shape))# 定义初始化参数参数def initialize_parameters(): \"\"\" 参数初始化，下面是每个参数的shape，总共有三层 W1 : [n_hidden_1, n_input] b1 : [n_hidden_1, 1] W2 : [n_hidden_2, n_hidden_1] b2 : [n_hidden_2, 1] W3 : [n_classes, n_hidden_2] b3 : [n_classes, 1] 返回: 包含所有权重和偏置项的dic \"\"\" # 设置随机数种子 tf.set_random_seed(42) # 为每一层的权重和偏置项进行初始化工作 W1 = tf.get_variable(\"W1\", [n_hidden_1, n_input], initializer = tf.contrib.layers.xavier_initializer(seed = 42)) b1 = tf.get_variable(\"b1\", [n_hidden_1, 1], initializer = tf.zeros_initializer()) W2 = tf.get_variable(\"W2\", [n_hidden_2, n_hidden_1], initializer = tf.contrib.layers.xavier_initializer(seed = 42)) b2 = tf.get_variable(\"b2\", [n_hidden_2, 1], initializer = tf.zeros_initializer()) W3 = tf.get_variable(\"W3\", [n_classes, n_hidden_2], initializer=tf.contrib.layers.xavier_initializer(seed = 42)) b3 = tf.get_variable(\"b3\", [n_classes, 1], initializer = tf.zeros_initializer()) # 将参数存储在一个dict对象里面返回去 parameters = &#123; \"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, \"b3\": b3 &#125; return parameters# 测试初始化参数tf.reset_default_graph()with tf.Session() as sess: parameters = initialize_parameters() print(\"W1 = &#123;w1&#125;\".format(w1=parameters[\"W1\"])) print(\"b1 = &#123;b1&#125;\".format(b1=parameters[\"b1\"])) print(\"W2 = &#123;w2&#125;\".format(w2=parameters[\"W2\"])) print(\"b2 = &#123;b2&#125;\".format(b2=parameters[\"b2\"])) # 前向传播算法（就是神经网络的前向步骤）def forward_propagation(X, parameters): \"\"\" 实现前向传播的模型 LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SOFTMAX 上面的显示就是三个线性层，每一层结束以后，实现relu的作用，实现非线性功能，最后三层以后用softmax实现分类 参数: X -- 输入训练数据的个数[784, n] 这里的n代表可以一次训练多个数据 parameters -- 包括上面所有的定义参数三个网络中的权重W和偏置项B 返回: Z3 -- 最后的一个线性单元输出 \"\"\" # 从参数dict里面取到所有的参数 W1 = parameters['W1'] b1 = parameters['b1'] W2 = parameters['W2'] b2 = parameters['b2'] W3 = parameters['W3'] b3 = parameters['b3'] # 前向传播过程 Z1 = tf.add(tf.matmul(W1,X), b1) # Z1 = np.dot(W1, X) + b1 A1 = tf.nn.relu(Z1) # A1 = relu(Z1) Z2 = tf.add(tf.matmul(W2,A1), b2) # Z2 = np.dot(W2, a1) + b2 A2 = tf.nn.relu(Z2) # A2 = relu(Z2) Z3 = tf.add(tf.matmul(W3,A2), b3) # Z3 = np.dot(W3,Z2) + b3 return Z3# 测试前向传播喊出tf.reset_default_graph()with tf.Session() as sess: X, Y = create_placeholders(n_input, n_classes) parameters = initialize_parameters() Z3 = forward_propagation(X, parameters) print(\"Z3 = &#123;final_Z&#125;\".format(final_Z=Z3))# 定义计算损失函数# 是计算loss的时候了def compute_cost(Z3, Y): \"\"\" 计算cost 参数: Z3 -- 前向传播的最终输出（[10, n]）n也是你输入的训练数据个数 Y -- 返回: cost - 损失函数 张量（Tensor） \"\"\" # 获得预测和准确的label logits = tf.transpose(Z3) labels = tf.transpose(Y) # 计算损失 cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels)) return cost# 测试计算损失函数tf.reset_default_graph()with tf.Session() as sess: X, Y = create_placeholders(n_input, n_classes) parameters = initialize_parameters() Z3 = forward_propagation(X, parameters) cost = compute_cost(Z3, Y) print(\"cost = &#123;cost&#125;\".format(cost=cost))# 这个就是关键了，因为每一层的参数都是通过反向传播来实现权重和偏置项参数更新的# 总体的原理就是经过前向传播，计算到最后的层，利用softmax加交叉熵，算出网络的损失函数# 然后对损失函数进行求偏导，利用反向传播算法实现每一层的权重和偏置项的更新def model(train, test, learning_rate=0.0001, num_epochs=16, minibatch_size=32, print_cost=True, graph_filename='costs'): \"\"\" 实现了一个三层的网络结构: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SOFTMAX. 参数: train -- 训练集 test -- 测试集 learning_rate -- 优化权重时候所用到的学习率 num_epochs -- 训练网络的轮次 minibatch_size -- 每一次送进网络训练的数据个数（也就是其他函数里面那个n参数） print_cost -- 每一轮结束以后的损失函数 返回: parameters -- 被用来学习的参数 \"\"\" # 确保参数不被覆盖重写 tf.reset_default_graph() tf.set_random_seed(42) seed = 42 # 获取输入和输出大小 (n_x, m) = train.images.T.shape n_y = train.labels.T.shape[0] costs = [] # 创建输入输出数据的占位符 X, Y = create_placeholders(n_x, n_y) # 初始化参数 parameters = initialize_parameters() # 进行前向传播 Z3 = forward_propagation(X, parameters) # 计算损失函数 cost = compute_cost(Z3, Y) # 使用AdamOptimizer优化器实现反向传播算法（最小化cost） # 其实我们这个地方的反向更新参数的过程都是tensorflow给做了 optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost) # 变量初始化器 init = tf.global_variables_initializer() # 开始tensorflow的sess 来计算tensorflow构建好的图 with tf.Session() as sess: # 这个就是之前说过的要进行初始化的 sess.run(init) # 训练轮次 for epoch in range(num_epochs): epoch_cost = 0. num_minibatches = int(m / minibatch_size) seed = seed + 1 for i in range(num_minibatches): # 获取下一个batch的训练数据和label数据 minibatch_X, minibatch_Y = train.next_batch(minibatch_size) # 执行优化器 _, minibatch_cost = sess.run([optimizer, cost], feed_dict=&#123;X: minibatch_X.T, Y: minibatch_Y.T&#125;) # 更新每一轮的损失 epoch_cost += minibatch_cost / num_minibatches # 打印每一轮的损失 if print_cost == True: print(\"Cost after epoch &#123;epoch_num&#125;: &#123;cost&#125;\".format(epoch_num=epoch, cost=epoch_cost)) costs.append(epoch_cost) # 使用matplot画出损失的变化曲线图 plt.figure(figsize=(16,5)) plt.plot(np.squeeze(costs), color='#2A688B') plt.xlim(0, num_epochs-1) plt.ylabel(\"cost\") plt.xlabel(\"iterations\") plt.title(\"learning rate = &#123;rate&#125;\".format(rate=learning_rate)) plt.savefig(graph_filename, dpi = 300) plt.show() # 保存参数 parameters = sess.run(parameters) print(\"Parameters have been trained!\") # 计算预测准率 correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y)) # 计算测试准率 accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) print (\"Train Accuracy:\", accuracy.eval(&#123;X: train.images.T, Y: train.labels.T&#125;)) print (\"Test Accuracy:\", accuracy.eval(&#123;X: test.images.T, Y: test.labels.T&#125;)) return parameters# 要开始训练我们的fashion mnist网络了train = fashion_mnist.train # 训练的数据test = fashion_mnist.test # 测试的数据parameters = model(train, test, learning_rate = 0.001, num_epochs = 16, graph_filename = 'fashion_mnist_costs') 上面的代码是写好了，这里有一个python的依赖库（matplotlib）需要安装以下，同样的办法，就是进去tensorflow这个环境里面，然后执行pip install matplotlib就可以了。 在这个过程中，可能从tensorflow下载数据的时候会很慢。（我们选择直接从上面给出下载数据集的github网址，直接下载以后，将数据拷贝在代码所在文件夹的input/data/文件夹里面，总共由四个文件组成）分别是训练数据图片、训练数据label和测试数据图片、测试数据label。这样就可以省去下载数据时候漫长的等待。 上面就是我们使用TensorFlow实现的fashion-mnist的识别，总体根据实验结果来说，从测试集的数据来看，我达到的准确率结果是88.5%，还算可以。后续我们可能使用其他一些现有的网络结构来实现fashion-mnist的识别，看看准确率会不会提高。 如下是我对上面TensorFLow出现的方法介绍： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748tf.placeholders( dtype, shape=None, name=None)从参数上面看到，总共有三个参数： dtype：在tensor中被喂数据的元素类型 shape: tensor的shape name：命名说明一下，这个函数返回的是一个tensor，在TensorFlow里面，tensor是一个很重要的概念，大家务必掌握，也叫张量，比如我们的一个数:就是0-阶张量，也叫标量。一个向量，就是1-阶张量。一个矩阵，就是2-阶张量，后面的就是一直往高维了走，对应的就是多少阶张量。这个方法，很重要的原因也在于它是定义在Session执行run的时候，在后面填充数据的占位符，也就是feed_dict这个变量里面的数据，所以大家，务必记住这一关键的概念。后续用起来就会很顺手。tf.get_variable()这个方法后续在展开来说，你先理解就是使用它可以定义变量（保存权重和偏置项的），还可以加一些优化器，比如说正则优化器等等tf.matmul( a, b,)展示给你们列出这两个参数： a：就是待操作的矩阵1 b: 就是待操作的矩阵2函数功能就是实现矩阵的相乘运算（当然要符合基本的矩阵运算格式）tf.transpose( a,)先列出来一个参数，就是矩阵的转置Session().run( fetches, feed_list=None, )这个方法就是运行图。很关键，先掌握两个参数: fetches: 你要从图里面取出的数据（） feed_list: 你要给图喂的数据（输入和label数据就是用这样的方式来做的） 比如我们训练的网络中输入的图片信息和对应的label信息tf.reduce_mean( input_tensor, axis=None, keepdims=None, name=None, redcution_indices=None, keep_dims=None )计算输入tensor的总和： input_tensor: 要叠加的tensor axis: 选择那个维度叠加 keepdims: 叠加元素以后，保留原来的维度信息 name：就是名字 redcution_indices：被axis取代 keep_dims：被keepdims取代 我们今天的任务量可能有一些大，大家坚持。总的来说就是使用神经网络对实际的一个fashion-mnist数据集进行服装种类的识别，大家主要看看我的代码。有什么不明白的我在代码里面都做出了注释。 邮箱——air@weaf.top欢迎来探讨","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]},{"title":"rsync的使用与配置","slug":"2018-03-19/rsync-configuration-and-use","date":"2018-03-25T13:08:56.000Z","updated":"2018-04-02T14:12:49.206Z","comments":true,"path":"posts/cfc65600/","link":"","permalink":"http://weafteam.github.io/posts/cfc65600/","excerpt":"一、什么是rsync rsync，remote synchronize顾名思意就知道它是一款实现远程同步功能的软件，它在同步文件的同时，可以保持原来文件的权限、时间、软硬链接等附加信息。 rsync是用 “rsync 算法”提供了一个客户机和远程文件服务器的文件同步的快速方法，而且可以通过ssh方式来传输文件，这样其保密性也非常好，另外它还是免费的软件。 二、rsync的安装 rysnc的官方网站：http://rsync.samba.org 可以从上面得到最新的版本。目前最新版是3.1.2。当然，因为rsync是一款如此有用的软件，所以很多Linux的发行版本都将它收录在内了。","text":"一、什么是rsync rsync，remote synchronize顾名思意就知道它是一款实现远程同步功能的软件，它在同步文件的同时，可以保持原来文件的权限、时间、软硬链接等附加信息。 rsync是用 “rsync 算法”提供了一个客户机和远程文件服务器的文件同步的快速方法，而且可以通过ssh方式来传输文件，这样其保密性也非常好，另外它还是免费的软件。 二、rsync的安装 rysnc的官方网站：http://rsync.samba.org 可以从上面得到最新的版本。目前最新版是3.1.2。当然，因为rsync是一款如此有用的软件，所以很多Linux的发行版本都将它收录在内了。 软件包安装 命令 平台 # sudo apt-get install rsync 注：在debian、ubuntu 等在线安装方法； # yum install rsync 注：Fedora、Redhat 等在线安装方法； # rpm -ivh rsync 注：Fedora、Redhat 等rpm包安装方法； 其它Linux发行版，请用相应的软件包管理方法来安装。 源码包安装 123 tar xvf rsync-xxx.tar.gz cd rsync-xxx ./configure --prefix=/usr ;make ;make install 注：在用源码 包编译安装之前，您得安装gcc等编译开具才行； 三、rsync的配置 ———– rsync的主要有以下三个配置文件rsyncd.conf(主配置文件)、rsyncd.secrets(密码文件)、rsyncd.motd(rysnc服务器信息) 比如我们要备份服务器上的/home和/opt，在/home中我想把easylife和samba目录排除在外； 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849 # Distributed under the terms of the GNU General Public License v2 # Minimal configuration file for rsync daemon # See rsync(1) and rsyncd.conf(5) man pages for help # This line is required by the /etc/init.d/rsyncd script pid file = /var/run/rsyncd.pid port = 873 address = 192.168.1.171 #uid = nobody #gid = nobody uid = root gid = root use chroot = yes read only = yes #limit access to private LANs hosts allow=192.168.1.0/255.255.255.0 10.0.1.0/255.255.255.0 hosts deny=* max connections = 5 motd file = /etc/rsyncd.motd #This will give you a separate log file #log file = /var/log/rsync.log #This will log every file transferred - up to 85,000+ per user, per sync #transfer logging = yes log format = %t %a %m %f %b syslog facility = local3 timeout = 300 [rhel4home] path = /home list=yes ignore errors auth users = root secrets file = /etc/rsyncd.secrets comment = This is RHEL 4 data exclude = easylife/ samba/ [rhel4opt] path = /opt list=no ignore errors comment = This is RHEL 4 opt auth users = easylife secrets file = /etc/rsyncd/rsyncd.secrets 注：关于auth users是必须在服务器上存在的真实的系统用户，如果你想用多个用户以,号隔开，比如auth users = easylife,root 设定密码文件 密码文件格式很简单，rsyncd.secrets的内容格式为： 用户名:密码 我们在例子中rsyncd.secrets的内容如下类似的；在文档中说，有些系统不支持长密码，自己尝试着设置一下吧。 12 easylife:keer root:mike 12 chown root.root rsyncd.secrets #修改属主 chmod 600 rsyncd.secrets #修改权限 注：1、将rsyncd.secrets这个密码文件的文件属性设为root拥有, 且权限要设为600, 否则无法备份成功! 出于安全目的，文件的属性必需是只有属主可读。 2、这里的密码值得注意，为了安全你不能把系统用户的密码写在这里。比如你的系统用户easylife密码是000000，为了安全你可以让rsync中的easylife为keer。这和samba的用户认证的密码原理是差不多的。 设定rsyncd.motd 文件; 它是定义rysnc服务器信息的，也就是用户登录信息。比如让用户知道这个服务器是谁提供的等；类似ftp服务器登录时，我们所看到的 linuxsir.org ftp ……。 当然这在全局定义变量时，并不是必须的，你可以用#号注掉，或删除；我在这里写了一个 rsyncd.motd的内容为： 1234 ++++++++++++++++++++++++++++++++++++++++++++++ Welcome to use the mike.org.cn rsync services!2002------2009 ++++++++++++++++++++++++++++++++++++++++++++++ 四、启动rsync服务器 相当简单，有以下几种方法 A、–daemon参数方式，是让rsync以服务器模式运行 1 #/usr/bin/rsync --daemon --config=/etc/rsyncd/rsyncd.conf #--config用于指定rsyncd.conf的位置,如果在/etc下可以不写 B、xinetd方式 12345 修改services加入如下内容 # nano -w /etc/services rsync 873/tcp # rsync rsync 873/udp # rsync 这一步一般可以不做，通常都有这两行(我的RHEL4和GENTOO默认都有)。修改的目的是让系统知道873端口对应的服务名为rsync。如没有的话就自行加入。 设定 /etc/xinetd.d/rsync, 简单例子如下: 12345678910111213 # default: off # description: The rsync server is a good addition to am ftp server, as it \\ # allows crc checksumming etc. service rsync &#123;disable = nosocket_type = streamwait = nouser = rootserver = /usr/bin/rsyncserver_args = --daemonlog_on_failure += USERID &#125; 上述, 主要是要打开rsync這個daemon, 一旦有rsync client要连接時, xinetd会把它转介給 rsyncd(port 873)。然后service xinetd restart, 使上述设定生效. rsync服务器和防火墙 Linux 防火墙是用iptables，所以我们至少在服务器端要让你所定义的rsync 服务器端口通过，客户端上也应该让通过。 12 #iptables -A INPUT -p tcp -m state --state NEW -m tcp --dport 873 -j ACCEPT #iptables -L 查看一下防火墙是不是打开了 873端口 如果你不太懂防火墙的配置，可以先service iptables stop 将防火墙关掉。当然在生产环境这是很危险的，做实验才可以这么做哟！ 五、通过rsync客户端来同步数据 B1、列出rsync 服务器上的所提供的同步内容； 首先：我们看看rsync服务器上提供了哪些可用的数据源 # rsync –list-only root@192.168.145.5:: ++++++++++++++++++++++++++++++++++++++++++++++ Welcome to use the mike.org.cn rsync services! 2002——2009 ++++++++++++++++++++++++++++++++++++++++++++++ rhel4home This is RHEL 4 data 注：前面是rsync所提供的数据源，也就是我们在rsyncd.conf中所写的[rhel4home]模块。而“This is RHEL 4 data”是由[rhel4home]模块中的 comment = This is RHEL 4 data 提供的；为什么没有把rhel4opt数据源列出来呢？因为我们在[rhel4opt]中已经把list=no了。 $ rsync –list-only root@192.168.145.5::rhel4home ++++++++++++++++++++++++++++++++++++++++++++++ Welcome to use the mike.org.cn rsync services! 2002——2009 ++++++++++++++++++++++++++++++++++++++++++++++ Password: drwxr-xr-x 4096 2009/03/15 21:33:13 . -rw-r–r– 1018 2009/03/02 02:33:41 ks.cfg -rwxr-xr-x 21288 2009/03/15 21:33:13 wgetpaste drwxrwxr-x 4096 2008/10/28 21:04:05 cvsroot drwx—— 4096 2008/11/30 16:30:58 easylife drwsr-sr-x 4096 2008/09/20 22:18:05 giddir drwx—— 4096 2008/09/29 14:18:46 quser1 drwx—— 4096 2008/09/27 14:38:12 quser2 drwx—— 4096 2008/11/14 06:10:19 test drwx—— 4096 2008/09/22 16:50:37 vbird1 drwx—— 4096 2008/09/19 15:28:45 vbird2 后面的root@ip中，root是指定密码文件中的用户名，之后的::rhel4home这是rhel4home模块名 ### B2、rsync客户端同步数据； #rsync -avzP root@192.168.145.5::rhel4home rhel4home Password: 这里要输入root的密码，是服务器端rsyncd.secrets提供的。在前面的例子中我们用的是mike，输入的密码并不回显，输好就回车。 注： 这个命令的意思就是说，用root用户登录到服务器上，把rhel4home数据，同步到本地当前目录rhel4home上。当然本地的目录是可以你自己 定义的。如果当你在客户端上当前操作的目录下没有rhel4home这个目录时，系统会自动为你创建一个；当存在rhel4home这个目录中，你要注意 它的写权限。 1 #rsync -avzP --delete linuxsir@linuxsir.org::rhel4home rhel4home 这回我们引入一个–delete 选项，表示客户端上的数据要与服务器端完全一致，如果 linuxsirhome目录中有服务器上不存在的文件，则删除。最终目的是让linuxsirhome目录上的数据完全与服务器上保持一致；用的时候要 小心点，最好不要把已经有重要数所据的目录，当做本地更新目录，否则会把你的数据全部删除； 設定 rsync client 设定密码文件 1 #rsync -avzP --delete --password-file=rsyncd.secrets root@192.168.145.5::rhel4home rhel4home 这次我们加了一个选项 –password-file=rsyncd.secrets，这是当我们以root用户登录rsync服务器同步数据时，密码将读取rsyncd.secrets这个文件。这个文件内容只是root用户的密码。我们要如下做； # touch rsyncd.secrets # chmod 600 rsyncd.secrets # echo “mike”&gt; rsyncd.secrets # rsync -avzP –delete –password-file=rsyncd.secrets root@192.168.145.5::rhel4home rhel4home 注：这里需要注意的是这份密码文件权限属性要设得只有属主可读。 这样就不需要密码了；其实这是比较重要的，因为服务器通过crond 计划任务还是有必要的； ### B3、让rsync客户端自动与服务器同步数据 服务器是重量级应用，所以数据的网络备份还是极为重要的。我们可以在生产型服务器上配置好rsync 服务器。我们可以把一台装有rysnc机器当做是备份服务器。让这台备份服务器，每天在早上4点开始同步服务器上的数据；并且每个备份都是完整备份。有时 硬盘坏掉，或者服务器数据被删除，完整备份还是相当重要的。这种备份相当于每天为服务器的数据做一个镜像，当生产型服务器发生事故时，我们可以轻松恢复数 据，能把数据损失降到最低；是不是这么回事？？ step1：创建同步脚本和密码文件 12345678 #mkdir /etc/cron.daily.rsync #cd /etc/cron.daily.rsync #touch rhel4home.sh rhel4opt.sh #chmod 755 /etc/cron.daily.rsync/*.sh #mkdir /etc/rsyncd/ #touch /etc/rsyncd/rsyncrhel4root.secrets #touch /etc/rsyncd/rsyncrhel4easylife.secrets #chmod 600 /etc/rsyncd/rsync.* 注： 我们在 /etc/cron.daily/中创建了两个文件rhel4home.sh和rhel4opt.sh ，并且是权限是755的。创建了两个密码文件root用户用的是rsyncrhel4root.secrets ，easylife用户用的是 rsyncrhel4easylife.secrets，权限是600； 我们编辑rhel4home.sh，内容是如下的： 123 #!/bin/sh #backup 192.168.145.5:/home /usr/bin/rsync -avzP --password-file=/etc/rsyncd/rsyncrhel4root.secrets root@192.168.145.5::rhel4home /home/rhel4homebak/$(date +&apos;%m-%d-%y&apos;) 我们编辑 rhel4opt.sh ，内容是： 123 #!/bin/sh #backup 192.168.145.5:/opt /usr/bin/rsync -avzP --password-file=/etc/rsyncd/rsyncrhel4easylife.secrets easylife@192.168.145.5::rhel4opt /home/rhel4hoptbak/$(date +&apos;%m-%d-%y&apos;) 注：你可以把rhel4home.sh和rhel4opt.sh的内容合并到一个文件中，比如都写到rhel4bak.sh中； 接着我们修改 /etc/rsyncd/rsyncrhel4root.secrets和rsyncrhel4easylife.secrets的内容； 12 # echo &quot;mike&quot; &gt; /etc/rsyncd/rsyncrhel4root.secrets # echo &quot;keer&quot;&gt; /etc/rsyncd/rsyncrhel4easylife.secrets 然后我们再/home目录下创建rhel4homebak 和rhel4optbak两个目录，意思是服务器端的rhel4home数据同步到备份服务器上的/home/rhel4homebak 下，rhel4opt数据同步到 /home/rhel4optbak/目录下。并按年月日归档创建目录；每天备份都存档； 12 #mkdir /home/rhel4homebak #mkdir /home/rhel4optbak step2：修改crond服务器的配置文件 加入到计划任务 1 #crontab -e 加入下面的内容： # Run daily cron jobs at 4:10 every day backup rhel4 data: 10 4 * * * /usr/bin/run-parts /etc/cron.daily.rsync 1&gt; /dev/null 注：第一行是注释，是说明内容，这样能自己记住。 第二行表示在每天早上4点10分的时候，运行 /etc/cron.daily.rsync 下的可执行脚本任务； 配置好后，要重启crond 服务器； 123456 # killall crond 注：杀死crond 服务器的进程； # ps aux |grep crond 注：查看一下是否被杀死； # /usr/sbin/crond 注：启动 crond 服务器； # ps aux |grep crond 注：查看一下是否启动了？ root 3815 0.0 0.0 1860 664 ? S 14:44 0:00 /usr/sbin/crond root 3819 0.0 0.0 2188 808 pts/1 S+ 14:45 0:00 grep crond","categories":[{"name":"Linux","slug":"Linux","permalink":"http://weafteam.github.io/categories/Linux/"}],"tags":[{"name":"Linux运维","slug":"Linux运维","permalink":"http://weafteam.github.io/tags/Linux运维/"}]},{"title":"文本聚类系列教程：（二）jieba中文分词工具进阶","slug":"2018-03-19/文本聚类系列教程：（二）jieba中文分词工具进阶","date":"2018-03-19T11:57:19.000Z","updated":"2018-04-02T14:12:49.206Z","comments":true,"path":"posts/931939a5/","link":"","permalink":"http://weafteam.github.io/posts/931939a5/","excerpt":"","text":"jieba中文分词工具使用进阶篇，废话不多说吗，我们开始本次的学习吧~ 如何让分词的更加准确 我们之前举得例子有些文本其实很简单，我们后来确实换了官方的测试文本《围城》，但是均没避免一个问题，这些测试例都十分地中规中矩。在实际中需要我们做分词的文本可能是多种多样的，这时候的切词有可能会不太特别理想，导致分词的不准确。 那我们不妨下一个别的电子书（这里我下载的是《斗破苍穹》，为了测试我只用了第一章的文本），然后再进行切词，看下是否存在这样的问题。这里我们稍微改改上次的去停用词的代码，代码如下： 1234567891011121314151617181920212223import sysimport jiebafrom os import pathd = path.dirname(__file__) # 获取当前文件的dir路径text_path = 'txt/chapter2.txt' #《斗破苍穹》第一章的文本路径text = open(path.join(d, text_path),'rb').read()def CutWords(text): mywordlist = [] seg_list = jieba.cut(text, cut_all=False) liststr=\"/ \".join(seg_list) # 添加切分符 for myword in liststr.split('/'): if len(myword.strip())&gt;1: mywordlist.append(myword) return ''.join(mywordlist) #返回一个字符串txt5 = CutWords(text)text_write = 'txt/5.txt'with open(text_write,'w') as f: f.write(txt5) print(\"Success\") 结果如下： 终于被我们找到了一个切词错误，原文是这样的： 萧媚脑中忽然浮现出三年前那意气风发的少年 按照我们正常的断句，应为： 萧媚/脑中/忽然/浮现….，而jieba却认为“萧媚脑”是一个单词，从而导致此处分词不理想。 jieba考虑了这种情况，而且有很多的应对方案，下面我们先说最简单的。 调整词典 方法1：动态修改词典 使用add_word(word,freq=None,tag=None)和del_word(word)可在程序中动态的修改词典，具体操作如下： 12345678910import sysimport jiebafrom os import pathd = path.dirname(__file__) # 获取当前文件的dir路径# 此处增加代码jieba.add_word('脑中') ···· 结果如下： 果然，这样的方法很直接的把我们原来切错的词变成了正确的词。与add_word()相对应的是delete_word()方法，根据字面意思我们也很容易理解delete_word()方法的作用，这里我就不做过多的演示了，大家在实际场景中直接运用就好了。 方法2：调节词频 使用suggest_freq(segment, tune=True)调节单个词语的词频，使得它更容易被分出来，或者不被分出来。 但是需要注意的是：自动计算的词频在使用 HMM 新词发现功能时可能无效。 所以此时我们在做切词的时候需要把是HMM置为False。我们看下官方给的Demo（如果关闭HMM，很多新发现的词都消失了，所以‘萧媚脑’也消失了，无法做测试，我们的例子也是为了方便大家理解，所以也没必要非得针对这一个词做词频调节），具体的做法如下： 12345678910111213import jiebaprint('/'.join(jieba.cut('如果放到post中将出错。', HMM=False)))jieba.suggest_freq(('中', '将'), True)print('/'.join(jieba.cut('如果放到post中将出错。', HMM=False)))print('/'.join(jieba.cut('「台中」正确应该不会被切开', HMM=False)))jieba.suggest_freq('台中', True)print('/'.join(jieba.cut('「台中」正确应该不会被切开', HMM=False))) 结果： 对比下结果，不难发现suggest_freq()的使用方法，通过这样的强调高频词和低频词的方法可以做到分词更准确。 添加自定义词典 比起默认的词典，我们自定义的词典更适合我们自己的文本，这一点是毋庸置疑的。 词典格式和 dict.txt 一样，一个词占一行；每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。file_name 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码。 这里我们的词典为： 12345678910云计算 5李小福 2 nr创新办 3 ieasy_install 3 eng好用 300韩玉赏鉴 3 nz八一双鹿 3 nz台中凱特琳 nzEdu Trust认证 2000 我们这个例子也用官方的Demo，代码如下： 12345678910111213141516171819202122232425262728293031323334import syssys.path.append(\"../\")import jiebajieba.load_userdict(\"userdict.txt\")# jieba在0.28版本之后采用延迟加载方式# “import jieba”不会立即触发词典的加载，而是在有必要的时候才会加载词典# 如果想手动加载，可执行代码： jieba.initialize() 进行手动初始化操作# 也正是有了延迟加载机制，我们现在可以改变主词典的路径：# jieba.set_dictionary('data/dict.txt.big')# 官方还提供了占用内存较小的词典和适用于繁体字的词典，均在官方的GitHub上，有需要的可以自行下载。import jieba.posseg as pseg# pseg切分可以显示词性# 以下三个操作是修改词典的巩固jieba.add_word('石墨烯')jieba.add_word('凱特琳')jieba.del_word('自定义词')test_sent = (\"李小福是创新办主任也是云计算方面的专家; 什么是八一双鹿\\n\"\"例如我输入一个带“韩玉赏鉴”的标题，在自定义词库中也增加了此词为N类\\n\"\"「台中」正確應該不會被切開。mac上可分出「石墨烯」；此時又可以分出來凱特琳了。\")words = jieba.cut(test_sent)print('/'.join(words))print(\"=\"*40)result = pseg.cut(test_sent)for w in result: print(w.word, \"/\", w.flag, \", \", end=' ') 结果如下： 像‘云计算’、‘创新办’等词在没加载词典的时候是不能被识别出来的。像‘石墨烯’等在没有add_word()的时候也是不能识别出来的。可见效果还是不错的。 并行分词 原理：将目标文本按行分隔后，把各行文本分配到多个 Python 进程并行分词，然后归并结果，从而获得分词速度的可观提升 但是令人遗憾的是，这个模块并不支持Windows平台，原因是因为jieba的该模块是基于python自带的 multiprocessing 模块，而这个模块并不支持Windows。这里我就贴一下用法，使用Linux系统的同学可以自行体验下这个可观的速度提升。 用法： jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数 jieba.disable_parallel() # 关闭并行分词模式 最后 以上所讲的内容在日常的使用中应该是够用了，当然像基于TextRank算法的关键词抽取等内容，我这里并没涉及，并不是因为不重要，而是我对这个算法还不是很了解，硬着头皮写肯定也是照本宣科，效果肯定很差，所以先挖个坑吧，以后再填。 感谢阅读~","categories":[{"name":"文本聚类","slug":"文本聚类","permalink":"http://weafteam.github.io/categories/文本聚类/"}],"tags":[{"name":"文本聚类","slug":"文本聚类","permalink":"http://weafteam.github.io/tags/文本聚类/"}]},{"title":"普通的 SQLAlchemy ORM 使用姿势","slug":"2018-03-12/usage-of-sqlalchemy","date":"2018-03-18T13:38:54.000Z","updated":"2018-08-07T13:19:42.435Z","comments":true,"path":"posts/39277c31/","link":"","permalink":"http://weafteam.github.io/posts/39277c31/","excerpt":"","text":"前言 SQLAlchemy 是 Python 世界中最常用的 SQL 工具之一，包含 SQL 渲染引擎和 ORM 两大部分，平时使用最多的就是 ORM。在我看来平时很多使用 ORM 的姿势是有问题的，或者说是不优雅的。所以这篇文章打算讲讲（搬运）其中一些普通的姿势和技巧（API 文档）。 property 和混合属性 property 下面是一个简单的用户表映射： 12345class User(Base): __tablename__ = 'user' id = Column(Integer, primary_key=True) name = Column(String(64)) password = Column(String(128)) 通常情况下，我们会加密用户的密码，在数据库中保存密文，但是这里有一个问题，我们得这么写： 1234# 创建用户user = User(name='zhang', password=encrypt('123456'))# 修改密码user.password = encrypt('654321') 这意味着我们需要不断的重复书写 encrypt 函数来保证加密了用户密码。 有没有什么方法能省去这一步呢？答案是 property。 现在把用户表映射改成这样： 12345678910111213class User(Base): __tablename__ = 'user' id = Column(Integer, primary_key=True) name = Column(String(64)) _password = Column(String(128)) @property def password(self): raise ValueError('write only!') @password.setter def password(self, value): self._password = encrypt(value) 现在只需要简单的写成： 1234# 创建用户user = User(name='zhang', password='123456')# 修改密码user.password = '654321' 就可以了。 关于 Python 中 property和描述符的使用值得再另写一篇文章描述，在这里就不详细说明了。 混合属性（hybrid_property） 上面的例子看上去让代码清爽了不少，但是有时候这种用法是无法满足需要的，譬如下面这个例子： 12345class Student(Base): __tablename__ = 'student' id = Column(Integer, primary_key=True) name = Column(String(64)) birthday = Column(DateTime) 这是一个学生表映射，增加了 birthday 字段。通常我们会保存用户的生日，再通过生日获取用户年龄。有了上面的例子，很容易写出获取年龄的代码： 123456class Student(Base): ... @property def age(self): return datetime.now().year - self.birthday.year 现在可以简单的使用 student.age 获取具体的生日。 这样做是有缺陷的：如果需要获取所有 18 岁的学生呢？我们希望可以这样写： 1session.query(Student).filter_by(age=18).all() 但是却没有任何结果返回。如果改成这样呢？ 1234now = datetime.now()start = datetime(now.year - 18, 1, 1)end = end = datetime(now.year + 1 - 18, 1, 1)session.query(Student).filter(Student.birthday &gt;= start, Student.birthday &lt; end).all() 这样倒是可以获取正确的结果了，但是也太丑了点吧？难道没办法写出像第一条一样的既清晰又简洁的查询么？ 答案自然是有的，SQLAlchemy 提供了混合属性（hybrid_property）来处理类似的情况，于是我们可以改写获取年龄的代码： 12345678910111213from sqlalchemy.ext.hybrid import hybrid_propertyfrom sqlalchemy import funcclass Student(Base): ... @hybrid_property def age(self): return datetime.now().year - self.birthday.year @age.expression def age(self): return datetime.now().year - func.year(self.birthday) 这里将原本的 property 替换为 SQLAlchemy 中的 hybrid_property，同时提供了一个 expression 装饰器，在被装饰的方法中把 Python 代码翻译成 SQL（代码示例的目标数据库为 MySQL，获取日期中的年份的函数为YEAR()，使用其他数据库请查阅对应数据库的相关文档）。有了这个方法，SQLAlchemy 就知道如何在 SQL 语句中处理 age 属性了。 接下来稍微提一下 hybrid_method。 和 hybrid_property 类似，只不过可以给 hybrid_method 传参数。下面这个例子不太合适，只为了展示hybrid_method 的功能。 如何找到所有 90 后同学？当然我们可以复用上面的 age 属性，先计算一下 90 后的同学现在多少岁，然后直接写在查询里就好： 1session.query(Student).filter(Student.age &gt;= now.year - 1990, Student.age &lt; now.year - 2000).all() 如果要判断某个学生是否是 90 后呢？又需要再写一遍： 12if now.year - 2000 &gt; student.age &gt;= now.year - 1990: ... 出现了很多不直观的代码，这时候可以使用 hybrid_method 简化： 123456789class Student(Base): ... @hybrid_method def born_after(self, years): return years + 10 &gt; self.birthday.year &gt;= years @born_after.expression def born_after(self, years): return and_(func.year(self.birthday) &lt; years + 10, func.year(self.birthday) &gt;= years) 于是现在可以这样做： 1234session.query(Student).filter(Student.born_after(1990)).all()if student.born_after(1990): ... 看上去好了一些（误 这一部分就到此为止，当然 hybrid 在 SQLAlchemy 中的用法不止上述这些，更详细和复杂的内容参见官方文档。 关联代理（association_proxy） 简化标量集合 关联代理用在有关联的表中，所以我们先创建如下映射关系： 1234567891011121314151617association = Table('association', Base.metadata, Column('blog_id', Integer, ForeignKey('blog.id'), primary_key=True), Column('tag_id', Integer, ForeignKey('tag.id'), primary_key=True))class Blog(Base): __tablename__ = 'blog' id = Column(Integer, primary_key=True) name = Column(String(64)) tags = relationship( 'Tag', secondary=association, backref=backref('blogs', lazy='dynamic'), lazy='dynamic')class Tag(Base): __tablename__ = 'tag' id = Column(Integer, primary_key=True) name = Column(String(64)) 一个经常被拿出来作为演示的 Many-To-Many 模型。 先填充一些数据： 12345In [1]: blog = Blog(name='first')In [2]: blog.tags.append(Tag(name='t1'))In [3]: blog.tags.append(Tag(name='t2'))In [4]: session.add(blog)In [5]: session.commit() 接下来就可以获取这些对象的所有信息了： 12345678In [4]: blog.tags.all()Out[4]: [&lt;Tag at 0x1fdbab6f198&gt;, &lt;Tag at 0x1fdbab6f208&gt;]In [5]: blog.tags.all()[0].nameOut[5]: 't1'In [6]: [t.name for t in blog.tags]Out[6]: ['t1', 't2'] 上面的操作有点复杂。对我们而言，Tag 对象只有 name 字段是有用的，为了获取 name 字段，我们要写很多额外的代码把 name 字段从 Tag 对象中剥离出来。association_proxy 就可以用来简化这个操作。 现在修改一下上面的 Blog 映射： 123456789from sqlalchemy.ext.associationproxy import association_proxyclass Blog(Base): ... tag_objects = relationship( 'Tag', secondary=association, backref=backref('blogs', lazy='dynamic'), lazy='dynamic') tags = association_proxy('tag_objects', 'name') 增加了一行 association_proxy 对象的声明，现在我们可以这样做： 12In [7]: blog.tagsOut[7]: ['t1', 't2'] 现在查询操作变得很简单了，但是新增标签的操作还是很麻烦： 1blog.tag_objects.append(Tag(name='t3')) 还是需要实例化一个 Tag 对象，能不能直接写： 1blog.tags.append('t4') 当然是可以的，只要再修改一下 association_proxy 的声明： 1234class Blog(Base): ... tags = association_proxy('tag_objects', 'name', creator=lambda name: Tag(name=name)) 参数 creator 接受一个可调用对象，它告诉 association_proxy 如何处理“新增”操作。 注意：creator 的默认参数是被代理对象的构造函数，如果提供了一个单参数的构造函数，那么可以省略 creator 参数。 简化关联对象 上面的例子里把 association 表作为一个普通的 Table 对象，是因为 association 中不需要保存额外信息，只需要作为 Blog 和 Tag 的中转。现在有了新的需求，我们需要知道每篇博客的标签是在什么时候加上的，这就需要在 association 表中增加一个额外的字段用来表示创建时间，同时为了获取这个时间，还要把 association 改造成一个真正的映射： 123456789101112131415161718192021class Association(Base): __tablename__ = 'association' blog_id = Column(Integer, ForeignKey('blog.id'), primary_key=True) tag_id = Column(Integer, ForeignKey('tag.id'), primary_key=True) created_at = Column(DateTime, default=datetime.now) blog = relationship('Blog', backref=backref('blog_tags', lazy='dynamic'), lazy='joined') tag = relationship('Tag', backref=backref('tag_blogs', lazy='dynamic'), lazy='joined')class Tag(Base): __tablename__ = 'tag' id = Column(Integer, primary_key=True) name = Column(String(64))class Blog(Base): __tablename__ = 'blog' id = Column(Integer, primary_key=True) name = Column(String(64)) 这里实际上是把 Many-To-Many 拆成了两个 One-To-Many。 然后构造一些数据： 1234567In [1]: blog = Blog(name='first') ...: tags = [Tag(name='t1'), Tag(name='t2')] ...: for tag in tags: ...: session.add(Association(blog=blog, tag=tag)) ...: session.add(blog) ...: session.add_all(tags) ...: session.commit() 现在就可以获取 Tag 和被添加的时间了： 12345In [2]: blog.blog_tags[0].tag.nameOut[2]: 't1'In [3]: blog.blog_tags[0].created_atOut[3]: datetime.datetime(2018, 3, 18, 16, 4, 17) 可以看到，给 Blog 增加标签要经过 Association 这个中间对象。虽然表结构的确如此，但是我们仍然希望 Association 表是透明的，仅当需要获取其中的创建时间时才明确获取 Association 对象。只需要在 Blog 中声明一个关联代理： 1234class Blog(Base): ... tags = association_proxy('blog_tags', 'tag', creator=lambda tag: Association(tag=tag)) 然后就可以这样写了： 12In [4]: blog.tags[0].nameOut[4]: 't1' 添加新的 Tag 也方便了很多： 12In [3]: for tag in [Tag(name='t3'), Tag(name='t4')]: ...: blog.tags.append(tag) 混合关联代理 现在回到了第一个问题的出发点，能不能在上一个例子的基础上简化 tags 的调用呢？同样没问题，只要在 Association 中加一个关联代理： 12345class Association(Base): ... tag_objects = relationship('Tag', backref=backref('tag_blogs', lazy='dynamic'), lazy='joined') tags = association_proxy('tag_objects', 'name', creator=lambda name: Tag(name=name)) 然后用起来就和第一个例子一样了： 123456In [1]: blog.tagsOut[1]: ['t1', 't2']In [2]: blog.tags.append('t3')In [3]: blog.tagsOut[3]: ['t1', 't2', 't3'] 结语 上述内容并没有很复杂的操作，都是一些易于实现并且可以改善日常使用体验的方法。SQLAlchemy 还有很多骚操作可以讲，但是受限于本人的姿势水平，很多并没有实际使用过，也谈不上有什么见解。那就这样吧~","categories":[],"tags":[]},{"title":"文本聚类系列教程：（一）jieba中文分词工具入门","slug":"2018-03-12/文本聚类系列教程：（一）jieba中文分词工具入门","date":"2018-03-17T09:20:22.000Z","updated":"2018-04-02T14:12:49.203Z","comments":true,"path":"posts/575e441b/","link":"","permalink":"http://weafteam.github.io/posts/575e441b/","excerpt":"","text":"最近在学习文本分类（聚类）的相关知识，所以接下来准备先写一个关于这个方面的系列博客。 写在前面： 先介绍下由我们四个人组成的组织：FOUR ELEMENTS。四元素分别对应WELL、EARTH、AIR、FLAME，根据首字母缩写，我们的博客主页得名WEAF。 接下来介绍下我自己，我叫Leno，对应于四元素里面的Well，目前研究生在读，方向为智能信息处理。我的博客主要会以日常遇到的问题以及学习的知识为主。 简单的介绍： 首先，我们要做的是对中文文本的聚类，如果做聚类的话，我们需要对文本的内容做分析，而分析的最小单位肯定是词。 其次，中文和英文的词是有区别的，最大的区别就是中文的词与词之间并不是用空格分隔开的，而且由于中国文化的博大精深，切词的时候我们需要考虑的词语组合情况就更多了。显然让我们自己去造一个这样的轮子有点不现实，其实像这样的工具，前辈们已经为我们做好了，而且超好用。 本文介绍的就是jieba中文分词，正如它的口号那样。如下图所示： 当然，这里有两本秘籍GitHub &amp;&amp; OSChina，既然你我有缘，便免费赠予你。 安装 这年头，没有什么是一句pip install 解决不了的。不管2或者3，直接pip即可。 1pip install jieba 结合官方Demo理解jieba的三种切词模式 三种模式： 精确模式（默认模式）：它会试图将句子最精确的切开，适合文本分析。 全模式：不考虑歧义，这个模式会将所有的可以成词的词语都扫描出来，因而速度会非常快。 搜索引擎模式：该模式是在精确模式的基础上，对长词再进行切分，提高召回率，适用于搜索引擎分词。 官方Demo： 12345678910111213import jiebaseg_list = jieba.cut(&quot;我来到北京清华大学&quot;, cut_all=True)print(&quot;全模式: &quot; + &quot;/ &quot;.join(seg_list)) # 全模式seg_list = jieba.cut(&quot;我来到北京清华大学&quot;, cut_all=False)print(&quot;精确模式: &quot; + &quot;/ &quot;.join(seg_list)) # 精确模式seg_list = jieba.cut(&quot;他来到了网易杭研大厦&quot;) # 默认是精确模式print(&quot;默认模式：&quot; + &quot;/ &quot;.join(seg_list))seg_list = jieba.cut_for_search(&quot;小明硕士毕业于中国科学院计算所，后在日本京都大学深造&quot;) # 搜索引擎模式print(&quot;搜索引擎模式：&quot; + &quot;/ &quot;.join(seg_list)) 结果： 模式分析： 这里我们先分析这三种模式，对于cut方法的讲解在后边会给出，so不要问我为啥不给出cut方法中第三个参数HMM。 通过对比前两条输出可以看出全模式情况下，它会找出所有可以组成词的划分，而精确模式与其对比给出的答案就会很清爽。所以结合上文所说，不难理解这两个模式的区别。 接下来我们看第四条输出，它是在精确模式的基础上对长词再做的划分。所以‘日本京都大学’，它会再次切分为‘日本’，‘京都’，‘大学’三个词，同理适用于‘中国科学院’。所以这个模式也不难理解吧。 补充分析： 最后看第三条输出内容，也许你会问，既然知道默认模式是精确模式了，为啥还要给出试例，况且还是一个不具有对比性质的对比。这里其实想说明的是： ‘杭研’并没有在词典中，但是jieba的Viterbi算法也将其识别了出来。 这时我们就需要考虑HMM这个参数了，关于HMM（Hidden Markov Model，HMM：隐马尔可夫模型），如果深究，那就需要另外一篇博文了，所以我们只要能理解官方给出的这句话即可：对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法。 可能说的比较干涩，我们实际测一下吧。 补充测试代码： 1234567import jiebaseg_list = jieba.cut(&quot;他来到了网易杭研大厦&quot;,HMM=False)print(&quot;HMM为False：&quot; + &quot;/ &quot;.join(seg_list))seg_list = jieba.cut(&quot;他来到了网易杭研大厦&quot;,HMM=True)print(&quot;HMM为True：&quot; + &quot;/ &quot;.join(seg_list)) 补充测试结果： 所以一般情况下，使用cut方法，不用考虑HMM这个参数就可以，让它默认为True即可，让Viterbi算法为我们识别新词。HMM也能有效的解决中文中的歧义问题。 启用HMM并不适用所有情况，根据需要开启！！！ 关于切词的方法以及切词的注意事项，请大家参考上文给出的两个链接，这里我不再赘述。 基于TF-IDF的关键词提取 相关知识： 对于一个文档，我们肯定不会对所有的词进行聚类，所以我们需要对文档进行关键词提取。 下面我们对TF-IDF做一下简单的说明。如果单讲这个知识点，拿出来又是一篇博文。不过后续我也会写一篇关于它的博文。暂时请大家自行查阅相关资料学习。 TF-IDF是一种统计方法，用于评估一个词对于一个文件集或者语料库中的一份文件的重要程度。 TF(term frequency)：指的是某一个给定的词语在该文件中出现的频率。公式如下： \\(tf_i,_j = \\frac{n_i,_j}{\\sum_k n_k,_j}\\) IDF(Inverse document frequency)：是一个词语普遍重要性的度量。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到： \\(idf(t,D) = log(\\frac{N}{\\lvert {d \\in D, t \\in d}\\rvert})\\) 关键词提取： 官方给了一个代码示例文件，源代码在这里：关键词提取源码 但是为了结果显示得更清晰一点，我做了些许的改动： 12345678910111213141516171819202122232425262728293031import syssys.path.append(&apos;../&apos;)import jiebaimport jieba.analysefrom optparse import OptionParserUSAGE = &quot;usage: python extract_tags.py [file name] -k [top k]&quot;parser = OptionParser(USAGE)parser.add_option(&quot;-k&quot;, dest=&quot;topK&quot;)opt, args = parser.parse_args()if len(args) &lt; 1: print(USAGE) sys.exit(1)file_name = args[0]if opt.topK is None: topK = 20else: topK = int(opt.topK)content = open(file_name, &apos;rb&apos;).read()tags = jieba.analyse.extract_tags(content, topK=topK,withWeight=True)for i in tags : print(i) 先说下用法，官方在文件的第8行给出了用法，即： 1python extract_tags.py [file name] -k [top k] 将这个Extract_tags.py文件和文本文件放在同一目录下，然后给利用如上命令便可得到文本的关键词。默认取得是top10，我改了下取了top20，我们这里做下测试（使用jieba的官方测试文档：《围城》），结果如下： 分析： 官方给的代码看着挺长，实际上超简单，其中重要的无非两句话，一句是读文件，另一句则是调用extract_tags()方法，我在原有的基础上设置了withWight=True，因而返回了一个权重值。大家如果嫌麻烦可以对上述关键代码进行抽取，写一个自己的测试。 正如上图所示，‘自己’、‘知道’、‘先生’等等等等，像这些词语都是些没有实际意义的单词，所以在聚类的时候这些单词不应该做为聚类（或者分类）的标准，它们属于stop_words，中文的意思就是停用词，所以我们接下来处理这个问题。 去除停用词 去除停用词，我们需要知道哪些属于停用词，我在CSDN上找到了一个1893规模的停用词表，链接如下：最全中文停用词表整理（1893个）。 我们接下来的工作思路是这样的，对《围城》（文件1.txt）进行切词，方法就是之前的cut()，读取StopWords文件，对比每个切分出来的单词是否是停用词，如果不是则加入到一个list中，然后再将这个list的内容存到另一个文件2.txt中，对文件2.txt使用之前说到的官方给的关键词提取文件做关键词提取即可。 去除停用词代码如下： 12345678910111213141516171819202122232425262728293031import sysimport jiebafrom os import pathd = path.dirname(__file__) # 获取当前文件的dir路径stopwords_path = &apos;stopwords1893.txt&apos; # 停用词表路径text_path = &apos;txt/1.txt&apos; #《围城》的文本路径text = open(path.join(d, text_path),&apos;rb&apos;).read()def RmStopWords(text): mywordlist = [] seg_list = jieba.cut(text, cut_all=False) liststr=&quot;/ &quot;.join(seg_list) # 添加切分符 f_stop = open(stopwords_path) try: f_stop_text = f_stop.read() finally: f_stop.close( ) f_stop_seg_list=f_stop_text.split(&apos;\\n&apos;) # 停用词是每行一个，所以用/n分离 for myword in liststr.split(&apos;/&apos;): #对于每个切分的词都去停用词表中对比 if not(myword.strip() in f_stop_seg_list) and len(myword.strip())&gt;1: mywordlist.append(myword) return &apos;&apos;.join(mywordlist) #返回一个字符串txt2 = RmStopWords(text)text_write = &apos;txt/2.txt&apos;with open(text_write,&apos;w&apos;) as f: f.write(txt2) print(&quot;Success&quot;) 结果： 分析： 由上图可见，我们的去停用词的效果还不错。 最后： 这篇博客先写到这里，下一篇博客我会讲到jieba中文分词的进阶篇。感谢阅读，如有问题可以通过邮件与我交流，邮箱：cliugeek@us-forever.com","categories":[{"name":"文本聚类","slug":"文本聚类","permalink":"http://weafteam.github.io/categories/文本聚类/"}],"tags":[{"name":"文本聚类","slug":"文本聚类","permalink":"http://weafteam.github.io/tags/文本聚类/"}]},{"title":"MySQL主从数据库的设置与Xtrabackup备份InnoDB(MySQL)","slug":"2018-03-12/linux-mysql","date":"2018-03-17T08:08:56.000Z","updated":"2018-04-02T14:12:49.202Z","comments":true,"path":"posts/2f5dded6/","link":"","permalink":"http://weafteam.github.io/posts/2f5dded6/","excerpt":"一、准备环境 两台服务器：服务器A、服务器B 服务器A：Red Hat Enterprise Linux Server release 6.5 (Santiago) 服务器B：Red Hat Enterprise Linux Server release 6.5 (Santiago) 服务器A IP：172.16.125.50 服务器B IP：172.16.125.52 MySQL版本：5.6.23 二、安装MySQL 具体安装请见 LinuxMySQL的安装(1) LinuxMySQL的安装(2) LinuxMySQL的安装(3)","text":"一、准备环境 两台服务器：服务器A、服务器B 服务器A：Red Hat Enterprise Linux Server release 6.5 (Santiago) 服务器B：Red Hat Enterprise Linux Server release 6.5 (Santiago) 服务器A IP：172.16.125.50 服务器B IP：172.16.125.52 MySQL版本：5.6.23 二、安装MySQL 具体安装请见 LinuxMySQL的安装(1) LinuxMySQL的安装(2) LinuxMySQL的安装(3) 三、主从库配置 1、主库在/etc/my.cnf里添加以下内容 12345678910#log日志log_bin=mysql_bin#server IDserver_id=2#忽略同步的库binlog-ignore-db=information_schemabinlog-ignore-db=clusterbinlog-ignore-db=mysql#需要同步的库binlog-do-db=test 2、从库在/etc/my.cnf里添加以下内容 12345678910log_bin=mysql_binserver_id=3binlog-ignore-db=information_schemabinlog-ignore-db=clusterbinlog-ignore-db=mysqlreplicate-do-db=ufind_dbreplicate-ignore-db=mysqllog-slave-updatesslave-skip-errors=allslave-net-timeout=60 四、主从库设置 1、进入主库，我们在主库中创建一个的账户，从库通过使用这个账号来同步数据。 1CREATE USER 'repl'@'172.16.125.52' IDENTIFIED BY '123456'; 2、赋予相应的权限 12345GRANT FILE ON *.* TO 'repl'@'172.16.125.52' IDENTIFIED BY '123456';GRANT REPLICATION SLAVE ON *.* TO 'repl'@'172.16.125.52' IDENTIFIED BY '123456';FLUSH PRIVILEGES; 3、重启数据库（主库）执行以下命令 1SHOW MASTER STATUS; 要记住以上的信息，在设置从库的时候需要填写并设置。 4、在从库里边执行以下命令 123stop slave;change master to master_host=&apos;172.16.125.50&apos;,master_user=&apos;repl&apos;,master_password=&apos;123456&apos;,master_log_file=&apos;mysql_bin.000023&apos;, master_log_pos=120;start slave; 5、然后执行一下命令查看状态 1show slave status \\G; 内容如下： 6、测试与提示 后期的测试中我们只针对test库进行了同步。 所以只能针对test进行的操作才有效。 如果后期对一些列库进行操作，需要 添加相应的配置 1234#主库配置文件binlog-do-db=test#从库配置文件replicate-do-db=test 并查询出最新的master的状态，停止从库。并改变从库的配置重启同步。 五、Xtrabackup的简单介绍 ——————- Percona XtraBackup 是世界上唯一的开源免费的MySQL热备份软件，可以执行非阻塞操作 InnoDB和XtraDB数据库的备份。 Percona XtraBackup可提供以下优点： 备份快速安全可靠 备份期间不间断的事务处理 节省磁盘空间和网络带宽 自动备份验证 更快的恢复时间保证正常工作 Percona XtraBackup 为所有版本的Percona服务器，MySQL和MariaDB提供MySQL热备份。 它可执行 流媒体，压缩和增量MySQL备份。 六、Xtrabackup的安装 如果在互联网下 可使用以下命令安装 1wget https://www.percona.com/downloads/XtraBackup/Percona-XtraBackup-2.4.4/binary/redhat/7/x86_64/percona-xtrabackup-24-2.4.4-1.el7.x86_64.rpm 获取相应rpm包 安装部分依赖(不同的操作系统可能已安装的库不尽相同) 1234rpm -ivh mysql-community-libs-compat-5.7.20-1.el7.x86_64.rpm#根据mysql版本而定yum list|grep perlyum -y install perl-DBI.x86_64 perl-DBD-MySQL.x86_64 然后安装Xtrabackup 1rpm -ivh percona-xtrabackup-24-2.4.4-1.el7.x86_64.rpm 参考： 1yum install cmake gcc gcc-c++ libaio libaio-devel automake autoconf bison libtool ncurses-devel libgcrypt-devel libev-devel libcurl-devel vim-common 七、Xtrabackup备份MySQL 12xtrabackup --defaults-file=/etc/my.cnf --user=root --password=root --host=localhost --backup --target-dir=/data/backups/可指定数据库--databases=test 八、Xtrabackup的备份恢复 备份之前必须先关闭MySQL server 然后删除data目录（/var/lib/mysql一般情况是这个） 1xtrabackup --copy-back --target-dir=/data/backups/ 执行完恢复之后需要设置文件权限 1chown -R mysql:mysql /var/lib/mysql 然后启动mysql 123systemctl start mysqld.service#或者使用服务service mysqld start 九、使用脚本自动备份7天之内的数据 12345678910111213141516171819202122#!/bin/sh# Database infoDB_USER=\"root\"DB_PASS=\"root\"DB_HOST=\"localhost\"# Others varsBCK_DIR=\"/opt/app/mysqlbackup\" #the backup file directoryCONF_DIR=\"/etc/my.cnf\"DATE=`date +%F`RMDATE=`date -d '-7 day' +%F`# TODOmkdir -p $BCK_DIR/$DATE/#Create dir for save backup dataxtrabackup --defaults-file=$CONF_DIR --user=$DB_USER --password=$DB_PASS --host=$DB_HOST --backup --target-dir=$BCK_DIR/$DATE/#Backup mysql datarm -rf $BCK_DIR/$RMDATE#Delete the backup 7 days ago#热备份数据库 加入crontab 130 2 * * * /bin/sh /home/scripts/mysqlbackup.sh 更多请参考官方文档","categories":[{"name":"Linux","slug":"Linux","permalink":"http://weafteam.github.io/categories/Linux/"}],"tags":[{"name":"Linux运维","slug":"Linux运维","permalink":"http://weafteam.github.io/tags/Linux运维/"}]},{"title":"chapter-01-AIR","slug":"2018-03-12/chapter-01-AIR","date":"2018-03-14T10:49:23.000Z","updated":"2018-04-02T14:12:49.111Z","comments":true,"path":"posts/8e8e4531/","link":"","permalink":"http://weafteam.github.io/posts/8e8e4531/","excerpt":"","text":"第一篇文章-TensorFlow Install 首先介绍一些我们这个组织，这是有四个人构成得一个组织，组织可以叫FOUR ELEMENTS。（也可以叫WEAF）分别对应WELL、EARTH、AIR、FLAME。（WEAF）。 其次我想做一下自我介绍，我的英文学名叫milittle。我开设的这个周刊名字叫AIR-周刊。希望把自己学习的一些内容分享给大家，也激励自己。学更多的知识。以后大家有什么要交流的，也可以一起交流。（邮箱地址会在文章末尾给出） 接下来我讲一下我后续每周在AIR-周刊里面会讲到的内容： 主要涉及TensorFlow框架使用多一些 后续也会分享一些机器学习方面的算法 也会有一些在人工智能方面的杂谈 上面说了一些，我想把这块做好，文章内容有什么变化，后续的文章里面会有所提及。 今天就介绍一些TensorFlow的简述和安装： TensorFlow是Google公司在2015年12月份开源的一个机器学习库，代码链接TensorFLow。 第二点为什么现在TensorFlow这么火，在人工智能界已经算得上是称霸的地位，我们可以从下面的图中可以看出TensorFlow的数据占据了一大半市场。 原因是什么呢 最主要的原因就是本身具有图运算的这个概念。使用简单，而且可以让程序员快捷的实现一些算法。从而可以用TensorFlow解决一些现实中的问题。图运算的概念我们后续会慢慢深入。大家不要着急。 还有一个原因，我想不用说大家也都知道，既然说了是Google的开源框架，那么技术就一定很牛逼。引得广大程序员的喜爱也是必然发生的事情。 而且用这个框架可以快速的解决一些机器学习的算法问题。是的编程效率也不断提高。 TensorFlow支持Mac、Windows、Linux。以后我们的实验有可能通过Windows进行，也有可能在Linux进行，而且以后的代码都是基于python3.X，所以希望大家可以实现基本的python3的语法知识和编程知识。还有就是TensorFlow支持CPU版本和GPU版本，安装的时候都有很多的注意事项，基于GPU版本的可能会比较麻烦。但是后续我会给大家出一个教程，分别在Windows下面和Linux下面配置自己的独立环境。让你的机器学习算法跑在你自己的机器上面。完成一些看起来炫酷的程序。 接下来我介绍一下TensorFlow的Windows CPU安装方法： 首先打开电脑，这个是一定的~ 去TensorFlow的官网下载Windows的版本。点击下面红色箭头的地方—随意，都可以跳转到一个关于windows安装的界面。（可能需要科学上网，逃） 点开界面以后的注意事项： windows7及其以后的操作系统版本 决定安装哪个TensorFlow的版本，GPU还是CPU（GPU会有有一些第三方的库依赖，CUDA），接下来我们的教程是CPU版本安装。 决定怎么安装TensorFlow：可选方式有native pip 和 Anaconda等（我们使用Anaconda） 最后一步验证你的安装效果 接下来一步一步来： 第一步、我们决定用Anaconda来安装TensorFlow，你要知道Anaconda是什么呢，它就是可以很好的管理python的一些依赖库。让你在不同python版本之间切换自如。所以我们使用这个工具来安装我们的TensorFlow。Anaconda也可以集成Spyder这些编程工具，使得你编写代码会方便一些。 第二步、首先你去Anaconda官网下载windows版本的Anaconda，具体安装就和普通的安装软件类似。这个地方需要注意的是不同python版本需要不同的Anaconda，别下错了。 第三步、安装好以后，我们打开Anaconda的控制台，就是开始里面找到Anaconda的应用，然后里面有一个Anaconda Prompt。打开以后，我们就开始了我们创建一个独立的TensorFlow独立的环境。 conda create -n tensorflow pip python=3.5 上面这命令的意思就是说在Anaconda管理的环境里面给我独立的创建一个python环境来，这个里面python的版本是3.5。注意一下，这个地方还没有安装tensorflow呢，上面的tensorflow只不过是创建的一个环境名字而已。 activate tensorflow 上面的命令是激活这个tensorflow的环境，你可以通过这个环境，添加一些你自己的python库，定制自己的python环境，这也是我使用Anaconda的原因，但是并不是只有Anaconda支持这样的方式。不要和我抬杠。 第四步，也就是正儿八经的安装TensorFlow的阶段，这里解释一下，上面为什么我执行的是tensorflow1，因为我的电脑上面已经有tensorflow这个环境了 pip install --ignore-installed --upgrade tensorflow 这个命令就是使用pip正常的安装tensorflow，这里的pip管理起来和普通的pip管理是一个道理，这里就不赘述了。 第五步，测试TensorFlow是否安装上 python 上面的命令是进入python解释器，然后执行下面的import语句 import tensorflow as tf 如果上面的命令执行完，如下图中一样，就算安装成功了，下面的那些语句是写了一个hello world！！！ 今天是为了我们以后在TensorFlow上开发所做的准备。希望大家安装顺利。我的个人邮箱是air@weaf.top。有什么问题可以单独发邮件问我。感谢你们的驻足。有什么不好的地方，可以给出意见。","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]}]}