{"meta":{"title":"WEAF 周刊","subtitle":null,"description":null,"author":"WEAF","url":"http://weafteam.github.io"},"pages":[],"posts":[{"title":"如何理解丘奇计数","slug":"2018-04-02/how-to-understand-church-numerals","date":"2018-04-07T19:39:29.000Z","updated":"2018-04-14T06:25:47.770Z","comments":true,"path":"posts/cd89a3b2/","link":"","permalink":"http://weafteam.github.io/posts/cd89a3b2/","excerpt":"","text":"前言 不想写 Python 了，这次换个主题：丘奇计数，又名 lambda 演算的自然数表示法。 什么是 lambda 演算 lambda 演算（也称为 λ 演算）是数学逻辑中的一种形式系统，它基于函数抽象和应用，使用变量绑定和替换来表示计算。 没错，上面这句话来自维基百科，基本上是一句正确的废话，看完了也不知道什么是 lambda 演算。不过这篇文章的重点不在 lambda 演算上，希望你已经了解了一些关于 lambda 演算的知识。如果有机会下一篇再展开说（可能 什么是自然数 在计算机科学和集合论中，我们把非负整数 \\((0, 1, 2, 3, 4...)\\) 称为自然数。皮亚诺给出了自然数的严格定义： \\(0\\) 是自然数； 如果 \\(n\\) 是自然数，那么 \\(n+1\\) 也是自然数（\\(n+1\\) 代表 \\(n\\) 的后继）； \\(0\\) 不是任何一个数的后继； 如果 \\(m\\) 与 \\(n\\) 都是自然数且 \\(m\\neq n\\)，那么 \\(n+1 \\neq m+1\\)； 设 \\(P(n)\\) 为关于自然数 \\(n\\) 的一个性质，如果 \\(P(0)\\) 正确， 且假设 \\(P(n)\\) 正确，则 \\(P(n+1)\\) 亦正确。那么 \\(P(n)\\) 对一切自然数 \\(n\\) 都正确。 存在一个集合 \\(N\\)，称其元素为自然数，当且仅当这些元素满足公理 1 - 5（也就是皮亚诺公理）。 在自然数集合上可以定义一组运算：加法、乘法等等，这里用加法举个例子： 1234def add(m, n): if n == 0: return m return add(m, n - 1) + 1 可以看出加法是由两条规则递归定义的： $ m + 0 = m$ \\(m + (n + 1) = (m + n) + 1\\) lambda 演算的自然数表示法 自然数当然不止可以用皮亚诺公理定义，丘奇首先把自然数和自然数上的运算定义在了 lambda 演算上，所以称之为丘奇计数。 下面就要开始丘奇计数的定义了，由于 lambda 演算的样子不太友好，所以还是用 Python 表示。 首先定义 0： 1zero = lambda f: lambda x: x 先不管它为什么是 0，让我们看看这个语句。它定义了一个接受一个参数 f 的函数，返回一个函数，这个函数接受一个参数 x，然后返回它。似乎很简单，但是它为什么是 0？ 把它放在一边，看看 1 的定义： 1one = lambda f: lambda x: f(x) 这个语句是什么意思呢？定义了一个接受一个参数 f 的函数，返回一个函数，这个函数接受一个参数 x，然后返回 f(x) 的调用结果。好像有些规律了，再看看 2： 1two = lambda f: lambda x: f(f(x)) 定义了一个接受一个参数 f 的函数，返回一个函数，这个函数接受一个参数 x，然后返回 f(f(x)) 的调用结果。 现在可以清楚的看到，每个自然数的后继都多调用了一次 f，自然数被表示为 f 的调用次数。 于是，我们可以很轻易的写出后继函数： 1succ = lambda n: lambda f: lambda x: f(n(f)(x)) 这个函数接受一个参数 n（也就是上面被定义的 0，1，2 等等），返回一个函数，这个函数在 n 的基础上多执行了一次 f，达到了求 n 的后继的目的。 现在让我们忘记 1 的定义，用 0 和后继重新定义一次： 12345678zero = lambda f: lambda x: xsucc = lambda n: lambda f: lambda x: f(n(f)(x))one = succ(zero) = (lambda n: lambda f: lambda x: f(n(f)(x)))(lambda f: lambda x: x) = lambda f: lambda x: f(((lambda f: lambda x: x)(f))(x)) = lambda f: lambda x: f((lambda x: x)(x)) = lambda f: lambda x: f(x) 和我们预想的完全一致。 加法 接下来试着定义一下加法，加法是两个数相加返回一个数（也就是说，加法是定义在自然数上的幺半群），所以签名长这样： 1add = lambda m: lambda n: lambda f: lambda x: ... 函数体呢？我们推广一下后继函数：后继函数在n的基础上多调用了一次 f，相当于 +1；那加法相当于 +m，也就是多调用 m 次 f，于是可以得出： 1add = lambda m: lambda n: lambda f: lambda x: m(f)(n(f)(x)) 乘法 乘法的签名也是一样： 1mul = lambda m: lambda n: lambda f: lambda x: ... 我们都知道乘法是从加法推广而来的：m * n 相当于加 m 次 n，所以可以使用加法的定义： 1mul = lambda m: lambda n: lambda f: lambda x: m(add(n))(zero)(f)(x) 上面的写法正确，不过太丑了，可以化简为： 1mul = lambda m: lambda n: lambda f: lambda x: m(n(f))(x) 求幂 求幂的签名也一样： 1pow = lambda m: lambda n: lambda f: lambda x: ... 求幂是由乘法推广而来的：mn 相当于乘 n 次 m，所以可以使用乘法的定义： 1pow = lambda m: lambda n: lambda f: lambda x: n(mul(m))(one)(f)(x) 同样可以化简为： 1pow = lambda m: lambda n: lambda f: lambda x: n(m)(f)(x) 结语 机智的同学一定发现我们并没有实现减法，这是因为减法的实现太复杂了。至于为什么减法的实现很复杂，以及如何实现减法，这里有一篇参考资料 ，有兴趣的话可以自行了解一下。","categories":[],"tags":[]},{"title":"chapter-04-AIR","slug":"2018-04-02/chapter-04-AIR","date":"2018-04-05T02:13:07.000Z","updated":"2018-04-14T06:25:47.769Z","comments":true,"path":"posts/466eca41/","link":"","permalink":"http://weafteam.github.io/posts/466eca41/","excerpt":"","text":"TensorFLow 基础 hi,又和大家见面了，上一次我们讲了建立模型步骤和一些基础的概念（Tensor、Placeholder），那么我们这次就继续我们的矩阵操作，因为在TensorFlow处理一些数学问题的时候，往往都是通过矩阵来存储数据，通过特定的矩阵运算，我们实现数据的处理，从而得到一些数据的特性。还有一些其他的Tensorflow的概念，我希望大家能坚持下去，只要将这些基础的概念学会，那么以后运用TensorFlow就会得心应手。 和TensorFlow一起工作的Matrices 12345678910111213141516171819202122232425262728293031# 矩阵和矩阵操作import tensorflow as tfimport numpy as npfrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()identity_matrix = tf.diag([1., 1., 1])print(sess.run(identity_matrix)) #注意这个地方，如果是TensorFlow的Tensor，# 那么使用sess的run方法才能将结果显示# 或者下面这种方式print(identity_matrix.eval(session = sess))A = tf.truncated_normal([2, 3]) # 2 * 3 大小 均值0 方差为1.print(sess.run(A))B = tf.fill([2, 3], 5.) # 2 * 3 使用5.填充print(sess.run(B))C = tf.random_uniform([3, 2]) # 3 * 2 随机初始化print(sess.run(C))D = tf.convert_to_tensor(np.array([[1., 2., 3.], [-3., -7., -1.], [0., 5., -2.]]))print(sess.run(D))print(sess.run(A+B)) # 加法和 tf.add() 一样print(sess.run(B-B)) # 减法和 tf.subtract() 一样print(sess.run(tf.matmul(B, identity_matrix))) # 矩阵乘法print(sess.run(tf.transpose(C))) # 矩阵转置print(sess.run(tf.matrix_determinant(D))) # 计算行列式print(sess.run(tf.matrix_inverse(D))) # 矩阵的逆print(sess.run(tf.cholesky(identity_matrix))) # cholesk分解（平方根分解）eigenvalues, eigenvectors = sess.run(tf.self_adjoint_eig(D)) # 求特征向量和特征值print(eigenvalues)print(eigenvectors) Math Operation（数学操作） 123456789101112131415161718192021222324252627282930import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()# math operationprint(sess.run(tf.div(3, 4)))print(sess.run(tf.truediv(3, 4)))print(sess.run(tf.floordiv(3., 4.)))print(sess.run(tf.mod(22., 5.)))print(sess.run(tf.cross([1., 0., 0.], [0., 1., 0.])))# Trig operationprint(sess.run(tf.sin(3.1416)))print(sess.run(tf.cos(3.1416)))print(sess.run(tf.div(tf.sin(3.1416 / 4.), tf.cos(3.1416 / 4.))))# custom operation# f(x) = 3 * x^2 - X + 10test_nums = range(15) # 生成一个listdef custom_polynomial(x_val): return (tf.subtract(3 * tf.square(x_val), x_val) + 10)print(sess.run(custom_polynomial(11)))# list expendexpected_output = [3 * x * x - x + 10 for x in test_nums]print(expected_output)for num in test_nums: print(sess.run(custom_polynomial(num))) Activation Function（激活函数） 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 激活函数主要是为了让神经网络模型具有非线性的特性import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()x_vals = np.linspace(start = -10, stop = 10, num = 100)# Relu Activation-&gt; max(0, x)print(sess.run(tf.nn.relu([-3., 3., 10.])))y_relu = sess.run(tf.nn.relu(x_vals))# Relu6 Activation-&gt; min(max(x, 0), 6)print(sess.run(tf.nn.relu6([-3., 3., 10.])))y_relu6 = sess.run(tf.nn.relu6(x_vals))# Sigmoidactivation-&gt; 见公式1print(sess.run(tf.nn.sigmoid([-1., 0., 1.])))y_sigmoid = sess.run(tf.nn.sigmoid(x_vals))# Hyper Tangent activation-&gt;见公式2print(sess.run(tf.nn.tanh([-1., 0., 1.])))y_tanh = sess.run(tf.nn.tanh(x_vals))# softsign activation-&gt;见公式3print(sess.run(tf.nn.softsign([-1., 0., 1.])))y_softsign = sess.run(tf.nn.softsign(x_vals))# softplus activation-&gt;见公式4print(sess.run(tf.nn.softplus([-1., 0., 1.])))y_softplus = sess.run(tf.nn.softplus(x_vals))# Exponential linear activation-&gt;见公式5print(sess.run(tf.nn.elu([-1., 0., 1.])))y_elu = sess.run(tf.nn.elu(x_vals))plt.plot(x_vals, y_softplus, 'r--', label='Softplus', linewidth=2)plt.plot(x_vals, y_relu, 'b:', label='ReLU', linewidth=2)plt.plot(x_vals, y_relu6, 'g-.', label='ReLU6', linewidth=2)plt.plot(x_vals, y_elu, 'k-', label='ExpLU', linewidth=0.5)plt.ylim([-1.5,7])plt.legend(loc='upper left')plt.show()plt.plot(x_vals, y_sigmoid, 'r--', label='Sigmoid', linewidth=2)plt.plot(x_vals, y_tanh, 'b:', label='Tanh', linewidth=2)plt.plot(x_vals, y_softsign, 'g-.', label='Softsign', linewidth=2)plt.ylim([-2,2])plt.legend(loc='upper left')plt.show()下图给出激活函数的曲线图 公式1： \\[ \\sigma(x)=\\frac{1}{1+e^{-x}} \\] 公式2： \\[ f(x)=\\frac{e^x-e^{-x}}{e^x + e^{-x}} \\] 公式3： \\[ f(x) = \\frac{1}{1 + |x|} \\] 公式4 \\[ f(x) = \\log(1 + e^x) \\] 公式5： \\[ elu(x) = \\begin{cases} x &amp; x &gt; 0 \\\\ \\alpha(exp(x) - 1) &amp; x \\leq 0 \\end{cases} \\] Operations on a Computational Graph 1234567891011121314151617181920212223242526272829import osimport matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()# 创建的数据是要喂给下面的placeholder的x_vals = np.array([1., 3., 5., 7., 9.])# 创建placeholderx_data = tf.placeholder(tf.float32)# 创建一个乘数m = tf.constant(3.)# 乘法prod = tf.multiply(x_data, m)for x_val in x_vals: print(sess.run(prod, feed_dict = &#123;x_data: x_val&#125;))#下面将数据计算图输出到文件里面，供我们后来启动tensorboard使用merged = tf.summary.merge_all(key = 'summary')if not os.path.exists('tensorboard_logs/'): os.makedirs('tensorboard_logs/')my_writer = tf.summary.FileWriter('./tensorboard_logs/', sess.graph) Layering Nested Operations 123456789101112131415161718192021222324252627282930313233343536373839404142import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tfimport osfrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()# 创建数据为了feedmy_array = np.array([[1., 3., 5., 7., 9.], [-2., 0., 2., 4., 6.], [-6., -3., 0., 3., 6.]])# 复制x_vals = np.array([my_array, my_array + 1])# 声明placeholderx_data = tf.placeholder(tf.float32, shape = [3, 5])# 声明常数来操作m1 = tf.constant([[1.], [0.], [-1.], [2.], [4]])m2 = tf.constant([[2.]])a1 = tf.constant([[10.]])# 声明操作prod1 = tf.matmul(x_data, m1)prod2 = tf.matmul(prod1, m2)add1 = tf.matmul(prod2, a1)# 打印验证结果for x_val in x_vals: print(sess.run(add1, feed_dict = &#123;x_data: x_val&#125;)) #下面将数据计算图输出到文件里面，供我们后来启动tensorboard使用merged = tf.summary.merge_all(key = 'summaries')if not os.path.exists('tensorboard_logs/'): os.makedirs('tensorflow_logs/')my_writer = tf.summary.FileWriter('tensorboard_logs/', sess.graph)#下图就是在操作过程中，tensorflow建立的图运算模型 Working With Multiple Layers 123456789101112131415161718192021222324252627282930313233343536373839404142434445import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tfimport osfrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()x_shape = [1, 4, 4, 1]# 定义一个4 * 4 大小的随机矩阵x_val = np.randim.uniform(size = x_shape)x_data = tf.placeholder(tf.float32, shape = x_shape)# 定义一个空间移动窗口，也就是卷积操作的卷积核# 大小是2 * 2， 步长是 2# filter的值是一个固定的值0.25my_filter = tf.constant(0.25, shape = [1, 2, 2, 1])my_strides = [1, 2, 2, 1]mov_avg_layer = tf.nn.conv2d(x_data, my_filter, my_strides, padding = 'SAME', name = 'Moving_Avg_Window')# 第二层def custom_layer(input_matrix): input_matrix_sqeezed = tf.squeeze(input_matrix) A = tf.constant([1., 2.], [-1., 3.]) b = tf.constant(1., shape = [2, 2]) output = tf.add(tf.matmul(A, input_matrix_sqeezed), b) return tf.nn.relu(output)with tf.name_scope('custom_layer') as scope: custom_layer1 = custom_layer(mov_avg_layer)# 运行结果print(sess.run(mov_avg_layer, feed_dict = &#123;x_data: x_val&#125;))print(sess.run(custom_layer1, feed_dict = &#123;x_data: x_val&#125;))#下面将数据计算图输出到文件里面，供我们后来启动tensorboard使用merged = tf.summary.merge_all(key = 'summaries')if not os.path.exists('tensorboard_logs/'): os.makedirs('tensorboard_logs/')my_writer = tf.summary.FileWriter('tensorboard_logs', sess.graph)# 下图是计算图 总结：这一次，一开始主要讲了矩阵的一些操作，后续又进行了数学操作，激活函数，运算图、层内元素嵌套运算还有最好的多层运算，并给出了tensorboard的计算图结构。大家不仅仅要看一看，也要动手做一做哦。 一如既往的有什么问题可以直接联系milittle，air@weaf.top邮箱","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]},{"title":"TensorFlow 建立网络模型","slug":"2018-03-26/chapter-03-AIR","date":"2018-03-31T10:23:35.000Z","updated":"2018-04-02T14:12:49.208Z","comments":true,"path":"posts/5b7df854/","link":"","permalink":"http://weafteam.github.io/posts/5b7df854/","excerpt":"","text":"TensorFlow 建立网络模型 上次一我们在fashion-mnist上面体验了一把，但是里面有一些建立模型和一些TensorFlow的基础概念都没有给大家讲，所以这节决定将这方面的知识介绍一些，上节是为了引起大家的注意，TensorFlow具有很强大的功能，我们只能后续慢慢的学习。 其实在上一次的实例中，有很多地方确实是很困惑的，如果没有接触过机器学习的小伙伴可能理解起来会有一些问题，那么我开头就稍微讲一下，机器学习有一些什么？就我现在了解的一些内容给大家介绍，有可能有一些不到位的地方，还请多多包涵： 其实机器学习，总的宗旨就是利用数据的特征来做识别和分类等任务 第一大类是分类工作，假设有一百类，经典的做法，就是使用神经网络提取一些数据的特征，然后利用softmax输出层进行不同种类概率的预测： \\[ softmax(i) = \\frac{X_i}{\\sum_{i=0,99}X_i} \\] 上面是softmax层计算的公式，从一百类里面找出每一类的概率值，然后按照概率值来预测输入数据是哪一种类型，就像上一次文章里面的fashion-mnist的数据一样，会预测出输出的类别。softmax(i)代表的就是这个种类的概率值，取最大值作为预测类别。 你可以把一个矩阵看成一个数据集合，一行是一个数据信息，就和我们的关系型数据一样，一行代表一个表的一条信息，那么每列就是每一行数据的一个属性，那么在机器学习里面就是数据的特征了，因为在网络模型中，每个特征都有对应的权重，那么，对于每个特征来说，对于最后的分类，识别等工作起的重要程度是不一样的。这也和我们的数据库信息差不多，有一些信息也是无关紧要的。有些信息可以主要决定这一行数据。 第二大类就是回归，回归可以看作是一个连续的分类，对于二维数据来说，其实就是根据你给出的数据来拟合一条线。对于三维来讲就是拟合一个平面。再高维就是超平面。 最近，也就是2018年3月31在加利福尼亚州山景城的计算机历史博物馆举办了第二届TensorFlow开发者峰会，会上有超过500名使用TensorFlow的用户，还有一些观众，大家有兴趣的话可以关注youtube的TensorFlow官方频道。可以查看开会的视频。 TensorFlow应用广泛，其中有使用TensorFlow来做开普勒任务分析的 也有使用TensorFlow预测心脏发作和中风概率 还有一些应用在现实当中的项目。 这让我们认识到TensorFlow对于实际领域中应用的越来越广泛，所以我们不学习是不是有点亏。这么好的开源项目。 上一次我们既然做过了一次服装类别识别，那么这次我主要从TensorFlow建立模型的步骤讲起：让大家再深入理解一下TensorFlow。 第一步也是很重要的一步，那就是导入数据。 第二步一般就是对数据进行的预处理，一般包括归一化数据，转换数据等操作。 第三步设置算法的超参数，一般也就是学习率，batch_size(批处理个数)，epoch(轮次)。这里举一个例子，假如你有10000条训练数据。那么，batch_size设置为100，那么你的一个epoch就迭代100次才能将所有数据训练一遍，每次输入数据是100条，因为一个epoch的意思就是训练完一次训练数据，所以一个epoch是迭代100次就可以结束一轮了。learning_rate一般设置为0.1-0.0001之间，但是也不排除一些特殊情况，主要是learning_rate设置的过小，反向传播更新参数的时候速度会很慢，设置的过大，会出现无法收敛的情况。 第四步设置变量和placeholders，变量是记录权重和偏置项信息的，一般在最小化loss函数的时候，反向传播算法会更新权重和偏置项，TensorFlow导入数据是通过placeholders来实现的，大家还记得我们上次的fashion-mnist识别，我的数据就是通过先定义placeholders，最后在Session运行的时候，在feed_dict这个字典参数里面将训练数据喂进去的。 1234a_var = tf.constant(42)x_input = tf.placeholder(tf.float32, [n_x, None], name=\"X\")x_output = tf.placeholder(tf.float32, [n_y, None], name=\"y\")# 定义输入数据的一些方式 第五步定义图模型，我们有了数据，初始化了变量和placeholders，那我们就需要定义一个图模型，来生成TensorFlow的图模型（计算图）我们必须告诉TensorFlow对我们的数据进行哪些操作，来让我们的模型具有预测能力（更加深入的运算我们在后续的博客里面会陆续讲到） 1h_pre_output = tf.add(tf.matmul(W, x_input) + B) 第六步声明loss函数，在上面计算图中我们定义了一些对我们数据的操作。那么我们需要验证我们预测的输出，和我们真实之间的差距，一般对于回归任务来讲的话，就是平方误差：这样就求得了平方误差。但是对于分类任务，那就是交叉熵误差。就像上一节我们用到的loss生成函数就是softmax这种方式。，交叉熵的公式后续用到再给大家介绍。 \\[ loss(i)=\\frac{1}{N}\\sum{_i}(y\\_pre_i-y\\_true_i)^2 \\] 12TensorFlow求法：loss = tf.reduce_mean(tf.square(y_pre - y_true)) 第七步声明了loss函数以后，我们需要使用BP算法也就是反向传播算法来更新权重和偏置项。在TenorFlow框架里面有好多这样的优化器，都在 tf.train这个模块里面。 12optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)这个就是我们上次使用的优化器，来优化我们的loss 最后一步那就是初始化会话Session()，开始训练模型 123with tf.Session() as session: session.run(init) ..... 由上面的步骤，大家再结合上一次的网络代码，是不是可以理解了TensorFlow在建立一个网络模型的时候的具体步骤。 其实在TensorFlow中还有一个很重要的概念，那就是Tensor，上次说过了它的概念，那么接下来我讲一下TensorFlow里面的Tensor。 1234567891011121314151617181920212223242526import tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()# 定义一个会话，记得，TensorFlow里面都是通过session来执行的sess = tf.Session()# 创建一个1 * 20的向量tensor_zeros = tf.zeros([1, 20])sess.run(tensor_zeros) # 你可以运行一下看看my_var = tf.Variable(tf.zeros([1, 20])) # 使用tenso来初始化变量sess.run(my_var.initializer) # 又一种运行变量初始化器的方式sess.run(my_var) #打印出来看看# tf.ones() 生成全是1# tf.zeros() 生成全是0# tf.constant() 生成一个常量Tensor# 如果我们想要通过一个已知的Tensor来创建另一个，则可以使用ones_like()和zeros_like()这两个函数zero_similar = tf.Variable(tf.zeros_like(tensor_zeros))sess.run(zero_similar.initializer)print(sess.run(zero_similar))# 注意上面的两个函数的参数是为了确定生成Tensor的大小，而产生的值是通过函数决定的tf.fill([row, col], -1) # 用具体的数字填充tf.linspace(start=0.0, stop=1.0, num=3) # 线性分布 包括endtf.range(start=6, limit=15, delta=3) # 也是线性均匀 不包括endtf.random_normal([row_dim, col_dim], mean=0.0, stddev=1.0) # 随机 均值0 方差1.0tf.random_uniform([row_dim, col_dim], minval=0, maxval=4) # 或者最小最大值随机初始化 1234567891011121314151617import tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()my_var = tf.Variable(tf.zeros([1,20]))merged = tf.summary.merge_all()writer = tf.summary.FileWriter(\"./tmp/variable_logs\", graph=sess.graph)initialize_op = tf.global_variables_initializer()sess.run(initialize_op)# 上面的就是一个Tensor放在一个变量里面，我们使用了一条语句 merged = tf.summary.merge_all() 还有writer = tf.summary.FileWriter(\"/tmp/variable_logs\", graph=sess.graph)，这两句这是为了将变量在TensorBoard里面显示出来，让我们更加了解TensorFLow的一些操作。# 上面的操作过程会在当前文件夹里面创建一个/tmp/variable_logs文件夹然后会将变量信息存储在一个文件里面 那怎么使用tensorboard 12#进去我们的环境变量，然后执行tensorboard --logdir=tmp的绝对路径 可以看到我上面执行的命令。然后在浏览器里面输入127.0.0.1:6006然后你就可以看到刚才那个变量的操作过程，这就是tensorboard的魅力 上面就是一个变量在进行初始化时候可视化显示 Placeholders使用(一样可以使用tensorboard来查看) 123456789101112131415import numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()# 定义一个placeholderx = tf.placeholder(tf.float32, shape = (4, 4))# 随机生成4 * 4的矩阵reand_array = np.random.rand(4, 4)y = tf.identity(x) # 返回与输入对象相同的内容和大小print(sess.run(y, feed_dict=&#123;x: rand_array&#125;))merged = tf.summary.merge_all()writer = tf.summary.FileWriter(&quot;./tmp/variable_logs&quot;, sess.graph) 总结 这次我们就TensorFlow的一些基础概念的介绍，也是为了让大家在以后的TensorFlow使用过程中少一些疑问，后面的章节，我们会慢慢深入。小伙伴们不要着急，我的邮箱是air@weaf.top，依旧是那个可以交流学习的milittle。谢谢大家的驻足。 第一篇 TensorFlow安装 第二篇 TensorFlow初体验（fasion-mnist识别） 修改pip全局镜像方法","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]},{"title":"文本聚类系列教程：（三）构建词袋空间VSM（Vector Space Model）","slug":"2018-03-26/文本聚类系列教程：（三）构建词袋空间VSM（Vector-Space-Model）","date":"2018-03-30T06:00:08.000Z","updated":"2018-04-14T05:59:22.791Z","comments":true,"path":"posts/a751f7e5/","link":"","permalink":"http://weafteam.github.io/posts/a751f7e5/","excerpt":"","text":"咱们今天先聊个概念吧，著名的聚类假设，这也是文本聚类的依据，内容如下：该假设认为，同类的文档相似度较大，而不同类的文档相似度较小。 概念： 对于上述概念，也就是做文本聚类的基础，如果不相关的文档反而相似度高，我们便无法做文本聚类。 接下来再说VSM(Vector Space Model),对于VSM的定义，我在网上搜罗了些资料，如下所示： Vector space model (or term vector model) is an algebraic model for representing text documents (and any objects, in general) as vectors of identifiers, such as, for example, index terms. It is used in information filtering, information retrieval, indexing and relevancy rankings. Its first use was in the SMART Information Retrieval System. A document is represented as a vector. Each dimension corresponds to a separate term. If a term occurs in the document, its value in the vector is non-zero. Several different ways of computing these values, also known as (term) weights, have been developed. One of the best known schemes is tf-idf weighting. The definition of term depends on the application. Typically terms are single words, keywords, or longer phrases. If the words are chosen to be the terms, the dimensionality of the vector is the number of words in the vocabulary (the number of distinct words occurring in the corpus). 拙劣的翻译： 向量空间模型是用来表示文本文档（通常也包含一些对象）的特征向量的代数模型，例如索引词项。它被应用于信息过滤、信息检索、索引和相关度计算。这个模型最早被应用于SMART信息检索系统。 一个文本文档表示一个向量。每一个维度相当于一个单独的词项（term）。如果一个词项（term）出现在一个文档中，那么它在表示该文档的向量中对应项不为0.有一些计算这些词项（term）权重的方法被逐渐提出来，其中最著名的方法就是tf-idf权重计算方法。 对于词项（term）的定义依赖于应用。一般而言，词项（term）可以是单词、关键字、或者长短语。如果单词作为词项（term），那么向量中的维度就是词汇表中的单词的个数（出现在文档全集中所有不同的单词的数量）。 小荔枝： 举个荔枝吧 ，方便理解上述的概念。首先假设有这样两个文本 1.我来到北京清华大学 2.他来到了网易杭研大厦 分词结果为：我/来到/北京/清华大学和他/来到/了/网易/杭研/大厦统计所有文档的词集合：我/来到/北京/清华大学/他/了/网易/杭研/大厦，按照1983停用词去除停用词后结果为：来到/北京/清华大学/网易/杭研/大厦 我们对这两个文本构建向量，结果如下 来到 北京 清华大学 网易 杭研 大厦 文本1 1 1 1 0 0 0 文本2 1 0 0 1 1 1 相信你已经对VSM的认识有了一个大致的轮廓，但是细心的你也可能发现了，我们在上述的例子中计算term值的方法仅仅只是计数，这样的term值是否有意义呢？我们是否能用这样的方法直接进行接下来的计算呢？对于前一个问题，答案是肯定的。不管在此基础上做什么样的改进，我们最基础的就是统计单词出现的次数，那就让我们先把上述的代码实现一下吧(与该文件同目录下有个名为txt1的文件夹，里面有1.txt和2.txt两个文件，内容分别是上述所说的两个文档，我们在上次RmStopWord.py的基础上再做修改)： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import sysimport jiebaimport osimport numpy as npfrom os import pathd = path.dirname(__file__) # 获取当前文件的dir路径stopwords_path = 'stopwords1893.txt' # 停用词表路径#text = open(path.join(d, text_path),'rb').read()def read_from_file(file_name): with open(file_name,\"r\") as fp: words = fp.read() return wordsdef RmStopWords(text): mywordlist = [] seg_list = jieba.cut(text, cut_all=False) liststr=\"/ \".join(seg_list) # 添加切分符 f_stop = open(stopwords_path) try: f_stop_text = f_stop.read() finally: f_stop.close( ) f_stop_seg_list=f_stop_text.split('\\n') # 停用词是每行一个，所以用/n分离 for myword in liststr.split('/'): #对于每个切分的词都去停用词表中对比 if not(myword.strip() in f_stop_seg_list) and len(myword.strip())&gt;1: mywordlist.append(myword) return mywordlistdef get_all_vector(file_path): names = [ os.path.join(file_path,f) for f in os.listdir(file_path) ] txts = [ open(name).read() for name in names] docs = [] word_set = set() for txt in txts: doc = RmStopWords(txt) docs.append(doc) word_set |= set(doc) word_set = list(word_set) docs_vsm = [] # 这里只是想显示有多少term for word in word_set[:30]: print(word) for doc in docs: temp_vector = [] for word in word_set: temp_vector.append(doc.count(word) * 1.0) docs_vsm.append(temp_vector) docs_matrix = np.array(docs_vsm) return docs_matrix # txt2 = RmStopWords(read_from_file(text1_path))# print(txt2)#文件路径为txt1/1.txt和2.txt，只不过我让程序循环扫描txt1下所有的文本文件txt3 = get_all_vector('txt1')print(txt3) 运行结果： 分析： 上述结果不言而喻，那么我们接着讨论，显而易见，我既然提出了第二个疑问就一定有它被提出的道理，仅仅只计算term值的方法显然存在问题，我们再随便举个例子，文本1中北京只出现了1次，但是文本1中只有3个单词，文本2中北京出现了10次但是文本2中有1000个单词，那我们用上述的方法显然不合适。所以接下来我们便要讲一个最著名的方法tf-idf计算权值的方法。 TF-IDF(term frequency–inverse document frequency) 维基百科和百度百科上的讲的很清楚，这里截取概念方便大家阅读，更详细的内容请参考前面所说的两个百科。 TF-IDF是一种统计方法，用以评估一个词(term)对于一个文件集或者一个语料库中的一份文件的重要程度。一个词(term)的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。 原理： TF-IDF的主要思想是：如果某个词或短语(term)在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语(term)具有很好的类别区分能力，适合用来分类。如果包含词条term的文档越少，也就是n越小，则IDF越大，则说明词条term也具有很好的类别区分能力。 思考： 现在想一下我们刚才提出的问题，针对我们上述的问题：同一词语在长文件里可能会比短文件有更高的词数，而不管该词重要与否。那么我们对词数做归一化就可以了，而TF就帮我们做了这样的事。那么我们就先给出TF的运算公式吧。 \\(tf_i,_j = \\frac{n_i,_j}{\\sum_k n_k,_j}\\) TF公式解读：上式中分子是该词在文件中出现的次数，而分母则是该词在文件中出现的词数之和。 我们再讲个小问题： 如果某一类文档C中包含词条t的文档数为m，而其他类包含t的文档总数为k，显然所有包含t的文档数n=m+k，而当m变大的时候，n也变大，这是后按照IDF的计算方法计算得到的IDF值会变小，也就相对应的说明该词条t类别区分能力不强。但是实际上，如果一个词条在一个类的文件中频繁出现，则说明该词条能够很好的代表这个类的文本的特征，这样的词条应该给它们赋予较高的权重，并选来作为该类文本的特征词以区别与其它类文档。其实这就是IDF的不足。 针对这个问题，我的想法是TF-IDF用来做信息检索和数据挖掘，为了获取更精准的效果，我们宁愿忽略这样不足来换取更加理想的效果（也就是TF-IDF计算出更大的权值）。（这里我的理解是这样的，如果有人有更好的解释，欢迎与我进行讨论，邮箱：well@weaf.top） 那么接下来就该给出IDF的计算公式了： \\(idf(t,D) = log(\\frac{N}{\\lvert {d \\in D, t \\in d}\\rvert})\\) IDF公式解读： |D|：语料库中文件的总数 分子为包含该词条t的文件数目，如果该词条不在语料库中，就会导致分母为零，因此一般使用1。 那就接着我们上述代码，运用TF-IDF，把对应的矩阵的单纯计数转换成权值计算吧： 123456789101112131415161718192021222324252627282930313233343536373839404142434445···def get_all_vector(file_path): names = [ os.path.join(file_path,f) for f in os.listdir(file_path) ] posts = [ open(name).read() for name in names] docs = [] word_set = set() for post in posts: doc = RmStopWords(post) docs.append(doc) word_set |= set(doc) word_set = list(word_set) docs_vsm = [] for word in word_set[:30]: print(word) for doc in docs: temp_vector = [] for word in word_set: temp_vector.append(doc.count(word) * 1.0) docs_vsm.append(temp_vector) docs_matrix = np.array(docs_vsm) #return docs_matrix column_sum = [ float(len(np.nonzero(docs_matrix[:,i])[0])) for i in range(docs_matrix.shape[1]) ] column_sum = np.array(column_sum) column_sum = docs_matrix.shape[0] / column_sum idf = np.log(column_sum) idf = np.diag(idf) i = 0 for doc_v in docs_matrix: if doc_v.sum() == 0: docs_matrix[i] = docs_matrix[i]/1 else: docs_matrix[i] = docs_matrix[i] / (doc_v.sum()) i+=1 tfidf = np.dot(docs_matrix,idf) return names,tfidftxt3 = get_all_vector(&apos;txt1&apos;)print(txt3) 结果： 本次的学习会用到很多numpy的知识，请大家自行查阅。如有兴趣，请思考为什么在新的权值矩阵中“来到”一词的权重变成了0。感谢大家的阅读~","categories":[{"name":"文本聚类","slug":"文本聚类","permalink":"http://weafteam.github.io/categories/文本聚类/"}],"tags":[{"name":"文本聚类","slug":"文本聚类","permalink":"http://weafteam.github.io/tags/文本聚类/"}]},{"title":"修改pip全局镜像","slug":"2018-03-26/alterM","date":"2018-03-27T12:29:36.000Z","updated":"2018-04-10T08:57:00.655Z","comments":true,"path":"posts/233074e6/","link":"","permalink":"http://weafteam.github.io/posts/233074e6/","excerpt":"","text":"修改pip全局镜像 第一次我们在windows上面安装了Anaconda，在使用pip安装Tensorflow中速度过慢，所以我为大家介绍一中修改全局pip源的方法（这样在使用pip下载依赖库的时候就会快一些）： 打开用户主目录：我的是C:\\Users\\milittle。 在里面新建pip文件夹，在pip文件夹中建立pip.ini文件。 在pip.ini文件中添加如下配置信息，我使用的豆瓣源： 123[global]timeout = 6000index-url = https://pypi.douban.com/simple 最后的目录结构就是：C:\\Users\\milittle\\pip\\pip.ini","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]},{"title":"如何理解描述符","slug":"2018-03-19/how-to-understand-descriptor","date":"2018-03-25T15:52:32.000Z","updated":"2018-04-02T14:12:49.205Z","comments":true,"path":"posts/5dd0238f/","link":"","permalink":"http://weafteam.github.io/posts/5dd0238f/","excerpt":"","text":"前言 上篇文章中挖了 property 和描述符的坑，这篇就把它填上好了_(:з)∠)_ property 是用描述符实现的，所以先说说 property。 property property 本身是一个实现了描述符协议的类，在不改变类接口的情况下，提供了一组对实例属性的读取、写入和删除操作。下面举个例子，一个银行账户的抽象，很容易实现： 12345class Account: def __init__(self, name, balance): self.name = name self.balance = balance 银行账户最常见的操作就是存款和取款了： 1234567891011121314In [1]: account = Account('zhang', 100) # 创建一个有 100 块存款的账户In [2]: account.balanceOut[2]: 100In [3]: account.balance -= 90 # 取 90 块In [4]: account.balance # 还剩 10 块Out[4]: 10In [5]: account.balance += 30 # 存 30 块In [6]: account.balance # 现在有 40 块Out[6]: 40 但是这里有个问题： 123456...In [7]: account.balance -= 50 # 再取 50 块In [8]: account.balance # 存款变成了负数！Out[8]: -10 当然这种操作是不该被允许的，我们需要对 balance 的写入做限制。Jawa 之类的语言会创建一组 getter、setter 方法来管理属性，但是这并不 Python，也对现有的代码不友好。正确的方式是使用 property。 12345678910111213141516class Account: def __init__(self, name, balance): self.name = name self.balance = balance @property def balance(self): return self._balance @balance.setter def balance(self, value): if value &lt; 0: raise ValueError('balance must greater than 0.') else: self._balance = value 现在 balance 被禁止设为小于 0 的数： 123456789101112131415161718192021In [1]: account = Account('zhang', 100)In [2]: account.balanceOut[2]: 100In [3]: account.balance += 40In [4]: account.balance -= 200---------------------------------------------------------------------------ValueError Traceback (most recent call last)...ValueError: balance must greater than 0.In [5]: account.balanceOut[5]: 140In [6]: account = Account('zhang', -1) # 初始化的时候也不行！---------------------------------------------------------------------------ValueError Traceback (most recent call last)...ValueError: balance must greater than 0. 可以看到我们使用 balance 的方式没有发生变化，但是对值的限制已经生效了。 property 还有一个 deleter 装饰器，处理应用于属性的 del；当然，del 本身用的也不多，大多数时候把销毁操作交给 Python 就可以了。不过如果涉及到复杂对象的引用，要做到 RAII（误，还是要手动实现的。 property 是类 property 本身是用 C 实现的，这里有一个纯 Python 的实现。正如上文所说，它本身是一个类，构造方法的签名如下： 123class property(object): def __init__(self, fget=None, fset=None, fdel=None, doc=None): pass 熟悉一点装饰器用法的话就可以看出上面的 123456class Account: ... @property def balance(self): pass 实际上就是 1234567class Account: ... def get_balance(self): pass balance = property(fget=get_balance) 如果不熟悉的话，下一篇就讲装饰器好了（误 property 的实例是类属性 上面的代码段同时展示了这样一个事实：property 的实例是类属性。这就涉及到了属性查找顺序的问题，简单试一下： 123456class Foo: data = 'data!' @property def bar(self): return 'bar!' 123456789101112In [1]: f = Foo()In [2]: f.dataOut[2]: 'data!'In [3]: f.data = 'f.data!'In [4]: f.dataOut[4]: 'f.data!'In [5]: Foo.dataOut[5]: 'data!' 实例属性覆盖了类属性，符合直觉。那么对 property 的实例来说呢？ 12345678In [6]: f.barOut[6]: 'bar!'In [7]: f.bar = 'bar'---------------------------------------------------------------------------AttributeError Traceback (most recent call last)...AttributeError: can't set attribute 尝试给 bar 赋值，失败了，也符合 property 的工作方式：执行赋值时，如果没有 setter 方法就抛出异常。那么直接修改 f.__dict__ 呢？ 1234In [8]: f.__dict__['bar'] = 'bar'In [9]: f.barOut[9]: 'bar!' 也不行，property 的实例完全覆盖了实例属性。但是，它是一个类属性，所以我们可以这样做： 123456789101112In [10]: Foo.barOut[10]: &lt;property at 0x29c44800408&gt;In [11]: Foo.bar = 'bar'In [12]: f.barOut[12]: 'bar'In [13]: f.bar = 'ba'In [14]: f.barOut[14]: 'ba' 对类属性的覆盖使 bar 不再是一个 property 的实例，所以也就不会覆盖后续的赋值了。 当然我们仍然可以用一个 property 的实例再次覆盖 Foo.bar： 1234In [15]: Foo.bar = property(fget=lambda self: 'bar!')In [16]: f.barOut[16]: 'bar!' 恢复原样。 property 的实例这种先从类中开始属性查找的方式，是一类描述符的工作模式。接下来就说说描述符。 描述符 描述符是指实现了描述符协议的类，这个协议包含四个方法，分别是 __get__，__set__，__delete__ 和 Python 3.6 新增的 __set_name__。通常，只要实现了 __get__ 或 __set__，就可以被称之为描述符。在某个角度上说，描述符的作用相当于抽象的 property，可以为一组属性提供相同的读取、写入和删除逻辑。接下来，还是从数据验证的例子开始。 下面是商店中一项商品的抽象，包含商品名、数量和单价： 12345678class Item: amount = Storage('amount') price = Storage('price') def __init__(self, name, amount, price): self.name = name self.amount = amount self.price = price 其中的 amount 和 price 都必须大于 0，所以可以用统一的描述符实现： 12345678910class Storage: def __init__(self, name): self.name = name def __set__(self, instance, value): if value &gt; 0: instance.__dict__[self.name] = value else: raise ValueError(f'&#123;self.name&#125; must greater than 0.') 由于我们并没有对读取方法有特别的需求，所以不用实现 __get__ 方法。 试一下： 1234567891011In [1]: item = Item('orange', 100, 0)---------------------------------------------------------------------------ValueError Traceback (most recent call last)...ValueError: price must greater than 0.In [2]: item = Item('orange', 0, 100)---------------------------------------------------------------------------ValueError Traceback (most recent call last)...ValueError: amount must greater than 0. 如果 amount 或 price 中的任何一个不大于 0，都会被禁止。 这里需要解释一下 __set__ 的签名中的 instance： 12def __set__(self, instance, value): pass instance 是 Item 的实例。因为描述符应该管理实例的属性，所以需要额外的参数提供相应的实例。这也是为什么我们不能这样写： 12def __set__(self, instance, value): self.__dict__[self.name] = value 这实际上是为描述符实例设置了值，而描述符实例是Item 类的类属性，所有的 Item 实例都共享相同的描述符实例。修改了某个描述符实例，相当于修改了所有的 Item 实例。 上面的例子有个缺点，初始化描述符实例的时候需要重复属性的名字。我们希望可以简单的写成： 1234class Item: amount = Storage() price = Storage() ... 而不需要在描述符的构造方法中重复属性名。这就是 Python 3.6 新增的 __set_name__ 方法的作用。只要实现 __set_name__ 方法： 12345class Storage: ... def __set_name__(self, owner, name): self.name = name 同样解释一下函数签名： 12def __set_name__(self, owner, name): pass owner 是 Item 类本身，name 是引用描述符实例的变量的名字。 如果使用的 Python 版本在 3.6 以下呢？有两个方法：第一个是用元类接管Item类的创建过程，这个不在这篇文章的内容之内（可能又挖了一个坑；第二个就是为每个描述符实例生成与属性名无关但是唯一字符串，用来代替属性名： 1234567891011121314151617class Storage: _counter = 0 def __init__(self): cls = self.__class__ self.name = f'_&#123;cls.__name__&#125;#&#123;cls._counter&#125;' cls._counter += 1 def __get__(self, instance, owner): return getattr(instance, self.name) def __set__(self, instance, value): if value &gt; 0: setattr(instance, self.name, value) else: raise ValueError('must greater than 0.') 由于 Item 中的属性名和我们实际保存的属性名不同，所以需要实现 __get__ 方法。与 __set_name__ 签名中的 owner 含义相同，__get__ 方法签名中的 owner 也是 Item 类本身。 现在，我们使用 _Storage#N 这样的名称在 Item 实例中保存属性。当然，这样的名称会让人有点困惑，特别是以类属性访问的时候： 12345In [1]: Item.amount---------------------------------------------------------------------------AttributeError Traceback (most recent call last)...AttributeError: 'NoneType' object has no attribute '_Storage#0' 为了避免在如此明显的地方暴露我们的实现细节，我们可以修改异常的错误消息，或者，内省描述符实例： 12345def __get__(self, instance, owner): if instance is None: return self else: return getattr(instance, self.name) 两类描述符 上述例子中对数据属性的控制和管理是描述符的典型用途之一。这种实现了 __set__ 方法，接管了设置属性行为的描述符，被称为覆盖型描述符，没有定义 __set__ 方法的描述符，被称为非覆盖型描述符。由于 Python 中对实例属性和类属性的处理方式不同，这两类描述符也有不同的行为。 覆盖型描述符 实现了 __set__ 方法的描述符就是覆盖型描述符。这类描述符虽然是类属性，但是会覆盖实例属性的赋值操作： 123456789101112class Override: def __get__(self, instance, owner): print('get!') def __set__(self, instance, value): print('set!')class Manager: override = Override() 下面做一些实验： 123456789101112131415161718In [1]: m = Manager()In [2]: m.overrideget!In [3]: m.override = 1set!In [4]: Manager.overrideget!In [5]: m.__dict__['override'] = 1In [6]: m.__dict__Out[6]: &#123;'override': 1&#125;In [7]: m.overrideget! 可以看出，无论以实例属性还是类属性访问 override，都会触发 __get__ 方法；为实例属性 override 赋值会触发 __set__ 方法；即使跳过描述符直接为 m.__dict__ 赋值，读取 override 的操作仍然会被描述符覆盖。 没有 __get__ 方法的覆盖型描述符 如果只实现了 __set__ 会发生什么呢？ 123456789class OverrideNoGet: def __set__(self, instance, value): print('set!')class Manager: override_no_get = OverrideNoGet() 123456789101112131415161718192021222324In [1]: m = Manager()In [2]: m.override_no_getOut[2]: &lt;__main__.OverrideNoGet at 0x29c44a97668&gt;In [3]: Manager.override_no_getOut[3]: &lt;__main__.OverrideNoGet at 0x29c44a97668&gt;In [4]: m.override_no_get = 1set!In [5]: m.override_no_getOut[5]: &lt;__main__.OverrideNoGet at 0x29c44a97668&gt;In [6]: m.__dict__['override_no_get'] = 1In [7]: m.override_no_getOut[7]: 1In [8]: m.override_no_get = 2set!In [9]: m.override_no_getOut[9]: 1 可以看到，没实现 __get__ 方法，无论以实例属性还是类属性访问 override_no_get，都会返回描述符实例；而赋值操作可以触发 __set__ 方法；由于我们的 __set__ 方法并没有真正修改实例属性，所以再次访问 override_no_get 仍然会得到描述符实例；通过 m.__dict__ 修改实例属性后，实例属性就会覆盖描述符；不过只有访问实例属性时才是如此，赋值仍然由 __set__ 处理。 非覆盖型描述符 没有实现 __set__ 方法的描述符就是非覆盖型描述符： 123456789class NonOverride: def __get__(self, instance, owner): print('get!')class Manager: non_override = NonOverride() 1234567891011121314151617181920In [1]: m = Manager()In [2]: m.non_overrideget!In [3]: Manager.non_overrideget!In [4]: m.non_override = 1In [5]: m.non_overrideOut[5]: 1In [6]: Manager.non_overrideget!In [7]: del m.non_overrideIn [8]: m.non_overrideget! 无论访问实例属性还是类属性，都会触发 __get__ 方法；由于没有 __set__ 方法，对属性的赋值不会被干涉；对属性复制之后，实例属性就会覆盖同名的描述符，但是访问类属性仍然可以触发 __get__ 方法；如果把 non_override 从实例中删除，访问 non_override 的操作又会交给 __get__。 当然，描述符都是定义在类上的，如果对同名的类属性进行赋值，就会完全替换掉描述符。这里表现出读、写属性时的不对等：对类属性的读操作可以被 __get__ 处理，但是写操作不会。当然，了解一些 Python 的话就会知道还存在着另一种不对等：读取实例属性时，会返回实例属性，如果实例属性不存在，会返回类属性；但是为实例属性赋值时，如果实例属性不存在，会在实例中创建属性，不会影响到类属性。 结语 描述符充斥在 Python 底层（举个例子：Python 中的方法是怎么实现的？）与各种框架中，理解描述符是体会 Python 世界工作原理和设计美学的重要方式。","categories":[],"tags":[]},{"title":"TensorFlow 初体验 （Fashion-mnist）","slug":"2018-03-19/chapter-02-AIR","date":"2018-03-25T13:18:37.000Z","updated":"2018-04-02T14:12:49.205Z","comments":true,"path":"posts/b0821049/","link":"","permalink":"http://weafteam.github.io/posts/b0821049/","excerpt":"","text":"TensorFlow 初体验（Fashion-mnist） 接着上一讲的内容，想必大家已经通过我的教程安装好了TensorFlow了吧，那我们这节课通过安装简单的跨平台的集成开发环境Spyder，在这个集成开发环境上面实现一些python程序。具体安装过程见如下阐述： 首先在应用程序里面找到Anaconda应用程序，打开里面的Anaconda Navigator，然后打开以后，选中我们上次建立好的环境tensorflow。 选中tensorflow这个环境变量以后，看到里面有一个集成开发环境叫spyder，这个工具就是今天我们要安装的，我的已经安装好了，所以是Launch，你们的没有安装好，所以是install状态，点解安装就好。（这个地方也可能需要翻墙）。 这个安装好以后，你就会在应用文件夹里面出现一个Spyder(tensorflow)这个应用程序，以后你就从应用文件夹启动就好。 那么启动以后：我也是启动了，出现了以下的情况：不慌，慢慢来。 看到上面的错误，这个错误提示是因为没有安装jedi这个依赖库，而且要求版本要大于0.9.0。那我们接下来解决一下这个问题。 小插曲，一下就可以解决，具体操作步骤: 还是打开上次那个AnacondaPrompt的命令行 进去以后，执行activate tensorflow 相当于你要在这个环境下面给这个spyder安装这个依赖 进去以后，执行pip install jedi==0.9.0 就可以了，然后重启spyder（可以直接在这个环境里面输入spyder命令就可以实现spyder的启动，你也可以在应用文件夹里面启动，性质是一样的） 不出什么意外的话，spyder使用就没有问题了，有什么问题可以发邮件给我！！！ 解决了上面的小插曲以后，我们在spyder中输入以下代码进行测试。 123456789101112import tensorflow as tfsess = tf.Session()init = tf.global_variables_initializer() # 此处的init是全局变量初始化器，# TensorFlow的session必须执行这个初始化器才能执行前面建立好的图，# 所以，这个是很重要的一点，后续也会强调#（也就是后续再网络中建立变量就是通过那个初始化器来进行初始化工作的）# 其实在没有变量的时候，这个初始化器是不需要的# 但是为了让大家形成习惯，还是写上sess.run(init)hello = tf.constant('hello world')print(sess.run(hello)) 上图中左面是代码书写区域，右面上半部分是变量查看区域，还有文件夹区域可以切换，右面下半部分是执行console区域，我输入上面的代码，执行以后console区域打出hello world字符串。 从上面的一些简单的测试以后，我们进入今天的主题，fashion-minist的识别，fashion-minist是一个服装识别的一个数据集，在这个数据集之前有一个mnist手写体识别数据集，这个手写数据集对应我们手写的十个数字，然后通过设计网络来识别手写体。但是今天我们不做手写体识别，直接来做fashion-minist识别。 闲话少说，上代码，边写边说。 首先目标是实现衣服种类的识别。 数据可以在 Zalando_Fashion_MNIST_repository这个Github仓库获取。 数据分为60000训练数据和10000测试数据，图片都是灰度图片，大小为28 X 28，总共也是由10类组成。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305# -*- coding: utf-8 -*-\"\"\"Created on Sun Mar 25 15:16:23 2018@author: milittle\"\"\"# 导入一些必要的库import numpy as np # 数学计算库import matplotlib.pyplot as plt # 画图的一个库import tensorflow as tf # TensorFlow的库from tensorflow.examples.tutorials.mnist import input_datafashion_mnist = input_data.read_data_sets('input/data', one_hot = True)# 定义一个服装对应表label_dict = &#123; 0: 'T-shirt/top', 1: 'Trouser', 2: 'Pullover', 3: 'Dress', 4: 'Coat', 5: 'Sandal', 6: 'Shirt', 7: 'Sneaker', 8: 'Bag', 9: 'Ankle boot'&#125;# 获取随机的数据和它的labelsample_1 = fashion_mnist.train.images[47].reshape(28,28)sample_label_1 = np.where(fashion_mnist.train.labels[47] == 1)[0][0]sample_2 = fashion_mnist.train.images[23].reshape(28,28)sample_label_2 = np.where(fashion_mnist.train.labels[23] == 1)[0][0]# 用matplot画出这个image和labelprint(\"y = &#123;label_index&#125; (&#123;label&#125;)\".format(label_index=sample_label_1, label=label_dict[sample_label_1]))plt.imshow(sample_1, cmap='Greys')plt.show()print(\"y = &#123;label_index&#125; (&#123;label&#125;)\".format(label_index=sample_label_2, label=label_dict[sample_label_2]))plt.imshow(sample_2, cmap='Greys')plt.show()# 接下来就是设计网络参数n_hidden_1 = 128 # 第一个隐藏层的单元个数n_hidden_2 = 128 # 第二个隐藏层的单元个数n_input = 784 # fashion mnist输入图片的维度（单元个数） (图片大小: 28*28)n_classes = 10 # fashion mnist的种类数目 (0-9 数字)# 创建 placeholdersdef create_placeholders(n_x, n_y): \"\"\" 为sess创建一个占位对象。 参数: n_x -- 向量, 图片大小 (28*28 = 784) n_y -- 向量, 种类数目 (从 0 到 9, 所以是 -&gt; 10种) 返回参数: X -- 为输入图片大小的placeholder shape是[784, None] Y -- 为输出种类大小的placeholder shape是[10, None] None在这里表示以后输入的数据可以任意多少 \"\"\" X = tf.placeholder(tf.float32, [n_x, None], name=\"X\") Y = tf.placeholder(tf.float32, [n_y, None], name=\"Y\") return X, Y# 测试上面的create_placeholders()X, Y = create_placeholders(n_input, n_classes)print(\"Shape of X: &#123;shape&#125;\".format(shape=X.shape))print(\"Shape of Y: &#123;shape&#125;\".format(shape=Y.shape))# 定义初始化参数参数def initialize_parameters(): \"\"\" 参数初始化，下面是每个参数的shape，总共有三层 W1 : [n_hidden_1, n_input] b1 : [n_hidden_1, 1] W2 : [n_hidden_2, n_hidden_1] b2 : [n_hidden_2, 1] W3 : [n_classes, n_hidden_2] b3 : [n_classes, 1] 返回: 包含所有权重和偏置项的dic \"\"\" # 设置随机数种子 tf.set_random_seed(42) # 为每一层的权重和偏置项进行初始化工作 W1 = tf.get_variable(\"W1\", [n_hidden_1, n_input], initializer = tf.contrib.layers.xavier_initializer(seed = 42)) b1 = tf.get_variable(\"b1\", [n_hidden_1, 1], initializer = tf.zeros_initializer()) W2 = tf.get_variable(\"W2\", [n_hidden_2, n_hidden_1], initializer = tf.contrib.layers.xavier_initializer(seed = 42)) b2 = tf.get_variable(\"b2\", [n_hidden_2, 1], initializer = tf.zeros_initializer()) W3 = tf.get_variable(\"W3\", [n_classes, n_hidden_2], initializer=tf.contrib.layers.xavier_initializer(seed = 42)) b3 = tf.get_variable(\"b3\", [n_classes, 1], initializer = tf.zeros_initializer()) # 将参数存储在一个dict对象里面返回去 parameters = &#123; \"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, \"b3\": b3 &#125; return parameters# 测试初始化参数tf.reset_default_graph()with tf.Session() as sess: parameters = initialize_parameters() print(\"W1 = &#123;w1&#125;\".format(w1=parameters[\"W1\"])) print(\"b1 = &#123;b1&#125;\".format(b1=parameters[\"b1\"])) print(\"W2 = &#123;w2&#125;\".format(w2=parameters[\"W2\"])) print(\"b2 = &#123;b2&#125;\".format(b2=parameters[\"b2\"])) # 前向传播算法（就是神经网络的前向步骤）def forward_propagation(X, parameters): \"\"\" 实现前向传播的模型 LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SOFTMAX 上面的显示就是三个线性层，每一层结束以后，实现relu的作用，实现非线性功能，最后三层以后用softmax实现分类 参数: X -- 输入训练数据的个数[784, n] 这里的n代表可以一次训练多个数据 parameters -- 包括上面所有的定义参数三个网络中的权重W和偏置项B 返回: Z3 -- 最后的一个线性单元输出 \"\"\" # 从参数dict里面取到所有的参数 W1 = parameters['W1'] b1 = parameters['b1'] W2 = parameters['W2'] b2 = parameters['b2'] W3 = parameters['W3'] b3 = parameters['b3'] # 前向传播过程 Z1 = tf.add(tf.matmul(W1,X), b1) # Z1 = np.dot(W1, X) + b1 A1 = tf.nn.relu(Z1) # A1 = relu(Z1) Z2 = tf.add(tf.matmul(W2,A1), b2) # Z2 = np.dot(W2, a1) + b2 A2 = tf.nn.relu(Z2) # A2 = relu(Z2) Z3 = tf.add(tf.matmul(W3,A2), b3) # Z3 = np.dot(W3,Z2) + b3 return Z3# 测试前向传播喊出tf.reset_default_graph()with tf.Session() as sess: X, Y = create_placeholders(n_input, n_classes) parameters = initialize_parameters() Z3 = forward_propagation(X, parameters) print(\"Z3 = &#123;final_Z&#125;\".format(final_Z=Z3))# 定义计算损失函数# 是计算loss的时候了def compute_cost(Z3, Y): \"\"\" 计算cost 参数: Z3 -- 前向传播的最终输出（[10, n]）n也是你输入的训练数据个数 Y -- 返回: cost - 损失函数 张量（Tensor） \"\"\" # 获得预测和准确的label logits = tf.transpose(Z3) labels = tf.transpose(Y) # 计算损失 cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels)) return cost# 测试计算损失函数tf.reset_default_graph()with tf.Session() as sess: X, Y = create_placeholders(n_input, n_classes) parameters = initialize_parameters() Z3 = forward_propagation(X, parameters) cost = compute_cost(Z3, Y) print(\"cost = &#123;cost&#125;\".format(cost=cost))# 这个就是关键了，因为每一层的参数都是通过反向传播来实现权重和偏置项参数更新的# 总体的原理就是经过前向传播，计算到最后的层，利用softmax加交叉熵，算出网络的损失函数# 然后对损失函数进行求偏导，利用反向传播算法实现每一层的权重和偏置项的更新def model(train, test, learning_rate=0.0001, num_epochs=16, minibatch_size=32, print_cost=True, graph_filename='costs'): \"\"\" 实现了一个三层的网络结构: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SOFTMAX. 参数: train -- 训练集 test -- 测试集 learning_rate -- 优化权重时候所用到的学习率 num_epochs -- 训练网络的轮次 minibatch_size -- 每一次送进网络训练的数据个数（也就是其他函数里面那个n参数） print_cost -- 每一轮结束以后的损失函数 返回: parameters -- 被用来学习的参数 \"\"\" # 确保参数不被覆盖重写 tf.reset_default_graph() tf.set_random_seed(42) seed = 42 # 获取输入和输出大小 (n_x, m) = train.images.T.shape n_y = train.labels.T.shape[0] costs = [] # 创建输入输出数据的占位符 X, Y = create_placeholders(n_x, n_y) # 初始化参数 parameters = initialize_parameters() # 进行前向传播 Z3 = forward_propagation(X, parameters) # 计算损失函数 cost = compute_cost(Z3, Y) # 使用AdamOptimizer优化器实现反向传播算法（最小化cost） # 其实我们这个地方的反向更新参数的过程都是tensorflow给做了 optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost) # 变量初始化器 init = tf.global_variables_initializer() # 开始tensorflow的sess 来计算tensorflow构建好的图 with tf.Session() as sess: # 这个就是之前说过的要进行初始化的 sess.run(init) # 训练轮次 for epoch in range(num_epochs): epoch_cost = 0. num_minibatches = int(m / minibatch_size) seed = seed + 1 for i in range(num_minibatches): # 获取下一个batch的训练数据和label数据 minibatch_X, minibatch_Y = train.next_batch(minibatch_size) # 执行优化器 _, minibatch_cost = sess.run([optimizer, cost], feed_dict=&#123;X: minibatch_X.T, Y: minibatch_Y.T&#125;) # 更新每一轮的损失 epoch_cost += minibatch_cost / num_minibatches # 打印每一轮的损失 if print_cost == True: print(\"Cost after epoch &#123;epoch_num&#125;: &#123;cost&#125;\".format(epoch_num=epoch, cost=epoch_cost)) costs.append(epoch_cost) # 使用matplot画出损失的变化曲线图 plt.figure(figsize=(16,5)) plt.plot(np.squeeze(costs), color='#2A688B') plt.xlim(0, num_epochs-1) plt.ylabel(\"cost\") plt.xlabel(\"iterations\") plt.title(\"learning rate = &#123;rate&#125;\".format(rate=learning_rate)) plt.savefig(graph_filename, dpi = 300) plt.show() # 保存参数 parameters = sess.run(parameters) print(\"Parameters have been trained!\") # 计算预测准率 correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y)) # 计算测试准率 accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) print (\"Train Accuracy:\", accuracy.eval(&#123;X: train.images.T, Y: train.labels.T&#125;)) print (\"Test Accuracy:\", accuracy.eval(&#123;X: test.images.T, Y: test.labels.T&#125;)) return parameters# 要开始训练我们的fashion mnist网络了train = fashion_mnist.train # 训练的数据test = fashion_mnist.test # 测试的数据parameters = model(train, test, learning_rate = 0.001, num_epochs = 16, graph_filename = 'fashion_mnist_costs') 上面的代码是写好了，这里有一个python的依赖库（matplotlib）需要安装以下，同样的办法，就是进去tensorflow这个环境里面，然后执行pip install matplotlib就可以了。 在这个过程中，可能从tensorflow下载数据的时候会很慢。（我们选择直接从上面给出下载数据集的github网址，直接下载以后，将数据拷贝在代码所在文件夹的input/data/文件夹里面，总共由四个文件组成）分别是训练数据图片、训练数据label和测试数据图片、测试数据label。这样就可以省去下载数据时候漫长的等待。 上面就是我们使用TensorFlow实现的fashion-mnist的识别，总体根据实验结果来说，从测试集的数据来看，我达到的准确率结果是88.5%，还算可以。后续我们可能使用其他一些现有的网络结构来实现fashion-mnist的识别，看看准确率会不会提高。 如下是我对上面TensorFLow出现的方法介绍： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748tf.placeholders( dtype, shape=None, name=None)从参数上面看到，总共有三个参数： dtype：在tensor中被喂数据的元素类型 shape: tensor的shape name：命名说明一下，这个函数返回的是一个tensor，在TensorFlow里面，tensor是一个很重要的概念，大家务必掌握，也叫张量，比如我们的一个数:就是0-阶张量，也叫标量。一个向量，就是1-阶张量。一个矩阵，就是2-阶张量，后面的就是一直往高维了走，对应的就是多少阶张量。这个方法，很重要的原因也在于它是定义在Session执行run的时候，在后面填充数据的占位符，也就是feed_dict这个变量里面的数据，所以大家，务必记住这一关键的概念。后续用起来就会很顺手。tf.get_variable()这个方法后续在展开来说，你先理解就是使用它可以定义变量（保存权重和偏置项的），还可以加一些优化器，比如说正则优化器等等tf.matmul( a, b,)展示给你们列出这两个参数： a：就是待操作的矩阵1 b: 就是待操作的矩阵2函数功能就是实现矩阵的相乘运算（当然要符合基本的矩阵运算格式）tf.transpose( a,)先列出来一个参数，就是矩阵的转置Session().run( fetches, feed_list=None, )这个方法就是运行图。很关键，先掌握两个参数: fetches: 你要从图里面取出的数据（） feed_list: 你要给图喂的数据（输入和label数据就是用这样的方式来做的） 比如我们训练的网络中输入的图片信息和对应的label信息tf.reduce_mean( input_tensor, axis=None, keepdims=None, name=None, redcution_indices=None, keep_dims=None )计算输入tensor的总和： input_tensor: 要叠加的tensor axis: 选择那个维度叠加 keepdims: 叠加元素以后，保留原来的维度信息 name：就是名字 redcution_indices：被axis取代 keep_dims：被keepdims取代 我们今天的任务量可能有一些大，大家坚持。总的来说就是使用神经网络对实际的一个fashion-mnist数据集进行服装种类的识别，大家主要看看我的代码。有什么不明白的我在代码里面都做出了注释。 邮箱——air@weaf.top欢迎来探讨","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]},{"title":"rsync的使用与配置","slug":"2018-03-19/rsync-configuration-and-use","date":"2018-03-25T13:08:56.000Z","updated":"2018-04-02T14:12:49.206Z","comments":true,"path":"posts/cfc65600/","link":"","permalink":"http://weafteam.github.io/posts/cfc65600/","excerpt":"一、什么是rsync rsync，remote synchronize顾名思意就知道它是一款实现远程同步功能的软件，它在同步文件的同时，可以保持原来文件的权限、时间、软硬链接等附加信息。 rsync是用 “rsync 算法”提供了一个客户机和远程文件服务器的文件同步的快速方法，而且可以通过ssh方式来传输文件，这样其保密性也非常好，另外它还是免费的软件。 二、rsync的安装 rysnc的官方网站：http://rsync.samba.org 可以从上面得到最新的版本。目前最新版是3.1.2。当然，因为rsync是一款如此有用的软件，所以很多Linux的发行版本都将它收录在内了。","text":"一、什么是rsync rsync，remote synchronize顾名思意就知道它是一款实现远程同步功能的软件，它在同步文件的同时，可以保持原来文件的权限、时间、软硬链接等附加信息。 rsync是用 “rsync 算法”提供了一个客户机和远程文件服务器的文件同步的快速方法，而且可以通过ssh方式来传输文件，这样其保密性也非常好，另外它还是免费的软件。 二、rsync的安装 rysnc的官方网站：http://rsync.samba.org 可以从上面得到最新的版本。目前最新版是3.1.2。当然，因为rsync是一款如此有用的软件，所以很多Linux的发行版本都将它收录在内了。 软件包安装 命令 平台 # sudo apt-get install rsync 注：在debian、ubuntu 等在线安装方法； # yum install rsync 注：Fedora、Redhat 等在线安装方法； # rpm -ivh rsync 注：Fedora、Redhat 等rpm包安装方法； 其它Linux发行版，请用相应的软件包管理方法来安装。 源码包安装 123 tar xvf rsync-xxx.tar.gz cd rsync-xxx ./configure --prefix=/usr ;make ;make install 注：在用源码 包编译安装之前，您得安装gcc等编译开具才行； 三、rsync的配置 ———– rsync的主要有以下三个配置文件rsyncd.conf(主配置文件)、rsyncd.secrets(密码文件)、rsyncd.motd(rysnc服务器信息) 比如我们要备份服务器上的/home和/opt，在/home中我想把easylife和samba目录排除在外； 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849 # Distributed under the terms of the GNU General Public License v2 # Minimal configuration file for rsync daemon # See rsync(1) and rsyncd.conf(5) man pages for help # This line is required by the /etc/init.d/rsyncd script pid file = /var/run/rsyncd.pid port = 873 address = 192.168.1.171 #uid = nobody #gid = nobody uid = root gid = root use chroot = yes read only = yes #limit access to private LANs hosts allow=192.168.1.0/255.255.255.0 10.0.1.0/255.255.255.0 hosts deny=* max connections = 5 motd file = /etc/rsyncd.motd #This will give you a separate log file #log file = /var/log/rsync.log #This will log every file transferred - up to 85,000+ per user, per sync #transfer logging = yes log format = %t %a %m %f %b syslog facility = local3 timeout = 300 [rhel4home] path = /home list=yes ignore errors auth users = root secrets file = /etc/rsyncd.secrets comment = This is RHEL 4 data exclude = easylife/ samba/ [rhel4opt] path = /opt list=no ignore errors comment = This is RHEL 4 opt auth users = easylife secrets file = /etc/rsyncd/rsyncd.secrets 注：关于auth users是必须在服务器上存在的真实的系统用户，如果你想用多个用户以,号隔开，比如auth users = easylife,root 设定密码文件 密码文件格式很简单，rsyncd.secrets的内容格式为： 用户名:密码 我们在例子中rsyncd.secrets的内容如下类似的；在文档中说，有些系统不支持长密码，自己尝试着设置一下吧。 12 easylife:keer root:mike 12 chown root.root rsyncd.secrets #修改属主 chmod 600 rsyncd.secrets #修改权限 注：1、将rsyncd.secrets这个密码文件的文件属性设为root拥有, 且权限要设为600, 否则无法备份成功! 出于安全目的，文件的属性必需是只有属主可读。 2、这里的密码值得注意，为了安全你不能把系统用户的密码写在这里。比如你的系统用户easylife密码是000000，为了安全你可以让rsync中的easylife为keer。这和samba的用户认证的密码原理是差不多的。 设定rsyncd.motd 文件; 它是定义rysnc服务器信息的，也就是用户登录信息。比如让用户知道这个服务器是谁提供的等；类似ftp服务器登录时，我们所看到的 linuxsir.org ftp ……。 当然这在全局定义变量时，并不是必须的，你可以用#号注掉，或删除；我在这里写了一个 rsyncd.motd的内容为： 1234 ++++++++++++++++++++++++++++++++++++++++++++++ Welcome to use the mike.org.cn rsync services!2002------2009 ++++++++++++++++++++++++++++++++++++++++++++++ 四、启动rsync服务器 相当简单，有以下几种方法 A、–daemon参数方式，是让rsync以服务器模式运行 1 #/usr/bin/rsync --daemon --config=/etc/rsyncd/rsyncd.conf #--config用于指定rsyncd.conf的位置,如果在/etc下可以不写 B、xinetd方式 12345 修改services加入如下内容 # nano -w /etc/services rsync 873/tcp # rsync rsync 873/udp # rsync 这一步一般可以不做，通常都有这两行(我的RHEL4和GENTOO默认都有)。修改的目的是让系统知道873端口对应的服务名为rsync。如没有的话就自行加入。 设定 /etc/xinetd.d/rsync, 简单例子如下: 12345678910111213 # default: off # description: The rsync server is a good addition to am ftp server, as it \\ # allows crc checksumming etc. service rsync &#123;disable = nosocket_type = streamwait = nouser = rootserver = /usr/bin/rsyncserver_args = --daemonlog_on_failure += USERID &#125; 上述, 主要是要打开rsync這個daemon, 一旦有rsync client要连接時, xinetd会把它转介給 rsyncd(port 873)。然后service xinetd restart, 使上述设定生效. rsync服务器和防火墙 Linux 防火墙是用iptables，所以我们至少在服务器端要让你所定义的rsync 服务器端口通过，客户端上也应该让通过。 12 #iptables -A INPUT -p tcp -m state --state NEW -m tcp --dport 873 -j ACCEPT #iptables -L 查看一下防火墙是不是打开了 873端口 如果你不太懂防火墙的配置，可以先service iptables stop 将防火墙关掉。当然在生产环境这是很危险的，做实验才可以这么做哟！ 五、通过rsync客户端来同步数据 B1、列出rsync 服务器上的所提供的同步内容； 首先：我们看看rsync服务器上提供了哪些可用的数据源 # rsync –list-only root@192.168.145.5:: ++++++++++++++++++++++++++++++++++++++++++++++ Welcome to use the mike.org.cn rsync services! 2002——2009 ++++++++++++++++++++++++++++++++++++++++++++++ rhel4home This is RHEL 4 data 注：前面是rsync所提供的数据源，也就是我们在rsyncd.conf中所写的[rhel4home]模块。而“This is RHEL 4 data”是由[rhel4home]模块中的 comment = This is RHEL 4 data 提供的；为什么没有把rhel4opt数据源列出来呢？因为我们在[rhel4opt]中已经把list=no了。 $ rsync –list-only root@192.168.145.5::rhel4home ++++++++++++++++++++++++++++++++++++++++++++++ Welcome to use the mike.org.cn rsync services! 2002——2009 ++++++++++++++++++++++++++++++++++++++++++++++ Password: drwxr-xr-x 4096 2009/03/15 21:33:13 . -rw-r–r– 1018 2009/03/02 02:33:41 ks.cfg -rwxr-xr-x 21288 2009/03/15 21:33:13 wgetpaste drwxrwxr-x 4096 2008/10/28 21:04:05 cvsroot drwx—— 4096 2008/11/30 16:30:58 easylife drwsr-sr-x 4096 2008/09/20 22:18:05 giddir drwx—— 4096 2008/09/29 14:18:46 quser1 drwx—— 4096 2008/09/27 14:38:12 quser2 drwx—— 4096 2008/11/14 06:10:19 test drwx—— 4096 2008/09/22 16:50:37 vbird1 drwx—— 4096 2008/09/19 15:28:45 vbird2 后面的root@ip中，root是指定密码文件中的用户名，之后的::rhel4home这是rhel4home模块名 ### B2、rsync客户端同步数据； #rsync -avzP root@192.168.145.5::rhel4home rhel4home Password: 这里要输入root的密码，是服务器端rsyncd.secrets提供的。在前面的例子中我们用的是mike，输入的密码并不回显，输好就回车。 注： 这个命令的意思就是说，用root用户登录到服务器上，把rhel4home数据，同步到本地当前目录rhel4home上。当然本地的目录是可以你自己 定义的。如果当你在客户端上当前操作的目录下没有rhel4home这个目录时，系统会自动为你创建一个；当存在rhel4home这个目录中，你要注意 它的写权限。 1 #rsync -avzP --delete linuxsir@linuxsir.org::rhel4home rhel4home 这回我们引入一个–delete 选项，表示客户端上的数据要与服务器端完全一致，如果 linuxsirhome目录中有服务器上不存在的文件，则删除。最终目的是让linuxsirhome目录上的数据完全与服务器上保持一致；用的时候要 小心点，最好不要把已经有重要数所据的目录，当做本地更新目录，否则会把你的数据全部删除； 設定 rsync client 设定密码文件 1 #rsync -avzP --delete --password-file=rsyncd.secrets root@192.168.145.5::rhel4home rhel4home 这次我们加了一个选项 –password-file=rsyncd.secrets，这是当我们以root用户登录rsync服务器同步数据时，密码将读取rsyncd.secrets这个文件。这个文件内容只是root用户的密码。我们要如下做； # touch rsyncd.secrets # chmod 600 rsyncd.secrets # echo “mike”&gt; rsyncd.secrets # rsync -avzP –delete –password-file=rsyncd.secrets root@192.168.145.5::rhel4home rhel4home 注：这里需要注意的是这份密码文件权限属性要设得只有属主可读。 这样就不需要密码了；其实这是比较重要的，因为服务器通过crond 计划任务还是有必要的； ### B3、让rsync客户端自动与服务器同步数据 服务器是重量级应用，所以数据的网络备份还是极为重要的。我们可以在生产型服务器上配置好rsync 服务器。我们可以把一台装有rysnc机器当做是备份服务器。让这台备份服务器，每天在早上4点开始同步服务器上的数据；并且每个备份都是完整备份。有时 硬盘坏掉，或者服务器数据被删除，完整备份还是相当重要的。这种备份相当于每天为服务器的数据做一个镜像，当生产型服务器发生事故时，我们可以轻松恢复数 据，能把数据损失降到最低；是不是这么回事？？ step1：创建同步脚本和密码文件 12345678 #mkdir /etc/cron.daily.rsync #cd /etc/cron.daily.rsync #touch rhel4home.sh rhel4opt.sh #chmod 755 /etc/cron.daily.rsync/*.sh #mkdir /etc/rsyncd/ #touch /etc/rsyncd/rsyncrhel4root.secrets #touch /etc/rsyncd/rsyncrhel4easylife.secrets #chmod 600 /etc/rsyncd/rsync.* 注： 我们在 /etc/cron.daily/中创建了两个文件rhel4home.sh和rhel4opt.sh ，并且是权限是755的。创建了两个密码文件root用户用的是rsyncrhel4root.secrets ，easylife用户用的是 rsyncrhel4easylife.secrets，权限是600； 我们编辑rhel4home.sh，内容是如下的： 123 #!/bin/sh #backup 192.168.145.5:/home /usr/bin/rsync -avzP --password-file=/etc/rsyncd/rsyncrhel4root.secrets root@192.168.145.5::rhel4home /home/rhel4homebak/$(date +&apos;%m-%d-%y&apos;) 我们编辑 rhel4opt.sh ，内容是： 123 #!/bin/sh #backup 192.168.145.5:/opt /usr/bin/rsync -avzP --password-file=/etc/rsyncd/rsyncrhel4easylife.secrets easylife@192.168.145.5::rhel4opt /home/rhel4hoptbak/$(date +&apos;%m-%d-%y&apos;) 注：你可以把rhel4home.sh和rhel4opt.sh的内容合并到一个文件中，比如都写到rhel4bak.sh中； 接着我们修改 /etc/rsyncd/rsyncrhel4root.secrets和rsyncrhel4easylife.secrets的内容； 12 # echo &quot;mike&quot; &gt; /etc/rsyncd/rsyncrhel4root.secrets # echo &quot;keer&quot;&gt; /etc/rsyncd/rsyncrhel4easylife.secrets 然后我们再/home目录下创建rhel4homebak 和rhel4optbak两个目录，意思是服务器端的rhel4home数据同步到备份服务器上的/home/rhel4homebak 下，rhel4opt数据同步到 /home/rhel4optbak/目录下。并按年月日归档创建目录；每天备份都存档； 12 #mkdir /home/rhel4homebak #mkdir /home/rhel4optbak step2：修改crond服务器的配置文件 加入到计划任务 1 #crontab -e 加入下面的内容： # Run daily cron jobs at 4:10 every day backup rhel4 data: 10 4 * * * /usr/bin/run-parts /etc/cron.daily.rsync 1&gt; /dev/null 注：第一行是注释，是说明内容，这样能自己记住。 第二行表示在每天早上4点10分的时候，运行 /etc/cron.daily.rsync 下的可执行脚本任务； 配置好后，要重启crond 服务器； 123456 # killall crond 注：杀死crond 服务器的进程； # ps aux |grep crond 注：查看一下是否被杀死； # /usr/sbin/crond 注：启动 crond 服务器； # ps aux |grep crond 注：查看一下是否启动了？ root 3815 0.0 0.0 1860 664 ? S 14:44 0:00 /usr/sbin/crond root 3819 0.0 0.0 2188 808 pts/1 S+ 14:45 0:00 grep crond","categories":[{"name":"Linux","slug":"Linux","permalink":"http://weafteam.github.io/categories/Linux/"}],"tags":[{"name":"Linux运维","slug":"Linux运维","permalink":"http://weafteam.github.io/tags/Linux运维/"}]},{"title":"文本聚类系列教程：（二）jieba中文分词工具进阶","slug":"2018-03-19/文本聚类系列教程：（二）jieba中文分词工具进阶","date":"2018-03-19T11:57:19.000Z","updated":"2018-04-02T14:12:49.206Z","comments":true,"path":"posts/931939a5/","link":"","permalink":"http://weafteam.github.io/posts/931939a5/","excerpt":"","text":"jieba中文分词工具使用进阶篇，废话不多说吗，我们开始本次的学习吧~ 如何让分词的更加准确 我们之前举得例子有些文本其实很简单，我们后来确实换了官方的测试文本《围城》，但是均没避免一个问题，这些测试例都十分地中规中矩。在实际中需要我们做分词的文本可能是多种多样的，这时候的切词有可能会不太特别理想，导致分词的不准确。 那我们不妨下一个别的电子书（这里我下载的是《斗破苍穹》，为了测试我只用了第一章的文本），然后再进行切词，看下是否存在这样的问题。这里我们稍微改改上次的去停用词的代码，代码如下： 1234567891011121314151617181920212223import sysimport jiebafrom os import pathd = path.dirname(__file__) # 获取当前文件的dir路径text_path = 'txt/chapter2.txt' #《斗破苍穹》第一章的文本路径text = open(path.join(d, text_path),'rb').read()def CutWords(text): mywordlist = [] seg_list = jieba.cut(text, cut_all=False) liststr=\"/ \".join(seg_list) # 添加切分符 for myword in liststr.split('/'): if len(myword.strip())&gt;1: mywordlist.append(myword) return ''.join(mywordlist) #返回一个字符串txt5 = CutWords(text)text_write = 'txt/5.txt'with open(text_write,'w') as f: f.write(txt5) print(\"Success\") 结果如下： 终于被我们找到了一个切词错误，原文是这样的： 萧媚脑中忽然浮现出三年前那意气风发的少年 按照我们正常的断句，应为： 萧媚/脑中/忽然/浮现….，而jieba却认为“萧媚脑”是一个单词，从而导致此处分词不理想。 jieba考虑了这种情况，而且有很多的应对方案，下面我们先说最简单的。 调整词典 方法1：动态修改词典 使用add_word(word,freq=None,tag=None)和del_word(word)可在程序中动态的修改词典，具体操作如下： 12345678910import sysimport jiebafrom os import pathd = path.dirname(__file__) # 获取当前文件的dir路径# 此处增加代码jieba.add_word('脑中') ···· 结果如下： 果然，这样的方法很直接的把我们原来切错的词变成了正确的词。与add_word()相对应的是delete_word()方法，根据字面意思我们也很容易理解delete_word()方法的作用，这里我就不做过多的演示了，大家在实际场景中直接运用就好了。 方法2：调节词频 使用suggest_freq(segment, tune=True)调节单个词语的词频，使得它更容易被分出来，或者不被分出来。 但是需要注意的是：自动计算的词频在使用 HMM 新词发现功能时可能无效。 所以此时我们在做切词的时候需要把是HMM置为False。我们看下官方给的Demo（如果关闭HMM，很多新发现的词都消失了，所以‘萧媚脑’也消失了，无法做测试，我们的例子也是为了方便大家理解，所以也没必要非得针对这一个词做词频调节），具体的做法如下： 12345678910111213import jiebaprint('/'.join(jieba.cut('如果放到post中将出错。', HMM=False)))jieba.suggest_freq(('中', '将'), True)print('/'.join(jieba.cut('如果放到post中将出错。', HMM=False)))print('/'.join(jieba.cut('「台中」正确应该不会被切开', HMM=False)))jieba.suggest_freq('台中', True)print('/'.join(jieba.cut('「台中」正确应该不会被切开', HMM=False))) 结果： 对比下结果，不难发现suggest_freq()的使用方法，通过这样的强调高频词和低频词的方法可以做到分词更准确。 添加自定义词典 比起默认的词典，我们自定义的词典更适合我们自己的文本，这一点是毋庸置疑的。 词典格式和 dict.txt 一样，一个词占一行；每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。file_name 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码。 这里我们的词典为： 12345678910云计算 5李小福 2 nr创新办 3 ieasy_install 3 eng好用 300韩玉赏鉴 3 nz八一双鹿 3 nz台中凱特琳 nzEdu Trust认证 2000 我们这个例子也用官方的Demo，代码如下： 12345678910111213141516171819202122232425262728293031323334import syssys.path.append(\"../\")import jiebajieba.load_userdict(\"userdict.txt\")# jieba在0.28版本之后采用延迟加载方式# “import jieba”不会立即触发词典的加载，而是在有必要的时候才会加载词典# 如果想手动加载，可执行代码： jieba.initialize() 进行手动初始化操作# 也正是有了延迟加载机制，我们现在可以改变主词典的路径：# jieba.set_dictionary('data/dict.txt.big')# 官方还提供了占用内存较小的词典和适用于繁体字的词典，均在官方的GitHub上，有需要的可以自行下载。import jieba.posseg as pseg# pseg切分可以显示词性# 以下三个操作是修改词典的巩固jieba.add_word('石墨烯')jieba.add_word('凱特琳')jieba.del_word('自定义词')test_sent = (\"李小福是创新办主任也是云计算方面的专家; 什么是八一双鹿\\n\"\"例如我输入一个带“韩玉赏鉴”的标题，在自定义词库中也增加了此词为N类\\n\"\"「台中」正確應該不會被切開。mac上可分出「石墨烯」；此時又可以分出來凱特琳了。\")words = jieba.cut(test_sent)print('/'.join(words))print(\"=\"*40)result = pseg.cut(test_sent)for w in result: print(w.word, \"/\", w.flag, \", \", end=' ') 结果如下： 像‘云计算’、‘创新办’等词在没加载词典的时候是不能被识别出来的。像‘石墨烯’等在没有add_word()的时候也是不能识别出来的。可见效果还是不错的。 并行分词 原理：将目标文本按行分隔后，把各行文本分配到多个 Python 进程并行分词，然后归并结果，从而获得分词速度的可观提升 但是令人遗憾的是，这个模块并不支持Windows平台，原因是因为jieba的该模块是基于python自带的 multiprocessing 模块，而这个模块并不支持Windows。这里我就贴一下用法，使用Linux系统的同学可以自行体验下这个可观的速度提升。 用法： jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数 jieba.disable_parallel() # 关闭并行分词模式 最后 以上所讲的内容在日常的使用中应该是够用了，当然像基于TextRank算法的关键词抽取等内容，我这里并没涉及，并不是因为不重要，而是我对这个算法还不是很了解，硬着头皮写肯定也是照本宣科，效果肯定很差，所以先挖个坑吧，以后再填。 感谢阅读~","categories":[{"name":"文本聚类","slug":"文本聚类","permalink":"http://weafteam.github.io/categories/文本聚类/"}],"tags":[{"name":"文本聚类","slug":"文本聚类","permalink":"http://weafteam.github.io/tags/文本聚类/"}]},{"title":"普通的 SQLAlchemy ORM 使用姿势","slug":"2018-03-12/usage-of-sqlalchemy","date":"2018-03-18T13:38:54.000Z","updated":"2018-04-02T14:12:49.203Z","comments":true,"path":"posts/39277c31/","link":"","permalink":"http://weafteam.github.io/posts/39277c31/","excerpt":"","text":"前言 SQLAlchemy 是 Python 世界中最常用的 SQL 工具之一，包含 SQL 渲染引擎和 ORM 两大部分，平时使用最多的就是 ORM。在我看来平时很多使用 ORM 的姿势是有问题的，或者说是不优雅的。所以这篇文章打算讲讲（搬运）其中一些普通的姿势和技巧（API 文档）。 property 和混合属性 property 下面是一个简单的用户表映射： 12345class User(Base): __tablename__ = 'user' id = Column(Integer, primary_key=True) name = Column(String(64)) password = Column(String(128)) 通常情况下，我们会加密用户的密码，在数据库中保存密文，但是这里有一个问题，我们得这么写： 1234# 创建用户user = User(name='zhang', password=encrypt('123456'))# 修改密码user.password = encrypt('654321') 这意味着我们需要不断的重复书写 encrypt 函数来保证加密了用户密码。 有没有什么方法能省去这一步呢？答案是 property。 现在把用户表映射改成这样： 12345678910111213class User(Base): __tablename__ = 'user' id = Column(Integer, primary_key=True) name = Column(String(64)) _password = Column(String(128)) @property def password(self): raise ValueError('write only!') @password.setter def password(self, value): self._password = encrypt(value) 现在只需要简单的写成： 1234# 创建用户user = User(name='zhang', password='123456')# 修改密码user.password = '654321' 就可以了。 关于 Python 中 property和描述符的使用值得再另写一篇文章描述，在这里就不详细说明了。 混合属性（hybrid_property） 上面的例子看上去让代码清爽了不少，但是有时候这种用法是无法满足需要的，譬如下面这个例子： 12345class Student(Base): __tablename__ = 'student' id = Column(Integer, primary_key=True) name = Column(String(64)) birthday = Column(DateTime) 这是一个学生表映射，增加了 birthday 字段。通常我们会保存用户的生日，再通过生日获取用户年龄。有了上面的例子，很容易写出获取年龄的代码： 123456class Student(Base): ... @property def age(self): return datetime.now().year - self.birthday.year 现在可以简单的使用 student.age 获取具体的生日。 这样做是有缺陷的：如果需要获取所有 18 岁的学生呢？我们希望可以这样写： 1session.query(Student).filter_by(age=18).all() 但是却没有任何结果返回。如果改成这样呢？ 1234now = datetime.now()start = datetime(now.year - 18, 1, 1)end = end = datetime(now.year + 1 - 18, 1, 1)session.query(Student).filter(Student.birthday &gt;= start, Student.birthday &lt; end).all() 这样倒是可以获取正确的结果了，但是也太丑了点吧？难道没办法写出像第一条一样的既清晰又简洁的查询么？ 答案自然是有的，SQLAlchemy 提供了混合属性（hybrid_property）来处理类似的情况，于是我们可以改写获取年龄的代码： 12345678910111213from sqlalchemy.ext.hybrid import hybrid_propertyfrom sqlalchemy import funcclass Student(Base): ... @hybrid_property def age(self): return datetime.now().year - self.birthday.year @age.expression def age(self): return datetime.now().year - func.year(self.birthday) 这里将原本的 property 替换为 SQLAlchemy 中的 hybrid_property，同时提供了一个 expression 装饰器，在被装饰的方法中把 Python 代码翻译成 SQL（代码示例的目标数据库为 MySQL，获取日期中的年份的函数为YEAR()，使用其他数据库请查阅对应数据库的相关文档）。有了这个方法，SQLAlchemy 就知道如何在 SQL 语句中处理 age 属性了。 接下来稍微提一下 hybrid_method。 和 hybrid_property 类似，只不过可以给 hybrid_method 传参数。下面这个例子不太合适，只为了展示hybrid_method 的功能。 如何找到所有 90 后同学？当然我们可以复用上面的 age 属性，先计算一下 90 后的同学现在多少岁，然后直接写在查询里就好： 1session.query(Student).filter(Student.age &gt;= now.year - 1990, Student.age &lt; now.year - 2000).all() 如果要判断某个学生是否是 90 后呢？又需要再写一遍： 12if now.year - 2000 &gt; student.age &gt;= now.year - 1990: ... 出现了很多不直观的代码，这时候可以使用 hybrid_method 简化： 123456789class Student(Base): ... @hybrid_method def born_after(self, years): return years + 10 &gt; self.birthday.year &gt;= years @born_after.expression def born_after(self, years): return and_(func.year(self.birthday) &lt; years + 10, func.year(self.birthday) &gt;= years) 于是现在可以这样做： 1234session.query(Student).filter(Student.born_after(1990)).all()if student.born_after(1990): ... 看上去好了一些（误 这一部分就到此为止，当然 hybrid 在 SQLAlchemy 中的用法不止上述这些，更详细和复杂的内容参见官方文档。 关联代理（association_proxy） 简化标量集合 关联代理用在有关联的表中，所以我们先创建如下映射关系： 1234567891011121314151617association = Table('association', Base.metadata, Column('blog_id', Integer, ForeignKey('blog.id'), primary_key=True), Column('tag_id', Integer, ForeignKey('tag.id'), primary_key=True))class Blog(Base): __tablename__ = 'blog' id = Column(Integer, primary_key=True) name = Column(String(64)) tags = relationship( 'Tag', secondary=association, backref=backref('blogs', lazy='dynamic'), lazy='dynamic')class Tag(Base): __tablename__ = 'tag' id = Column(Integer, primary_key=True) name = Column(String(64)) 一个经常被拿出来作为演示的 Many-To-Many 模型。 先填充一些数据： 12345In [1]: blog = Blog(name='first')In [2]: blog.tags.append(Tag(name='t1'))In [3]: blog.tags.append(Tag(name='t2'))In [4]: session.add(blog)In [5]: session.commit() 接下来就可以获取这些对象的所有信息了： 12345678In [4]: blog.tags.all()Out[4]: [&lt;Tag at 0x1fdbab6f198&gt;, &lt;Tag at 0x1fdbab6f208&gt;]In [5]: blog.tags.all()[0].nameOut[5]: 't1'In [6]: [t.name for t in blog.tags]Out[6]: ['t1', 't2'] 上面的操作有点复杂。对我们而言，Tag 对象只有 name 字段是有用的，为了获取 name 字段，我们要写很多额外的代码把 name 字段从 Tag 对象中剥离出来。association_proxy 就可以用来简化这个操作。 现在修改一下上面的 Blog 映射： 123456789from sqlalchemy.ext.associationproxy import association_proxyclass Blog(Base): ... tag_objects = relationship( 'Tag', secondary=association, backref=backref('blogs', lazy='dynamic'), lazy='dynamic') tags = association_proxy('tag_objects', 'name') 增加了一行 association_proxy 对象的声明，现在我们可以这样做： 12In [7]: blog.tagsOut[7]: ['t1', 't2'] 现在查询操作变得很简单了，但是新增标签的操作还是很麻烦： 1blog.tag_objects.append(Tag(name='t3')) 还是需要实例化一个 Tag 对象，能不能直接写： 1blog.tags.append('t4') 当然是可以的，只要再修改一下 association_proxy 的声明： 1234class Blog(Base): ... tags = association_proxy('tag_objects', 'name', creator=lambda name: Tag(name=name)) 参数 creator 接受一个可调用对象，它告诉 association_proxy 如何处理“新增”操作。 注意：creator 的默认参数是被代理对象的构造函数，如果提供了一个单参数的构造函数，那么可以省略 creator 参数。 简化关联对象 上面的例子里把 association 表作为一个普通的 Table 对象，是因为 association 中不需要保存额外信息，只需要作为 Blog 和 Tag 的中转。现在有了新的需求，我们需要知道每篇博客的标签是在什么时候加上的，这就需要在 association 表中增加一个额外的字段用来表示创建时间，同时为了获取这个时间，还要把 association 改造成一个真正的映射： 123456789101112131415161718192021class Association(Base): __tablename__ = 'association' blog_id = Column(Integer, ForeignKey('blog.id'), primary_key=True) tag_id = Column(Integer, ForeignKey('tag.id'), primary_key=True) created_at = Column(DateTime, default=datetime.now) blog = relationship('Blog', backref=backref('blog_tags', lazy='dynamic'), lazy='joined') tag = relationship('Tag', backref=backref('tag_blogs', lazy='dynamic'), lazy='joined')class Tag(Base): __tablename__ = 'tag' id = Column(Integer, primary_key=True) name = Column(String(64))class Blog(Base): __tablename__ = 'blog' id = Column(Integer, primary_key=True) name = Column(String(64)) 这里实际上是把 Many-To-Many 拆成了两个 One-To-Many。 然后构造一些数据： 1234567In [1]: blog = Blog(name='first') ...: tags = [Tag(name='t1'), Tag(name='t2')] ...: for tag in tags: ...: session.add(Association(blog=blog, tag=tag)) ...: session.add(blog) ...: session.add_all(tags) ...: session.commit() 现在就可以获取 Tag 和被添加的时间了： 12345In [2]: blog.blog_tags[0].tag.nameOut[2]: 't1'In [3]: blog.blog_tags[0].created_atOut[3]: datetime.datetime(2018, 3, 18, 16, 4, 17) 可以看到，给 Blog 增加标签要经过 Association 这个中间对象。虽然表结构的确如此，但是我们仍然希望 Association 表是透明的，仅当需要获取其中的创建时间时才明确获取 Association 对象。只需要在 Blog 中声明一个关联代理： 1234class Blog(Base): ... tags = association_proxy('blog_tags', 'tag', creator=lambda tag: Association(tag=tag)) 然后就可以这样写了： 12In [4]: blog.tags[0].nameOut[4]: 't1' 添加新的 Tag 也方便了很多： 12In [3]: for tag in [Tag(name='t3'), Tag(name='t4')]: ...: blog.tags.append(tag) 混合关联代理 现在回到了第一个问题的出发点，能不能在上一个例子的基础上简化 tags 的调用呢？同样没问题，只要在 Association 中加一个关联代理： 12345class Association(Base): ... tag_objects = relationship('Tag', backref=backref('tag_blogs', lazy='dynamic'), lazy='joined') tags = association_proxy('tag_objects', 'name', creator=lambda name: Tag(name=name)) 然后用起来就和第一个例子一样了： 123456In [1]: blog.tagsOut[1]: ['t1', 't2']In [2]: blog.tags.append('t3')In [3]: blog.tagsOut[3]: ['t1', 't2', 't3'] 结语 上述内容并没有很复杂的操作，都是一些易于实现并且可以改善日常使用体验的方法。SQLAlchemy 还有很多骚操作可以讲，但是受限于本人的姿势水平，很多并没有实际使用过，也谈不上有什么见解。那就这样吧~","categories":[],"tags":[]},{"title":"文本聚类系列教程：（一）jieba中文分词工具入门","slug":"2018-03-12/文本聚类系列教程：（一）jieba中文分词工具入门","date":"2018-03-17T09:20:22.000Z","updated":"2018-04-02T14:12:49.203Z","comments":true,"path":"posts/575e441b/","link":"","permalink":"http://weafteam.github.io/posts/575e441b/","excerpt":"","text":"最近在学习文本分类（聚类）的相关知识，所以接下来准备先写一个关于这个方面的系列博客。 写在前面： 先介绍下由我们四个人组成的组织：FOUR ELEMENTS。四元素分别对应WELL、EARTH、AIR、FLAME，根据首字母缩写，我们的博客主页得名WEAF。 接下来介绍下我自己，我叫Leno，对应于四元素里面的Well，目前研究生在读，方向为智能信息处理。我的博客主要会以日常遇到的问题以及学习的知识为主。 简单的介绍： 首先，我们要做的是对中文文本的聚类，如果做聚类的话，我们需要对文本的内容做分析，而分析的最小单位肯定是词。 其次，中文和英文的词是有区别的，最大的区别就是中文的词与词之间并不是用空格分隔开的，而且由于中国文化的博大精深，切词的时候我们需要考虑的词语组合情况就更多了。显然让我们自己去造一个这样的轮子有点不现实，其实像这样的工具，前辈们已经为我们做好了，而且超好用。 本文介绍的就是jieba中文分词，正如它的口号那样。如下图所示： 当然，这里有两本秘籍GitHub &amp;&amp; OSChina，既然你我有缘，便免费赠予你。 安装 这年头，没有什么是一句pip install 解决不了的。不管2或者3，直接pip即可。 1pip install jieba 结合官方Demo理解jieba的三种切词模式 三种模式： 精确模式（默认模式）：它会试图将句子最精确的切开，适合文本分析。 全模式：不考虑歧义，这个模式会将所有的可以成词的词语都扫描出来，因而速度会非常快。 搜索引擎模式：该模式是在精确模式的基础上，对长词再进行切分，提高召回率，适用于搜索引擎分词。 官方Demo： 12345678910111213import jiebaseg_list = jieba.cut(&quot;我来到北京清华大学&quot;, cut_all=True)print(&quot;全模式: &quot; + &quot;/ &quot;.join(seg_list)) # 全模式seg_list = jieba.cut(&quot;我来到北京清华大学&quot;, cut_all=False)print(&quot;精确模式: &quot; + &quot;/ &quot;.join(seg_list)) # 精确模式seg_list = jieba.cut(&quot;他来到了网易杭研大厦&quot;) # 默认是精确模式print(&quot;默认模式：&quot; + &quot;/ &quot;.join(seg_list))seg_list = jieba.cut_for_search(&quot;小明硕士毕业于中国科学院计算所，后在日本京都大学深造&quot;) # 搜索引擎模式print(&quot;搜索引擎模式：&quot; + &quot;/ &quot;.join(seg_list)) 结果： 模式分析： 这里我们先分析这三种模式，对于cut方法的讲解在后边会给出，so不要问我为啥不给出cut方法中第三个参数HMM。 通过对比前两条输出可以看出全模式情况下，它会找出所有可以组成词的划分，而精确模式与其对比给出的答案就会很清爽。所以结合上文所说，不难理解这两个模式的区别。 接下来我们看第四条输出，它是在精确模式的基础上对长词再做的划分。所以‘日本京都大学’，它会再次切分为‘日本’，‘京都’，‘大学’三个词，同理适用于‘中国科学院’。所以这个模式也不难理解吧。 补充分析： 最后看第三条输出内容，也许你会问，既然知道默认模式是精确模式了，为啥还要给出试例，况且还是一个不具有对比性质的对比。这里其实想说明的是： ‘杭研’并没有在词典中，但是jieba的Viterbi算法也将其识别了出来。 这时我们就需要考虑HMM这个参数了，关于HMM（Hidden Markov Model，HMM：隐马尔可夫模型），如果深究，那就需要另外一篇博文了，所以我们只要能理解官方给出的这句话即可：对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法。 可能说的比较干涩，我们实际测一下吧。 补充测试代码： 1234567import jiebaseg_list = jieba.cut(&quot;他来到了网易杭研大厦&quot;,HMM=False)print(&quot;HMM为False：&quot; + &quot;/ &quot;.join(seg_list))seg_list = jieba.cut(&quot;他来到了网易杭研大厦&quot;,HMM=True)print(&quot;HMM为True：&quot; + &quot;/ &quot;.join(seg_list)) 补充测试结果： 所以一般情况下，使用cut方法，不用考虑HMM这个参数就可以，让它默认为True即可，让Viterbi算法为我们识别新词。HMM也能有效的解决中文中的歧义问题。 启用HMM并不适用所有情况，根据需要开启！！！ 关于切词的方法以及切词的注意事项，请大家参考上文给出的两个链接，这里我不再赘述。 基于TF-IDF的关键词提取 相关知识： 对于一个文档，我们肯定不会对所有的词进行聚类，所以我们需要对文档进行关键词提取。 下面我们对TF-IDF做一下简单的说明。如果单讲这个知识点，拿出来又是一篇博文。不过后续我也会写一篇关于它的博文。暂时请大家自行查阅相关资料学习。 TF-IDF是一种统计方法，用于评估一个词对于一个文件集或者语料库中的一份文件的重要程度。 TF(term frequency)：指的是某一个给定的词语在该文件中出现的频率。公式如下： \\(tf_i,_j = \\frac{n_i,_j}{\\sum_k n_k,_j}\\) IDF(Inverse document frequency)：是一个词语普遍重要性的度量。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到： \\(idf(t,D) = log(\\frac{N}{\\lvert {d \\in D, t \\in d}\\rvert})\\) 关键词提取： 官方给了一个代码示例文件，源代码在这里：关键词提取源码 但是为了结果显示得更清晰一点，我做了些许的改动： 12345678910111213141516171819202122232425262728293031import syssys.path.append(&apos;../&apos;)import jiebaimport jieba.analysefrom optparse import OptionParserUSAGE = &quot;usage: python extract_tags.py [file name] -k [top k]&quot;parser = OptionParser(USAGE)parser.add_option(&quot;-k&quot;, dest=&quot;topK&quot;)opt, args = parser.parse_args()if len(args) &lt; 1: print(USAGE) sys.exit(1)file_name = args[0]if opt.topK is None: topK = 20else: topK = int(opt.topK)content = open(file_name, &apos;rb&apos;).read()tags = jieba.analyse.extract_tags(content, topK=topK,withWeight=True)for i in tags : print(i) 先说下用法，官方在文件的第8行给出了用法，即： 1python extract_tags.py [file name] -k [top k] 将这个Extract_tags.py文件和文本文件放在同一目录下，然后给利用如上命令便可得到文本的关键词。默认取得是top10，我改了下取了top20，我们这里做下测试（使用jieba的官方测试文档：《围城》），结果如下： 分析： 官方给的代码看着挺长，实际上超简单，其中重要的无非两句话，一句是读文件，另一句则是调用extract_tags()方法，我在原有的基础上设置了withWight=True，因而返回了一个权重值。大家如果嫌麻烦可以对上述关键代码进行抽取，写一个自己的测试。 正如上图所示，‘自己’、‘知道’、‘先生’等等等等，像这些词语都是些没有实际意义的单词，所以在聚类的时候这些单词不应该做为聚类（或者分类）的标准，它们属于stop_words，中文的意思就是停用词，所以我们接下来处理这个问题。 去除停用词 去除停用词，我们需要知道哪些属于停用词，我在CSDN上找到了一个1893规模的停用词表，链接如下：最全中文停用词表整理（1893个）。 我们接下来的工作思路是这样的，对《围城》（文件1.txt）进行切词，方法就是之前的cut()，读取StopWords文件，对比每个切分出来的单词是否是停用词，如果不是则加入到一个list中，然后再将这个list的内容存到另一个文件2.txt中，对文件2.txt使用之前说到的官方给的关键词提取文件做关键词提取即可。 去除停用词代码如下： 12345678910111213141516171819202122232425262728293031import sysimport jiebafrom os import pathd = path.dirname(__file__) # 获取当前文件的dir路径stopwords_path = &apos;stopwords1893.txt&apos; # 停用词表路径text_path = &apos;txt/1.txt&apos; #《围城》的文本路径text = open(path.join(d, text_path),&apos;rb&apos;).read()def RmStopWords(text): mywordlist = [] seg_list = jieba.cut(text, cut_all=False) liststr=&quot;/ &quot;.join(seg_list) # 添加切分符 f_stop = open(stopwords_path) try: f_stop_text = f_stop.read() finally: f_stop.close( ) f_stop_seg_list=f_stop_text.split(&apos;\\n&apos;) # 停用词是每行一个，所以用/n分离 for myword in liststr.split(&apos;/&apos;): #对于每个切分的词都去停用词表中对比 if not(myword.strip() in f_stop_seg_list) and len(myword.strip())&gt;1: mywordlist.append(myword) return &apos;&apos;.join(mywordlist) #返回一个字符串txt2 = RmStopWords(text)text_write = &apos;txt/2.txt&apos;with open(text_write,&apos;w&apos;) as f: f.write(txt2) print(&quot;Success&quot;) 结果： 分析： 由上图可见，我们的去停用词的效果还不错。 最后： 这篇博客先写到这里，下一篇博客我会讲到jieba中文分词的进阶篇。感谢阅读，如有问题可以通过邮件与我交流，邮箱：cliugeek@us-forever.com","categories":[{"name":"文本聚类","slug":"文本聚类","permalink":"http://weafteam.github.io/categories/文本聚类/"}],"tags":[{"name":"文本聚类","slug":"文本聚类","permalink":"http://weafteam.github.io/tags/文本聚类/"}]},{"title":"MySQL主从数据库的设置与Xtrabackup备份InnoDB(MySQL)","slug":"2018-03-12/linux-mysql","date":"2018-03-17T08:08:56.000Z","updated":"2018-04-02T14:12:49.202Z","comments":true,"path":"posts/2f5dded6/","link":"","permalink":"http://weafteam.github.io/posts/2f5dded6/","excerpt":"一、准备环境 两台服务器：服务器A、服务器B 服务器A：Red Hat Enterprise Linux Server release 6.5 (Santiago) 服务器B：Red Hat Enterprise Linux Server release 6.5 (Santiago) 服务器A IP：172.16.125.50 服务器B IP：172.16.125.52 MySQL版本：5.6.23 二、安装MySQL 具体安装请见 LinuxMySQL的安装(1) LinuxMySQL的安装(2) LinuxMySQL的安装(3)","text":"一、准备环境 两台服务器：服务器A、服务器B 服务器A：Red Hat Enterprise Linux Server release 6.5 (Santiago) 服务器B：Red Hat Enterprise Linux Server release 6.5 (Santiago) 服务器A IP：172.16.125.50 服务器B IP：172.16.125.52 MySQL版本：5.6.23 二、安装MySQL 具体安装请见 LinuxMySQL的安装(1) LinuxMySQL的安装(2) LinuxMySQL的安装(3) 三、主从库配置 1、主库在/etc/my.cnf里添加以下内容 12345678910#log日志log_bin=mysql_bin#server IDserver_id=2#忽略同步的库binlog-ignore-db=information_schemabinlog-ignore-db=clusterbinlog-ignore-db=mysql#需要同步的库binlog-do-db=test 2、从库在/etc/my.cnf里添加以下内容 12345678910log_bin=mysql_binserver_id=3binlog-ignore-db=information_schemabinlog-ignore-db=clusterbinlog-ignore-db=mysqlreplicate-do-db=ufind_dbreplicate-ignore-db=mysqllog-slave-updatesslave-skip-errors=allslave-net-timeout=60 四、主从库设置 1、进入主库，我们在主库中创建一个的账户，从库通过使用这个账号来同步数据。 1CREATE USER 'repl'@'172.16.125.52' IDENTIFIED BY '123456'; 2、赋予相应的权限 12345GRANT FILE ON *.* TO 'repl'@'172.16.125.52' IDENTIFIED BY '123456';GRANT REPLICATION SLAVE ON *.* TO 'repl'@'172.16.125.52' IDENTIFIED BY '123456';FLUSH PRIVILEGES; 3、重启数据库（主库）执行以下命令 1SHOW MASTER STATUS; 要记住以上的信息，在设置从库的时候需要填写并设置。 4、在从库里边执行以下命令 123stop slave;change master to master_host=&apos;172.16.125.50&apos;,master_user=&apos;repl&apos;,master_password=&apos;123456&apos;,master_log_file=&apos;mysql_bin.000023&apos;, master_log_pos=120;start slave; 5、然后执行一下命令查看状态 1show slave status \\G; 内容如下： 6、测试与提示 后期的测试中我们只针对test库进行了同步。 所以只能针对test进行的操作才有效。 如果后期对一些列库进行操作，需要 添加相应的配置 1234#主库配置文件binlog-do-db=test#从库配置文件replicate-do-db=test 并查询出最新的master的状态，停止从库。并改变从库的配置重启同步。 五、Xtrabackup的简单介绍 ——————- Percona XtraBackup 是世界上唯一的开源免费的MySQL热备份软件，可以执行非阻塞操作 InnoDB和XtraDB数据库的备份。 Percona XtraBackup可提供以下优点： 备份快速安全可靠 备份期间不间断的事务处理 节省磁盘空间和网络带宽 自动备份验证 更快的恢复时间保证正常工作 Percona XtraBackup 为所有版本的Percona服务器，MySQL和MariaDB提供MySQL热备份。 它可执行 流媒体，压缩和增量MySQL备份。 六、Xtrabackup的安装 如果在互联网下 可使用以下命令安装 1wget https://www.percona.com/downloads/XtraBackup/Percona-XtraBackup-2.4.4/binary/redhat/7/x86_64/percona-xtrabackup-24-2.4.4-1.el7.x86_64.rpm 获取相应rpm包 安装部分依赖(不同的操作系统可能已安装的库不尽相同) 1234rpm -ivh mysql-community-libs-compat-5.7.20-1.el7.x86_64.rpm#根据mysql版本而定yum list|grep perlyum -y install perl-DBI.x86_64 perl-DBD-MySQL.x86_64 然后安装Xtrabackup 1rpm -ivh percona-xtrabackup-24-2.4.4-1.el7.x86_64.rpm 参考： 1yum install cmake gcc gcc-c++ libaio libaio-devel automake autoconf bison libtool ncurses-devel libgcrypt-devel libev-devel libcurl-devel vim-common 七、Xtrabackup备份MySQL 12xtrabackup --defaults-file=/etc/my.cnf --user=root --password=root --host=localhost --backup --target-dir=/data/backups/可指定数据库--databases=test 八、Xtrabackup的备份恢复 备份之前必须先关闭MySQL server 然后删除data目录（/var/lib/mysql一般情况是这个） 1xtrabackup --copy-back --target-dir=/data/backups/ 执行完恢复之后需要设置文件权限 1chown -R mysql:mysql /var/lib/mysql 然后启动mysql 123systemctl start mysqld.service#或者使用服务service mysqld start 九、使用脚本自动备份7天之内的数据 12345678910111213141516171819202122#!/bin/sh# Database infoDB_USER=\"root\"DB_PASS=\"root\"DB_HOST=\"localhost\"# Others varsBCK_DIR=\"/opt/app/mysqlbackup\" #the backup file directoryCONF_DIR=\"/etc/my.cnf\"DATE=`date +%F`RMDATE=`date -d '-7 day' +%F`# TODOmkdir -p $BCK_DIR/$DATE/#Create dir for save backup dataxtrabackup --defaults-file=$CONF_DIR --user=$DB_USER --password=$DB_PASS --host=$DB_HOST --backup --target-dir=$BCK_DIR/$DATE/#Backup mysql datarm -rf $BCK_DIR/$RMDATE#Delete the backup 7 days ago#热备份数据库 加入crontab 130 2 * * * /bin/sh /home/scripts/mysqlbackup.sh 更多请参考官方文档","categories":[{"name":"Linux","slug":"Linux","permalink":"http://weafteam.github.io/categories/Linux/"}],"tags":[{"name":"Linux运维","slug":"Linux运维","permalink":"http://weafteam.github.io/tags/Linux运维/"}]},{"title":"chapter-01-AIR","slug":"2018-03-12/chapter-01-AIR","date":"2018-03-14T10:49:23.000Z","updated":"2018-04-02T14:12:49.111Z","comments":true,"path":"posts/8e8e4531/","link":"","permalink":"http://weafteam.github.io/posts/8e8e4531/","excerpt":"","text":"第一篇文章-TensorFlow Install 首先介绍一些我们这个组织，这是有四个人构成得一个组织，组织可以叫FOUR ELEMENTS。（也可以叫WEAF）分别对应WELL、EARTH、AIR、FLAME。（WEAF）。 其次我想做一下自我介绍，我的英文学名叫milittle。我开设的这个周刊名字叫AIR-周刊。希望把自己学习的一些内容分享给大家，也激励自己。学更多的知识。以后大家有什么要交流的，也可以一起交流。（邮箱地址会在文章末尾给出） 接下来我讲一下我后续每周在AIR-周刊里面会讲到的内容： 主要涉及TensorFlow框架使用多一些 后续也会分享一些机器学习方面的算法 也会有一些在人工智能方面的杂谈 上面说了一些，我想把这块做好，文章内容有什么变化，后续的文章里面会有所提及。 今天就介绍一些TensorFlow的简述和安装： TensorFlow是Google公司在2015年12月份开源的一个机器学习库，代码链接TensorFLow。 第二点为什么现在TensorFlow这么火，在人工智能界已经算得上是称霸的地位，我们可以从下面的图中可以看出TensorFlow的数据占据了一大半市场。 原因是什么呢 最主要的原因就是本身具有图运算的这个概念。使用简单，而且可以让程序员快捷的实现一些算法。从而可以用TensorFlow解决一些现实中的问题。图运算的概念我们后续会慢慢深入。大家不要着急。 还有一个原因，我想不用说大家也都知道，既然说了是Google的开源框架，那么技术就一定很牛逼。引得广大程序员的喜爱也是必然发生的事情。 而且用这个框架可以快速的解决一些机器学习的算法问题。是的编程效率也不断提高。 TensorFlow支持Mac、Windows、Linux。以后我们的实验有可能通过Windows进行，也有可能在Linux进行，而且以后的代码都是基于python3.X，所以希望大家可以实现基本的python3的语法知识和编程知识。还有就是TensorFlow支持CPU版本和GPU版本，安装的时候都有很多的注意事项，基于GPU版本的可能会比较麻烦。但是后续我会给大家出一个教程，分别在Windows下面和Linux下面配置自己的独立环境。让你的机器学习算法跑在你自己的机器上面。完成一些看起来炫酷的程序。 接下来我介绍一下TensorFlow的Windows CPU安装方法： 首先打开电脑，这个是一定的~ 去TensorFlow的官网下载Windows的版本。点击下面红色箭头的地方—随意，都可以跳转到一个关于windows安装的界面。（可能需要科学上网，逃） 点开界面以后的注意事项： windows7及其以后的操作系统版本 决定安装哪个TensorFlow的版本，GPU还是CPU（GPU会有有一些第三方的库依赖，CUDA），接下来我们的教程是CPU版本安装。 决定怎么安装TensorFlow：可选方式有native pip 和 Anaconda等（我们使用Anaconda） 最后一步验证你的安装效果 接下来一步一步来： 第一步、我们决定用Anaconda来安装TensorFlow，你要知道Anaconda是什么呢，它就是可以很好的管理python的一些依赖库。让你在不同python版本之间切换自如。所以我们使用这个工具来安装我们的TensorFlow。Anaconda也可以集成Spyder这些编程工具，使得你编写代码会方便一些。 第二步、首先你去Anaconda官网下载windows版本的Anaconda，具体安装就和普通的安装软件类似。这个地方需要注意的是不同python版本需要不同的Anaconda，别下错了。 第三步、安装好以后，我们打开Anaconda的控制台，就是开始里面找到Anaconda的应用，然后里面有一个Anaconda Prompt。打开以后，我们就开始了我们创建一个独立的TensorFlow独立的环境。 conda create -n tensorflow pip python=3.5 上面这命令的意思就是说在Anaconda管理的环境里面给我独立的创建一个python环境来，这个里面python的版本是3.5。注意一下，这个地方还没有安装tensorflow呢，上面的tensorflow只不过是创建的一个环境名字而已。 activate tensorflow 上面的命令是激活这个tensorflow的环境，你可以通过这个环境，添加一些你自己的python库，定制自己的python环境，这也是我使用Anaconda的原因，但是并不是只有Anaconda支持这样的方式。不要和我抬杠。 第四步，也就是正儿八经的安装TensorFlow的阶段，这里解释一下，上面为什么我执行的是tensorflow1，因为我的电脑上面已经有tensorflow这个环境了 pip install --ignore-installed --upgrade tensorflow 这个命令就是使用pip正常的安装tensorflow，这里的pip管理起来和普通的pip管理是一个道理，这里就不赘述了。 第五步，测试TensorFlow是否安装上 python 上面的命令是进入python解释器，然后执行下面的import语句 import tensorflow as tf 如果上面的命令执行完，如下图中一样，就算安装成功了，下面的那些语句是写了一个hello world！！！ 今天是为了我们以后在TensorFlow上开发所做的准备。希望大家安装顺利。我的个人邮箱是air@weaf.top。有什么问题可以单独发邮件问我。感谢你们的驻足。有什么不好的地方，可以给出意见。","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]}]}