{"meta":{"title":"WEAF 周刊","subtitle":null,"description":null,"author":"WEAF","url":"http://weafteam.github.io"},"pages":[],"posts":[{"title":"chapter-08-AIR","slug":"2018-04-30/chapter-08-AIR","date":"2018-05-06T03:30:48.000Z","updated":"2018-05-06T07:18:24.197Z","comments":true,"path":"posts/839e2740/","link":"","permalink":"http://weafteam.github.io/posts/839e2740/","excerpt":"","text":"TensorFlow Linear Regression 2 回顾一下线性回归的loss函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsdef run(): ops.reset_default_graph() sess = tf.Session() batch_size = 100 x_vals = np.linspace(0, 10, 100) y_vals = x_vals + np.random.normal(0, 1, 100) x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32) y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32) # Create variables for linear regression A = tf.Variable(tf.random_normal(shape=[1, 1])) b = tf.Variable(tf.random_normal(shape=[1, 1])) # Declare model operations model_output = tf.add(tf.matmul(x_data, A), b) loss_l1 = tf.reduce_mean(tf.abs(y_target - model_output)) # Declare optimizers my_opt_l1 = tf.train.GradientDescentOptimizer(0.0001) train_step_l1 = my_opt_l1.minimize(loss_l1) # Initialize variables init = tf.global_variables_initializer() sess.run(init) loss_vec_l1 = [] for i in range(1000): rand_index = np.random.choice(len(x_vals), size=batch_size) rand_x = np.transpose([x_vals[rand_index]]) rand_y = np.transpose([y_vals[rand_index]]) sess.run(train_step_l1, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) temp_loss_l1 = sess.run(loss_l1, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) loss_vec_l1.append(temp_loss_l1) if (i + 1) % 25 == 0: print('Step #' + str(i + 1) + ' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b))) ops.reset_default_graph() # Create graph sess = tf.Session() x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32) y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32) # Create variables for linear regression A = tf.Variable(tf.random_normal(shape=[1, 1])) b = tf.Variable(tf.random_normal(shape=[1, 1])) # Declare model operations model_output = tf.add(tf.matmul(x_data, A), b) loss_l2 = tf.reduce_mean(tf.square(y_target - model_output)) # Declare optimizers my_opt_l2 = tf.train.GradientDescentOptimizer(0.0001) train_step_l2 = my_opt_l2.minimize(loss_l2) # Initialize variables init = tf.global_variables_initializer() sess.run(init) loss_vec_l2 = [] for i in range(1000): rand_index = np.random.choice(len(x_vals), size=batch_size) rand_x = np.transpose([x_vals[rand_index]]) rand_y = np.transpose([y_vals[rand_index]]) sess.run(train_step_l2, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) temp_loss_l2 = sess.run(loss_l2, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) loss_vec_l2.append(temp_loss_l2) if (i + 1) % 25 == 0: print('Step #' + str(i + 1) + ' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b))) plt.plot(loss_vec_l1, 'k-', label='L1 Loss') plt.plot(loss_vec_l2, 'r--', label='L2 Loss') plt.title('L1 and L2 Loss per Generation') plt.xlabel('Generation') plt.ylabel('L1 Loss') plt.legend(loc='upper right') plt.show()def main(_): run()if __name__ == '__main__': tf.app.run() 戴明回归: 看图：找不同： loss函数是关键:下面是deming regression的loss函数 \\[ \\frac{\\mid{A*x+b-y}\\mid}{\\sqrt{A^2 + 1}} \\] 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2018/5/6 14:32# @Author : milittle# @Site : www.weaf.top# @File : deming_lr.py# @Software: PyCharmimport matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()def run(): x_vals = np.linspace(0, 10, 100) y_vals = x_vals + np.random.normal(0, 1, 100) batch_size = 125 x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32) y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32) A = tf.Variable(tf.random_normal(shape=[1, 1])) b = tf.Variable(tf.random_normal(shape=[1, 1])) model_output = tf.add(tf.matmul(x_data, A), b) # 注意这里的loss函数的求解 demming_numerator = tf.abs(tf.subtract(tf.add(tf.matmul(x_data, A), b), y_target)) demming_denominator = tf.sqrt(tf.add(tf.square(A), 1)) loss = tf.reduce_mean(tf.truediv(demming_numerator, demming_denominator)) my_opt = tf.train.GradientDescentOptimizer(0.01) train_step = my_opt.minimize(loss) # Initialize variables init = tf.global_variables_initializer() sess.run(init) loss_vec = [] for i in range(1500): rand_index = np.random.choice(len(x_vals), size=batch_size) rand_x = np.transpose([x_vals[rand_index]]) rand_y = np.transpose([y_vals[rand_index]]) sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) temp_loss = sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) loss_vec.append(temp_loss) if (i + 1) % 100 == 0: print(&apos;Step #&apos; + str(i + 1) + &apos; A = &apos; + str(sess.run(A)) + &apos; b = &apos; + str(sess.run(b))) print(&apos;Loss = &apos; + str(temp_loss)) [W] = sess.run(A) [bias] = sess.run(b) # Get best fit line best_fit = [] for i in x_vals: best_fit.append(W * i + bias) plt.plot(x_vals, y_vals, &apos;o&apos;, label=&apos;Data Points&apos;) plt.plot(x_vals, best_fit, &apos;r-&apos;, label=&apos;Best fit line&apos;, linewidth=3) plt.legend(loc=&apos;upper left&apos;) plt.title(&apos;Sepal Length vs Pedal Width&apos;) plt.xlabel(&apos;Pedal Width&apos;) plt.ylabel(&apos;Sepal Length&apos;) plt.show() # Plot loss over time plt.plot(loss_vec, &apos;k-&apos;) plt.title(&apos;Demming Loss per Generation&apos;) plt.xlabel(&apos;Iteration&apos;) plt.ylabel(&apos;Demming Loss&apos;) plt.show()def main(_): run()if __name__ == &apos;__main__&apos;: tf.app.run() LASSO and Ridge Regression(关键的地方还是loss函数的不同，其他步骤是一致的) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2018/5/6 14:46# @Author : milittle# @Site : www.weaf.top# @File : LASSO_Ridge_lr.py# @Software: PyCharmimport matplotlib.pyplot as pltimport sysimport numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()regression_type = 'LASSO'def run(): sess = tf.Session() x_vals = np.linspace(0, 10, 100) y_vals = x_vals + np.random.normal(0, 1, 100) batch_size = 125 x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32) y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32) seed = 13 np.random.seed(seed) tf.set_random_seed(seed) A = tf.Variable(tf.random_normal(shape=[1, 1])) b = tf.Variable(tf.random_normal(shape=[1, 1])) model_output = tf.add(tf.matmul(x_data, A), b) if regression_type == 'LASSO': lasso_param = tf.constant(0.9) heavyside_step = tf.truediv(1., tf.add(1., tf.exp(tf.multiply(-50., tf.subtract(A, lasso_param))))) regularization_param = tf.multiply(heavyside_step, 99.) loss = tf.add(tf.reduce_mean(tf.square(y_target - model_output)), regularization_param) elif regression_type == 'Ridge': ridge_param = tf.constant(1.) ridge_loss = tf.reduce_mean(tf.square(A)) loss = tf.expand_dims( tf.add(tf.reduce_mean(tf.square(y_target - model_output)), tf.multiply(ridge_param, ridge_loss)), 0) else: print('Invalid regression_type parameter value', file=sys.stderr) my_opt = tf.train.GradientDescentOptimizer(0.001) train_step = my_opt.minimize(loss) init = tf.global_variables_initializer() sess.run(init) # Training loop loss_vec = [] for i in range(1500): rand_index = np.random.choice(len(x_vals), size=batch_size) rand_x = np.transpose([x_vals[rand_index]]) rand_y = np.transpose([y_vals[rand_index]]) sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) temp_loss = sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) loss_vec.append(temp_loss[0]) if (i + 1) % 300 == 0: print('Step #' + str(i + 1) + ' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b))) print('Loss = ' + str(temp_loss)) print('\\n') [W] = sess.run(A) [bias] = sess.run(b) # Get best fit line best_fit = [] for i in x_vals: best_fit.append(W * i + bias) # Plot the result plt.plot(x_vals, y_vals, 'o', label='Data Points') plt.plot(x_vals, best_fit, 'r-', label='Best fit line', linewidth=3) plt.legend(loc='upper left') plt.title('Sepal Length vs Pedal Width') plt.xlabel('Pedal Width') plt.ylabel('Sepal Length') plt.show() # Plot loss over time plt.plot(loss_vec, 'k-') plt.title(regression_type + ' Loss per Generation') plt.xlabel('Generation') plt.ylabel('Loss') plt.show()def main(_): run()if __name__ == '__main__': tf.app.run() Elastic Net Regression(利用多个loss函数的叠加进行训练)弹性的方式 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2018/5/6 14:57# @Author : milittle# @Site : www.weaf.top# @File : Elastic_Net_Regression.py# @Software: PyCharmimport matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.python.framework import opssess = tf.Session()x_vals = np.linspace(0, 10, 100)y_vals = x_vals + np.random.normal(0, 1, 100)batch_size = 16seed = 13np.random.seed(seed)tf.set_random_seed(seed)x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32, name='input')y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32, name='output')A = tf.Variable(tf.random_normal(shape=[1, 1]))b = tf.Variable(tf.random_normal(shape=[1, 1]))model_output = tf.add(tf.matmul(x_data, A), b)elastic_param1 = tf.constant(1.)elastic_param2 = tf.constant(1.)l1_a_loss = tf.reduce_mean(tf.abs(A))l2_a_loss = tf.reduce_mean(tf.square(A))e1_term = tf.multiply(elastic_param1, l1_a_loss)e2_term = tf.multiply(elastic_param2, l2_a_loss)loss = tf.expand_dims(tf.add(tf.add(tf.reduce_mean(tf.square(y_target - model_output)), e1_term), e2_term), 0)my_opt = tf.train.GradientDescentOptimizer(0.001)train_step = my_opt.minimize(loss)init = tf.global_variables_initializer()sess.run(init)# Training looploss_vec = []for i in range(1000): rand_index = np.random.choice(len(x_vals), size=batch_size) rand_x = np.transpose([x_vals[rand_index]]) rand_y = np.transpose([y_vals[rand_index]]) sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) temp_loss = sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) loss_vec.append(temp_loss[0]) if (i + 1) % 250 == 0: print('Step #' + str(i + 1) + ' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b))) print('Loss = ' + str(temp_loss))W = sess.run(A)bias = sess.run(b)plt.plot(loss_vec, 'k-')plt.title('Loss per Generation')plt.xlabel('Generation')plt.ylabel('Loss')plt.show() 好，今天到此把一些线性模型的应用问题大体讲完了，还是有什么问题，可以积极讨论，可以给我发邮件air@weaf.top，上面的代码由于很简单，所以我想大家都可以看懂的。其实这么多类型的回归问题，总的来说就是loss函数不一样，只要将loss函数理解了，那么问题就迎刃而解。","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]},{"title":"chapter-07-AIR","slug":"2018-04-23/chapter-07-AIR","date":"2018-05-06T03:29:55.000Z","updated":"2018-05-06T06:15:34.433Z","comments":true,"path":"posts/1ceb091/","link":"","permalink":"http://weafteam.github.io/posts/1ceb091/","excerpt":"","text":"TensorFlow Linear Regression 1 亲爱的小伙伴们，上周又事情给耽搁了，这周将上周的内容一起补充一下。我们前面的内容将TensorFlow的基础内容都介绍了，所以接下来我们需要实现一些基本的算法，如果大家想跑一些现在主流的一些网络结构，大家可以移步到我的GitHub MLModel这个仓库MLModel，里面会定期更新一些主流的网络框架。喜欢的话，给个star可好。接下来呢我们开始我们今天的线性回归模型的各种求解方法。 在TensorFlow中使用矩阵的逆来解决线性模型，这个解决方案，相比学过线性代数的你，都会解。 大家可曾记得线性代数里面的线性方程组： \\[ A * x + b = y \\] x就是我们要解的未知解，那x该怎么解来着？ \\[ x = {(A^T * A)}^{-1} * A^T * y - b \\] 上面的公式一下就能看出来是怎么回事对不对？ 推导过程： \\[ 第一步:A^T * A * x = A^T*y-b\\\\第二步：{(A^T*A)}^{-1} *{(A^T*A)} * x = {(A^T*A)}^{-1} * A^T*y-b\\\\第三步：左边是不是就是一个单位矩阵了？\\\\得到最后的结果是：x = {(A^T * A)}^{-1} * A^T * y-b \\] 我们下面使用的A的特征维度是一维，(再加上b这个维度)这是为了可以可视化结果：在直接利用矩阵运算而得到的解对于这些数据来说，是最好的结果。也是唯一解。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()def run(): # 构造数据 x_vals = np.linspace(0, 10, 100) y_vals = x_vals + np.random.normal(0, 1, 100) x_vals_column = np.transpose(np.matrix(x_vals)) ones_column = np.transpose(np.matrix(np.repeat(1, 100))) A = np.column_stack((x_vals_column, ones_column)) y = np.transpose(np.matrix(y_vals)) A_tensor = tf.constant(A) y_tensor = tf.constant(y) #利用矩阵的逆解决这个线性问题。 t_A_A = tf.matmul(tf.transpose(A_tensor), A_tensor) #求矩阵转置和本身的乘积 t_A_A_inverse = tf.matrix_inverse(t_A_A) # 求矩阵的逆 product = tf.matmul(t_A_A_inverse, tf.transpose(A_tensor)) solution = tf.matmul(product, y_tensor) solution_eval = sess.run(solution) W = solution_eval[0][0] bias = solution_eval[1][0] print('W: ' + str(W)) print('bias: ' + str(bias)) # Get best fit line best_fit = [] for i in x_vals: best_fit.append(W * i + bias) plt.plot(x_vals, y_vals, 'o', label = 'data') plt.plot(x_vals, best_fit, 'r-', label = 'best fit line', linewidth = 3) plt.legend(loc='upper left') plt.show()def main(_): run()if __name__ == '__main__': tf.app.run() 其实大家也可以看出，我们使用矩阵的逆去求线性模型是又快又准，但是你们想过没有为啥神经网络里面在求解线性回归的时候还要用到反向传播梯度下降的方式呢，给你们举个例子，假设现在一个样本有上万或者十万个特征维度，你想想我们在求解矩阵逆的时候要花费多长时间，那是你意想不到的，所以才会使用方向传播算法去解决W的求解问题。详细需要花费的时间。 第二种求解方式：Cholesky Method \\[ A*x=y \\] 思路如下：在Cholesky method方法中，我们需要将A分解为L和L的转置的乘积，然后再进行x的求解，为什么要这么做呢?当然是为了避免求矩阵的逆，它很耗费时间。 因为A要求在求解Cholesky Depcomposition的时候是square也就是方阵。但是一般的A都不是方阵，那么我们就构造出来一个方阵： \\[ A^T*A \\] 它就是一个方阵，那么我们就求A转置和A乘积的Cholesky Decomposition。 首先明确分解步骤： \\[ 第一步：A^T*A=L^T*L\\\\第二步：L^T*L*x=A^T*Y\\\\第三步：L^T*z=A^T*Y\\\\where z = L*x \\] 其次明确求解步骤： \\[ 第一步：计算A的Cholesky Decomposition\\\\where A^T*A=L^T*L\\\\第二步：求解z，利用L^T*z=A^T*y这个公式\\\\第三步：求解x，利用L*x=z这个公式 \\] 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()def run(): # 构造数据 x_vals = np.linspace(0, 10, 100) y_vals = x_vals + np.random.normal(0, 1, 100) x_vals_column = np.transpose(np.matrix(x_vals)) ones_column = np.transpose(np.matrix(np.repeat(1, 100))) A = np.column_stack((x_vals_column, ones_column)) y = np.transpose(np.matrix(y_vals)) A_tensor = tf.constant(A) y_tensor = tf.constant(y) tA_A = tf.matmul(tf.transpose(A_tensor), A_tensor) L = tf.cholesky(tA_A) tA_y = tf.matmul(tf.transpose(A_tensor), y) sol1 = tf.matrix_solve(L, tA_y) sol2 = tf.matrix_solve(tf.transpose(L), sol1) solution_eval = sess.run(sol2) W = solution_eval[0][0] bias = solution_eval[1][0] print('slope: ' + str(W)) print('y_intercept: ' + str(bias)) best_fit = [] for i in x_vals: best_fit.append(W * i + bias) plt.plot(x_vals, y_vals, 'o', label='Data') plt.plot(x_vals, best_fit, 'r-', label='Best fit line', linewidth=3) plt.legend(loc='upper left') plt.show()def main(_): run()if __name__ == '__main__': tf.app.run() 使用反向传播梯度下降的方式求解线性模型。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()def run(): # 构造数据 batch_size = 32 x_vals = np.linspace(0, 10, 100) y_vals = x_vals + np.random.normal(0, 1, 100) x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32) y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32) A = tf.Variable(tf.random_normal(shape=[1, 1])) b = tf.Variable(tf.random_normal(shape=[1, 1])) model_output = tf.add(tf.matmul(x_data, A), b) loss = tf.reduce_mean(tf.square(y_target - model_output)) my_opt = tf.train.GradientDescentOptimizer(0.00001) train_step = my_opt.minimize(loss) sess.run(tf.global_variables_initializer()) loss_vec = [] for i in range(10000): rand_index = np.random.choice(len(x_vals), size = batch_size) rand_x = np.transpose([x_vals[rand_index]]) rand_y = np.transpose([y_vals[rand_index]]) sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) temp_loss = sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) loss_vec.append(temp_loss) if (i + 1) % 25 == 0: print('Step #' + str(i + 1) + ' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b))) print('Loss = ' + str(temp_loss)) [W] = sess.run(A) [b] = sess.run(b) best_fit = [] for i in x_vals: best_fit.append(W * i + b) plt.plot(x_vals, y_vals, 'o', label='Data Points') plt.plot(x_vals, best_fit, 'r-', label='Best fit line', linewidth=3) plt.legend(loc='upper left') plt.title('Sepal Length vs Pedal Width') plt.xlabel('Pedal Width') plt.ylabel('Sepal Length') plt.show() plt.plot(loss_vec, 'b-') plt.title('L2 Loss per Generation') plt.xlabel('Generation') plt.ylabel('L2 Loss') plt.show()def main(_): run()if __name__ == '__main__': tf.app.run() 这三种方式求解的线性模型，大家都熟悉一下。有什么不确定的地方，可以发邮件给我air@weaf.top。上面的方式其实在现实里面都可以使用，只不过前两种方法，在数据维度较大的时候，计算耗时。所以才会使用梯度下降的去求最优解。这次的文章就到这里。如果哪里表述不清的地方或者错误的地方，还请大家指出。","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]},{"title":"asyncio 不完全指北（四）","slug":"2018-04-30/guide-to-asyncio-4","date":"2018-05-04T14:32:16.000Z","updated":"2018-05-06T03:27:59.024Z","comments":true,"path":"posts/7eb3a479/","link":"","permalink":"http://weafteam.github.io/posts/7eb3a479/","excerpt":"","text":"书接上文。 同步原语 虽然使用 asyncio 的程序通常都以单线程运行，但仍然可以作为并发程序。每个协程或任务可以根据来自 I / O 或其他外部事件的延迟和中断以不可预测的顺序执行。为了支持安全并发，和 threading和 multiprocessing 模块一样，asyncio 包含了一些相同的低级原语的实现。 锁 锁对共享资源的访问提供了保护。只有锁的持有者才能使用资源。第二次及以上获取锁的尝试将被阻止，因此每次只有一个持有者： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import asyncioimport functoolsdef unlock(lock): print('callback releasing lock') lock.release()async def coro1(lock): print('`coro1` waiting for the lock') with await lock: print('`coro1` acquired lock') print('`coro1` released lock')async def coro2(lock): print('`coro2` waiting for the lock') await lock try: print('`coro2` acquired lock') finally: print('`coro2` released lock') lock.release()async def main(loop): # 创建并持有一个锁 lock = asyncio.Lock() print('acquiring the lock before starting coroutines') await lock.acquire() print(f'lock acquired: &#123;lock.locked()&#125;') # 安排一个回调释放锁 loop.call_later(0.1, functools.partial(unlock, lock)) # 运行希望持有锁的协程 print('waiting for coroutines') await asyncio.wait([coro1(lock), coro2(lock)])event_loop = asyncio.get_event_loop()try: event_loop.run_until_complete(main(event_loop))finally: event_loop.close() 可以使用 await 持有一个锁，并用 release() 释放，就像coro2() 的做法一样；同时也可以像 coro1() 一样，用带有 await 的异步上下文处理器来持有并释放一个锁： 12345678910acquiring the lock before starting coroutineslock acquired: Truewaiting for coroutines`coro1` waiting for the lock`coro2` waiting for the lockcallback releasing lock`coro1` acquired lock`coro1` released lock`coro2` acquired lock`coro2` released lock Event asyncio.Event 与 threading.Event 类似，用于允许多个协程等待某个事件发生，而不需要监听一个特定值来实现类似通知的功能： 123456789101112131415161718192021222324252627282930313233343536import asyncioimport functoolsdef set_event(event): print('setting event in callback') event.set()async def coro1(event): print('coro1 waiting for event') await event.wait() print('coro1 triggered')async def coro2(event): print('coro2 waiting for event') await event.wait() print('coro2 triggered')async def main(loop): event = asyncio.Event() print(f'event start state: &#123;event.is_set()&#125;') loop.call_later(0.1, functools.partial(set_event, event)) await asyncio.gather(coro1(event), coro2(event)) print(f'event end state: &#123;event.is_set()&#125;')event_loop = asyncio.get_event_loop()try: event_loop.run_until_complete(main(event_loop))finally: event_loop.close() 与锁一样，coro1() 和 coro2() 都会等待 event 被设置。不同之处在于，它们可以在 event 状态发生变化时立即启动，并且它们不需要获取 event 对象的唯一使用权： 1234567event start state: Falsecoro2 waiting for eventcoro1 waiting for eventsetting event in callbackcoro2 triggeredcoro1 triggeredevent end state: True Condition Condition的作用类似于 Event，不同之处在于，Condition 不会唤醒所有等待中的协程，唤醒的数量由 notify() 的参数控制： 1234567891011121314151617181920212223242526272829303132333435363738394041424344import asyncioasync def consumer(condition, n): with await condition: print(f'consumer &#123;n&#125; is waiting') await condition.wait() print(f'consumer &#123;n&#125; triggered') print(f'ending consumer &#123;n&#125;')async def manipulate_condition(condition): print('starting manipulate_condition') await asyncio.sleep(0.1) for i in range(1, 3): with await condition: print(f'notifying &#123;i&#125; consumers') condition.notify(n=i) await asyncio.sleep(0.1) with await condition: print('notifying remaining consumers') condition.notify_all() print('ending manipulate_condition')async def main(loop): condition = asyncio.Condition() consumers = [consumer(condition, i) for i in range(5)] loop.create_task(manipulate_condition(condition)) await asyncio.gather(*consumers)event_loop = asyncio.get_event_loop()try: result = event_loop.run_until_complete(main(event_loop))finally: event_loop.close() 这个例子中启动了五个 Condition 的消费者。每个都使用 wait() 方法等待通知它们继续的消息。manipulate_condition() 通知一个消费者，然后通知两个消费者，最后通知所有剩余的消费者： 1234567891011121314151617181920starting manipulate_conditionnotifying 1 consumersconsumer 2 is waitingconsumer 3 is waitingconsumer 0 is waitingconsumer 4 is waitingconsumer 1 is waitingnotifying 2 consumersconsumer 2 triggeredending consumer 2consumer 3 triggeredending consumer 3notifying remaining consumersending manipulate_conditionconsumer 0 triggeredending consumer 0consumer 4 triggeredending consumer 4consumer 1 triggeredending consumer 1 Queue asyncio.Queue 为协程提供了一个先进先出的数据结构，类似于与多线程中的 queue.Queue，多进程中的 multiprocessing.Queue： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import asyncioasync def consumer(n, q): print(f'consumer &#123;n&#125;: starting') while True: print(f'consumer &#123;n&#125;: waiting for item') item = await q.get() print(f'consumer &#123;n&#125;: has item &#123;item&#125;') # None 表示终止信号 if item is None: q.task_done() break else: await asyncio.sleep(0.01 * item) q.task_done() print(f'consumer &#123;n&#125;: ending')async def producer(q, num_workers): print('producer: starting') # 向队列中添加一些数据 for i in range(num_workers * 3): await q.put(i) print(f'producer: added task &#123;i&#125; to the queue') # 传入终止信号 print('producer: adding stop signals to the queue') for i in range(num_workers): await q.put(None) print('producer: waiting for queue to empty') await q.join() print('producer: ending')async def main(loop, num_consumers): # 创建指定大小的队列 # 超过队列大小时生产者会阻塞，直到有消费者取出数据 q = asyncio.Queue(maxsize=num_consumers) # 调度消费者 consumers = [loop.create_task(consumer(i, q)) for i in range(num_consumers)] # 调度生产者 prod = loop.create_task(producer(q, num_consumers)) # 等待所有任务完成 await asyncio.gather(*consumers, prod)event_loop = asyncio.get_event_loop()try: event_loop.run_until_complete(main(event_loop, 2))finally: event_loop.close() 使用 put() 添加项或使用 get() 获取并删除项都是异步操作，因为队列大小可能是固定的（阻塞添加操作），或者队列可能是空的（阻塞获取项的操作）： 123456789101112131415161718192021222324252627282930consumer 0: startingconsumer 0: waiting for itemconsumer 1: startingconsumer 1: waiting for itemproducer: startingproducer: added task 0 to the queueproducer: added task 1 to the queueconsumer 0: has item 0consumer 1: has item 1producer: added task 2 to the queueproducer: added task 3 to the queueconsumer 0: waiting for itemconsumer 0: has item 2producer: added task 4 to the queueconsumer 1: waiting for itemconsumer 1: has item 3producer: added task 5 to the queueproducer: adding stop signals to the queueconsumer 0: waiting for itemconsumer 0: has item 4consumer 1: waiting for itemconsumer 1: has item 5producer: waiting for queue to emptyconsumer 0: waiting for itemconsumer 0: has item Noneconsumer 0: endingconsumer 1: waiting for itemconsumer 1: has item Noneconsumer 1: endingproducer: ending 用 Protocol 抽象类实现异步 I / O 到目前为止，这些示例都避免了将并发和 I / O 操作混合在一起，一次只关注一个概念。但是，在 I / O 阻塞时切换上下文是 asyncio 的主要使用情形之一。在已经介绍的并发概念的基础上，本节将实现简单的 echo 服务器程序和客户端程序。客户端可以连接到服务器，发送一些数据，然后接收与响应相同的数据。每次启动 I / O 操作时，执行代码都会放弃对事件循环的控制，从而允许其他任务运行，直到 I / O 操作就绪。 Echo 服务器 服务器首先导入所需的 asyncio 和 logging 模块，然后创建事件循环对象： 1234567891011121314import asyncioimport loggingimport sysSERVER_ADDRESS = ('localhost', 10000)logging.basicConfig( level=logging.DEBUG, format='%(name)s: %(message)s', stream=sys.stderr,)log = logging.getLogger('main')event_loop = asyncio.get_event_loop() 然后定义了一个 asyncio.Protocol 的子类，用来处理与客户端的通信。Protocol 对象的方法是基于与服务器 socket 关联的事件调用的： 1class EchoServer(asyncio.Protocol): 每个新的客户端连接都会触发对 connection_made() 的调用。transport 参数是asyncio.Transport 的实例，它提供了使用 socket 进行异步 I / O 的抽象。不同类型的通信提供不同的 tansport 实现，所有这些实现都具有相同的 API。例如，有单独的 transport 类用于与 socket 通信、与子进程通过管道通信。传入客户端的地址可以通过 transport 的 get_extra_info() 获取，这是一种特定于实现的方法： 12345def connection_made(self, transport): self.transport = transport self.address = transport.get_extra_info('peername') self.log = logging.getLogger('EchoServer_&#123;&#125;_&#123;&#125;'.format(*self.address)) self.log.debug('connection accepted') 建立连接后，当数据从客户端发送到服务器时，将调用协议的 data_received() 方法将数据传入以进行处理。数据以字节串的形式传递，由应用程序以适当的方式对其进行解码。在这里记录结果，然后通过调用 transport.write() 立即将响应发送回客户端： 1234def data_received(self, data): self.log.debug('received &#123;!r&#125;'.format(data)) self.transport.write(data) self.log.debug('sent &#123;!r&#125;'.format(data)) 某些 transport 支持特殊的文件结束标识符（EOF）。遇到 EOF 时，将调用 eof_received() 方法。在这个实现中，EOF 被发送回客户端来表示它已被接收。由于并非所有 transport 都支持显式 EOF，因此 protocol 首先询问 transport 发送 EOF 是否安全： 1234def eof_received(self): self.log.debug('received EOF') if self.transport.can_write_eof(): self.transport.write_eof() 当连接关闭时，无论是正常关闭还是错误关闭，都会调用 protocol 的 connection_lost() 方法。如果发生错误，参数会包含适当的异常对象，否则为 None： 123456def connection_lost(self, error): if error: self.log.error(f'ERROR: &#123;error&#125;') else: self.log.debug('closing') super().connection_lost(error) 启动服务器有两个步骤。首先，应用程序告诉事件循环要使用的 protocol 类以及要侦听的主机名和 socket，用来创建新的服务器对象。create_server() 方法是协程，因此必须由事件循环处理结果，才能真正的启动服务器。然后，协程完成后产生了一个绑定到事件循环的 asyncio.Server 实例： 123factory = event_loop.create_server(EchoServer, *SERVER_ADDRESS)server = event_loop.run_until_complete(factory)log.debug('starting up on &#123;&#125; port &#123;&#125;'.format(*SERVER_ADDRESS)) 然后，需要运行事件循环以处理事件和客户端请求。对于长期运行的服务，run_forever() 方法是最简单的方法。当事件循环停止时，无论是通过应用程序代码还是通过发信号通知进程，服务器都可以关闭，以便正确清理 socket，然后可以关闭事件循环，以便在程序退出之前完成对任何其他事务的处理： 12345678try: event_loop.run_forever()finally: log.debug('closing server') server.close() event_loop.run_until_complete(server.wait_closed()) log.debug('closing event loop') event_loop.close() Echo 客户端 使用 protocol 类构造客户端非常类似于构造服务器。首先导入所需的 asyncio 和 logging 模块，然后创建事件循环对象： 1234567891011121314151617181920import asyncioimport functoolsimport loggingimport sysMESSAGES = [ b'This is the message. ', b'It will be sent ', b'in parts.',]SERVER_ADDRESS = ('localhost', 10000)logging.basicConfig( level=logging.DEBUG, format='%(name)s: %(message)s', stream=sys.stderr,)log = logging.getLogger('main')event_loop = asyncio.get_event_loop() 客户端 protocol 类定义了与服务器相同的方法，但实现方式不同。类构造函数接受两个参数，一个是要发送的消息列表，另一个是 future 的实例，用于通过接收来自服务器的响应来表明客户端已经完成了一个工作周期： 1234567class EchoClient(asyncio.Protocol): def __init__(self, messages, future): super().__init__() self.messages = messages self.log = logging.getLogger('EchoClient') self.f = future 当客户端成功连接到服务器时，它将立即开始通信。消息序列一次发送一条，尽管底层网络代码可以将多个消息组合成一个传输。当所有消息都用尽时，将发送 EOF。 虽然看起来数据都是立即发送的，但实际上 transport 对象缓冲传出的数据，并在当 socket 的缓冲区准备好接收数据时设置回调来进行实际的传输。所有这些都是透明处理的，因此可以编写应用程序代码，就好像 I / O 操作正在立即发生一样： 123456789101112def connection_made(self, transport): self.transport = transport self.address = transport.get_extra_info('peername') self.log.debug('connecting to &#123;&#125; port &#123;&#125;'.format(*self.address)) # 这里可以是 transport.writelines() # 但这会使显示要发送的消息的每个部分变得更加困难 for msg in self.messages: transport.write(msg) self.log.debug(f'sending &#123;msg!r&#125;') if transport.can_write_eof(): transport.write_eof() 收到来自服务器的响应时，将记录该响应： 12def data_received(self, data): self.log.debug(f'received &#123;data!r&#125;') 当从服务器端接收到 EOF 或者连接被关闭时，本地 transport 对象被关闭，并通过设置结果将 future 对象标记为完成： 123456789101112def eof_received(self): self.log.debug('received EOF') self.transport.close() if not self.f.done(): self.f.set_result(True)def connection_lost(self, exc): self.log.debug('server closed connection') self.transport.close() if not self.f.done(): self.f.set_result(True) super().connection_lost(exc) 通常，protocol 类被传递到事件循环以创建连接。在这种情况下，由于事件循环没有向 protocol 构造函数传递额外参数的工具，因此需要 functools.partial() 来包装客户端类，并传递要发送的消息列表和 future 的实例。然后，在调用 create_connection() 建立客户端连接时，将使用该新的可调用对象代替 protocol 类： 1234567891011client_completed = asyncio.Future()client_factory = functools.partial( EchoClient, messages=MESSAGES, future=client_completed,)factory_coroutine = event_loop.create_connection( client_factory, *SERVER_ADDRESS,) 为了触发客户端运行，事件循环将调用一次创建客户端的协程，然后再调用一次指定给客户端的 future 实例，以便在完成后进行通信。使用这样的两个调用避免了客户端程序中的无限循环，客户端程序可能希望在完成与服务器的通信后退出。如果仅使用第一个调用来等待协程创建客户端，那它可能无法处理所有响应数据并正确清理与服务器的连接： 1234567log.debug('waiting for client to complete')try: event_loop.run_until_complete(factory_coroutine) event_loop.run_until_complete(client_completed)finally: log.debug('closing event loop') event_loop.close() 输出 在一个窗口中运行服务器而在另一个窗口中运行客户端。 客户端将产生以下输出： 12345678910asyncio: Using selector: KqueueSelectormain: waiting for client to completeEchoClient: connecting to ::1 port 10000EchoClient: sending b'This is the message. 'EchoClient: sending b'It will be sent 'EchoClient: sending b'in parts.'EchoClient: received b'This is the message. It will be sent in parts.'EchoClient: received EOFEchoClient: server closed connectionmain: closing event loop 虽然客户端总是单独发送消息，但客户端第一次运行时，服务器会收到一条大消息，并将该消息返回给客户端。根据网络的繁忙程度以及是否在准备所有数据之前刷新网络缓冲区，这些结果在后续运行中会有所不同： 12345678asyncio: Using selector: KqueueSelectormain: starting up on localhost port 10000EchoServer_::1_55307: connection acceptedEchoServer_::1_55307: received b'This is the message. It will be sent in parts.'EchoServer_::1_55307: sent b'This is the message. It will be sent in parts.'EchoServer_::1_55307: received EOFEchoServer_::1_55307: closing 1234567EchoServer_::1_55309: connection acceptedEchoServer_::1_55309: received b'This is the message. 'EchoServer_::1_55309: sent b'This is the message. 'EchoServer_::1_55309: received b'It will be sent in parts.'EchoServer_::1_55309: sent b'It will be sent in parts.'EchoServer_::1_55309: received EOFEchoServer_::1_55309: closing","categories":[],"tags":[]},{"title":"asyncio 不完全指北（三）","slug":"2018-04-23/guide-to-asyncio-3","date":"2018-05-01T03:58:22.000Z","updated":"2018-05-06T03:27:58.998Z","comments":true,"path":"posts/c99ebd1c/","link":"","permalink":"http://weafteam.github.io/posts/c99ebd1c/","excerpt":"","text":"书接上文。 并行执行任务 任务是与事件循环交互的主要方式之一。任务包装协程并跟踪它们完成的时间。任务是 future 的子类，因此其它协程可以等待任务，并且每个任务都有一个结果，可以在任务完成后获取。 启动任务 使用 create_task() 创建任务实例。只要事件循环正在运行且协程不返回，生成的任务将作为事件循环管理的并发操作的一部分运行： 12345678910111213141516171819202122import asyncioasync def task_func(): print('in task_func') return 'the result'async def main(loop): print('creating task') task = loop.create_task(task_func()) print(f'waiting for &#123;task!r&#125;') return_value = await task print(f'task completed &#123;task!r&#125;') print(f'return value: &#123;return_value!r&#125;')event_loop = asyncio.get_event_loop()try: event_loop.run_until_complete(main(event_loop))finally: event_loop.close() main() 函数在退出前等待任务返回结果： 12345creating taskwaiting for &lt;Task pending coro=&lt;task_func() running at *.py:4&gt;&gt;in task_functask completed &lt;Task finished coro=&lt;task_func() done, defined at *.py:4&gt; result='the result'&gt;return value: 'the result' 取消任务 通过保留 create_task() 返回的任务对象，可以在任务完成之前取消其操作： 1234567891011121314151617181920212223242526272829import asyncioasync def task_func(): print('in task_func') return 'the result'async def main(loop): print('creating task') task = loop.create_task(task_func()) print('canceling task') task.cancel() print(f'canceled task &#123;task!r&#125;') try: await task except asyncio.CancelledError: print('caught error from canceled task') else: print(f'task result: &#123;task.result()!r&#125;')event_loop = asyncio.get_event_loop()try: event_loop.run_until_complete(main(event_loop))finally: event_loop.close() 在启动事件循环之前取消任务时，await task 会抛出 CancelledError 异常： 1234creating taskcanceling taskcanceled task &lt;Task cancelling coro=&lt;task_func() running at *.py:4&gt;&gt;caught error from canceled task 如果某个任务在等待另一个并发操作时被取消，则会通过在其等待的位置抛出 CancelledError 异常来通知该任务： 12345678910111213141516171819202122232425262728293031323334import asyncioasync def task_func(): print('in task_func, sleeping') try: await asyncio.sleep(1) except asyncio.CancelledError: print('task_func was canceled') raise return 'the result'def task_canceller(t): print('in task_canceller') t.cancel() print('canceled the task')async def main(loop): print('creating task') task = loop.create_task(task_func()) loop.call_soon(task_canceller, task) try: await task except asyncio.CancelledError: print('main() also sees task as canceled')event_loop = asyncio.get_event_loop()try: event_loop.run_until_complete(main(event_loop))finally: event_loop.close() 捕获该异常可以清理已完成工作： 123456creating taskin task_func, sleepingin task_cancellercanceled the tasktask_func was canceledmain() also sees task as canceled 从协程创建任务 ensure_future() 返回一个与协程的执行相关联的任务。然后，可以将该任务实例传递给其他代码，这些代码可以在不知道原始的协程是如何构造或调用的情况下等待它： 1234567891011121314151617181920212223242526272829import asyncioasync def wrapped(): print('wrapped') return 'result'async def inner(task): print('inner: starting') print(f'inner: waiting for &#123;task!r&#125;') result = await task print(f'inner: task returned &#123;result!r&#125;')async def starter(): print('starter: creating task') task = asyncio.ensure_future(wrapped()) print('starter: waiting for inner') await inner(task) print('starter: inner returned')event_loop = asyncio.get_event_loop()try: print('entering event loop') result = event_loop.run_until_complete(starter())finally: event_loop.close() 可以注意到传入 ensure_future() 的协程不会马上启动，而是直到某个地方用 await 调用了用它创建的任务： 12345678entering event loopstarter: creating taskstarter: waiting for innerinner: startinginner: waiting for &lt;Task pending coro=&lt;wrapped() running at *.py:4&gt;&gt;wrappedinner: task returned 'result'starter: inner returned 用控制结构组合协程 一系列线性执行的协程可以很方便的使用关键字 await 管理。对于复杂的控制结构，例如一个协程等待其他几个协程并行完成，也可以用 asyncio 中的工具实现。 等待多个协程 将一个操作分成许多部分并分别执行它们是很常见的场景。例如，下载多个远程资源，或查询远程 API。在执行顺序无关紧要，并且可能存在任意数量的操作的情况下，wait() 可以用于暂停一个协程，直到其他后台操作完成： 123456789101112131415161718192021222324import asyncioasync def phase(i): print(f'in phase &#123;i&#125;') await asyncio.sleep(0.1 * i) print(f'done with phase &#123;i&#125;') return f'phase &#123;i&#125; result'async def main(num_phases): print('starting main') phases = [phase(i) for i in range(num_phases)] print('waiting for phases to complete') completed, pending = await asyncio.wait(phases) results = [t.result() for t in completed] print(f'results: &#123;results!r&#125;')event_loop = asyncio.get_event_loop()try: event_loop.run_until_complete(main(3))finally: event_loop.close() 在内部，wait() 使用一个集合来保存它创建的任务实例，所以任务的执行顺序是无序的。wait() 的返回值是一个包含两个集合的元组，第一个保存了状态为 done 的任务，第二个保存了状态为 pending 的任务。 123456789starting mainwaiting for phases to completein phase 1in phase 0in phase 2done with phase 0done with phase 1done with phase 2results: ['phase 1 result', 'phase 0 result', 'phase 2 result'] 调用 wait() 时如果指定了 timeout 参数，才会出现状态为 pending 的任务： 12345678910111213141516171819202122232425262728293031323334import asyncioasync def phase(i): print(f'in phase &#123;i&#125;') try: await asyncio.sleep(0.1 * i) except asyncio.CancelledError: print(f'phase &#123;i&#125; canceled') raise else: print(f'done with phase &#123;i&#125;') return f'phase &#123;i&#125; result'async def main(num_phases): print('starting main') phases = [phase(i) for i in range(num_phases)] print('waiting 0.1 for phases to complete') completed, pending = await asyncio.wait(phases, timeout=0.1) print(f'&#123;len(completed)&#125; completed and &#123;len(pending)&#125; pending') # 取消状态为 pending 的任务 if pending: print('canceling tasks') for t in pending: t.cancel() print('exiting main')event_loop = asyncio.get_event_loop()try: event_loop.run_until_complete(main(3))finally: event_loop.close() 这些状态为 pending 的任务应被取消或者继续等待它们完成。事件循环继续运行时这些任务将继续执行，如果wait() 函数的完成被认为所有操作都已经终止了，那这样的结果是不正确的；如果在事件循环结束时仍未完成这些任务，则会生成警告。所以有必要在 wait() 函数结束后取消所有状态为 pending 的任务。 1234567891011starting mainwaiting 0.1 for phases to completein phase 0in phase 1in phase 2done with phase 01 completed and 2 pendingcanceling tasksexiting mainphase 1 canceledphase 2 canceled 收集协程的结果 如果要执行的多个协程已经被定义好，并且只关心它们的结果，那么 gather() 是一种比较好的收集结果的方法： 1234567891011121314151617181920212223242526272829303132import asyncioasync def phase1(): print('in phase1') await asyncio.sleep(2) print('done with phase1') return 'phase1 result'async def phase2(): print('in phase2') await asyncio.sleep(1) print('done with phase2') return 'phase2 result'async def main(): print('starting main') print('waiting for phases to complete') results = await asyncio.gather( phase1(), phase2(), ) print(f'results: &#123;results!r&#125;')event_loop = asyncio.get_event_loop()try: event_loop.run_until_complete(main())finally: event_loop.close() gather() 创建的任务不会公开，因此无法取消。返回值是一个结果列表，顺序与传递给 gather() 的参数的顺序相同，与实际完成的顺序无关： 1234567starting mainwaiting for phases to completein phase2in phase1done with phase2done with phase1results: [&apos;phase1 result&apos;, &apos;phase2 result&apos;] 在任务完成后做一些事 as_completed() 是一个生成器，它管理当作参数传递给它的协程列表的执行，每次迭代都会产生一个执行完的协程。与 wait() 一样，as_completed() 也不保证顺序；与 wait() 不同的是它不会等到所有后台操作完成后才可以执行其它操作： 1234567891011121314151617181920212223242526272829import asyncioasync def phase(i): print(f'in phase &#123;i&#125;') await asyncio.sleep(0.5 - (0.1 * i)) print(f'done with phase &#123;i&#125;') return f'phase &#123;i&#125; result'async def main(num_phases): print('starting main') phases = [phase(i) for i in range(num_phases)] print('waiting for phases to complete') results = [] for next_to_complete in asyncio.as_completed(phases): print(f'start &#123;next_to_complete&#125;') answer = await next_to_complete print(f'received answer &#123;answer!r&#125;') results.append(answer) print(f'results: &#123;results!r&#125;') return resultsevent_loop = asyncio.get_event_loop()try: event_loop.run_until_complete(main(3))finally: event_loop.close() 这个例子启动了几个协程，这些协程以它们开始顺序的相反顺序结束。当生成器被消耗时，循环使用 await等待协程的结果： 123456789101112starting mainwaiting for phases to completein phase 0in phase 1in phase 2done with phase 2received answer 'phase 2 result'done with phase 1received answer 'phase 1 result'done with phase 0received answer 'phase 0 result'results: ['phase 2 result', 'phase 1 result', 'phase 0 result'] 参考资料 Executing Tasks Concurrently Composing Coroutines with Control Structures","categories":[],"tags":[]},{"title":"asyncio 不完全指北（二）","slug":"2018-04-16/guide-to-asyncio-2","date":"2018-04-24T15:43:20.000Z","updated":"2018-05-06T03:27:58.968Z","comments":true,"path":"posts/84801e4e/","link":"","permalink":"http://weafteam.github.io/posts/84801e4e/","excerpt":"","text":"书接上文。 调度常规函数 除了管理协程和 I / O 回调之外，asyncio 事件循环还可以根据循环中的计时器调度常规函数。 立即调度 如果函数执行的时机无关紧要，call_soon() 可以用于在事件循环的下一次迭代中调度函数。 123456789101112131415161718192021222324import asyncioimport functoolsdef callback(arg, *, kwarg='default'): print(f'callback invoked with &#123;arg&#125; and &#123;kwarg&#125;')async def main(loop): print('registering callbacks') loop.call_soon(callback, 1) wrapped = functools.partial(callback, kwarg='not default') loop.call_soon(wrapped, 2) await asyncio.sleep(0.1)event_loop = asyncio.get_event_loop()try: print('entering event loop') event_loop.run_until_complete(main(event_loop))finally: print('closing event loop') event_loop.close() call_soon() 的第一个参数为回调函数，剩下的位置参数都会被传递给回调函数。如果想传入关键字参数，就需要用到 functools.partical()。 回调函数按照调度顺序被依次调用： 12345entering event loopregistering callbackscallback invoked with 1 and defaultcallback invoked with 2 and not defaultclosing event loop 有延迟的调度 要将回调函数的执行推迟到将来的某个时间，可以使用 call_later()。它的第一个参数是以秒为单位的延迟，第二个参数是回调函数。 1234567891011121314151617181920212223import asynciodef callback(n): print(f'callback &#123;n&#125; invoked')async def main(loop): print('registering callbacks') loop.call_later(0.2, callback, 1) loop.call_later(0.1, callback, 2) loop.call_soon(callback, 3) await asyncio.sleep(0.4)event_loop = asyncio.get_event_loop()try: print('entering event loop') event_loop.run_until_complete(main(event_loop))finally: print('closing event loop') event_loop.close() 在这个例子中，相同的回调函数参与了三次调度，分别使用了几个不同的延迟和不同的参数。最后使用了 call_soon()，它使回调函数在所有延迟调度之前调用，这表明 call_soon() 通常意味着最小延迟： 123456entering event loopregistering callbackscallback 3 invokedcallback 2 invokedcallback 1 invokedclosing event loop 在特定时间调度 还可以在特定时间调度函数。事件循环使用单调时钟，而不是 Unix 时钟，以确保当前的值永不回归。要在特定的时间调度，必须使用事件循环的time()方法。 12345678910111213141516171819202122232425262728import asyncioimport timedef callback(n, loop): print(f'callback &#123;n&#125; invoked at &#123;loop.time()&#125;')async def main(loop): now = loop.time() print(f'clock time: &#123;time.time()&#125;') print(f'loop time: &#123;now&#125;') print('registering callbacks') loop.call_at(now + 0.2, callback, 1, loop) loop.call_at(now + 0.1, callback, 2, loop) loop.call_soon(callback, 3, loop) await asyncio.sleep(1)event_loop = asyncio.get_event_loop()try: print('entering event loop') event_loop.run_until_complete(main(event_loop))finally: print('closing event loop') event_loop.close() 可以注意到事件循环内的时间与 time.time() 不一致： 12345678entering event loopclock time: 1524502404.7036376loop time: 4562.515registering callbackscallback 3 invoked at 4562.515callback 2 invoked at 4562.625callback 1 invoked at 4562.718closing event loop 异步产生结果 future是尚未完成的工作的结果。事件循环可以监视future对象的状态直到它完成，从而允许应用程序的一部分等待另一部分完成某些工作。 等待 future future就像协程，所以任何用于处理协程的方法也可以用来处理future。这个例子将future传递给事件循环的run_until_complete()方法。记住我们在上一篇中提到的，通常我们不应该自行创建 future 对象，这里只为演示。 1234567891011121314151617181920212223import asynciodef mark_done(future, result): print(f'setting future result to &#123;result!r&#125;') future.set_result(result)event_loop = asyncio.get_event_loop()try: all_done = asyncio.Future() print('scheduling mark_done') event_loop.call_soon(mark_done, all_done, 'the result') print('entering event loop') result = event_loop.run_until_complete(all_done) print(f'returned result: &#123;result!r&#125;')finally: print('closing event loop') event_loop.close()print(f'future result: &#123;all_done.result()!r&#125;') 调用 set_result() 时，future 的状态会被更改为已完成，future的实例将保存结果，并返回： 123456scheduling mark_doneentering event loopsetting future result to 'the result'returned result: 'the result'closing event loopfuture result: 'the result' future 也可以与 await 一起使用： 1234567891011121314151617181920212223import asynciodef mark_done(future, result): print(f'setting future result to &#123;result!r&#125;') future.set_result(result)async def main(loop): all_done = asyncio.Future() print('scheduling mark_done') loop.call_soon(mark_done, all_done, 'the result') result = await all_done print(f'returned result: &#123;result!r&#125;')event_loop = asyncio.get_event_loop()try: event_loop.run_until_complete(main(event_loop))finally: event_loop.close() future 的结果由 await 返回： 123scheduling mark_donesetting future result to 'the result'returned result: 'the result' future 的回调 除了像协程一样工作之外，future还可以在完成时调用回调函数。 1234567891011121314151617181920212223242526import asyncioimport functoolsdef callback(future, n): print(f'&#123;n&#125;: future done: &#123;future.result()&#125;')async def register_callbacks(all_done): print('registering callbacks on future') all_done.add_done_callback(functools.partial(callback, n=1)) all_done.add_done_callback(functools.partial(callback, n=2))async def main(all_done): await register_callbacks(all_done) print('setting result of future') all_done.set_result('the result')event_loop = asyncio.get_event_loop()try: all_done = asyncio.Future() event_loop.run_until_complete(main(all_done))finally: event_loop.close() 添加回调的函数只需要一个参数，即回调函数；回调函数也只接受一个参数，即 future 实例。若要传递其他参数给回调函数，要使用 functools.partical() 。回调函数按注册顺序被调用： 1234registering callbacks on futuresetting result of future1: future done: the result2: future done: the result 参考资料 Scheduling Calls to Regular Functions Producing Results Asynchronously","categories":[],"tags":[]},{"title":"Kafka在SpringBoot 2.0中的整合","slug":"2018-04-23/SpringBoot-integration-with-Kafka","date":"2018-04-23T10:13:02.000Z","updated":"2018-05-06T03:27:58.986Z","comments":true,"path":"posts/23949c22/","link":"","permalink":"http://weafteam.github.io/posts/23949c22/","excerpt":"","text":"一、Windows平台Kafka的环境搭建 注意：确保JAVA环境变量的正确 1.ZooKeeper的安装 Kafka的运行依赖于Zookeeper，所以需要先安装Zookeeper. Zookeeper下载地址：Zookeeper 解压出来，放在指定位置。 在conf文件夹下修改zoo_sample.cfg名为zoo.cfg 然后打开zoo.cfg 添加一下变量（如果没有请添加，存在请修改） 12dataDir=D:\\data\\logs\\zookeeper dataLogDir=D:\\data\\logs\\zookeeper 然后进入bin目录双击zkServer.cmd运行。如下图： 2.Kafka的安装 Kafka下载地址：Kafka 解压文件到指定地方 打开config下的server.properties 修改以下变量 1log.dirs=D:\\data\\logs\\kafka 我们可以看到bin目录下的是linux的启动脚本，然后有个单独的文件夹装着windows的脚本。 我们在根目录下打开命令行，运行以下命令启动Kafka。 我们在运行前需要注意以下几点 确认JAVA环境变量没有问题 路径不能有空格，不然可能会出现无法加载主类的错误。 出现无法加载主类错误，可修改bin-run-class.bat中 set COMMAND=%JAVA% %KAFKA_HEAP_OPTS% %KAFKA_JVM_PERFORMANCE_OPTS% %KAFKA_JMX_OPTS% %KAFKA_LOG4J_OPTS% -cp %CLASSPATH% %KAFKA_OPTS% %* 中“%CLASSPATH%”加上双引号 1.\\bin\\windows\\kafka-server-start.bat .\\config\\server.properties 二、SpringBoot2.0相关配置 pom文件加入以下依赖 pom.xml 123456&lt;!-- kafka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt; &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt; &lt;version&gt;2.1.5.RELEASE&lt;/version&gt; &lt;/dependency&gt; 我这里SpringBoot的配置文件使用的是YAML。 在相应环境中配置Kafka ##### application-local.yml 12345678910111213141516171819202122232425262728293031323334server: port: 7777spring: datasource: name: test driverClassName: com.mysql.jdbc.Driver url: jdbc:mysql://..... username: ... password: .... redis: database: 0 host: localhost port: 6379 jedis: pool: min-idle: 0 max-idle: 8 max-active: 8 max-wait: -1ms password: 123456 kafka: consumer: group-id: foo auto-offset-reset: earliest key-deserializer: org.apache.kafka.common.serialization.StringDeserializer value-deserializer: org.apache.kafka.common.serialization.StringDeserializer producer: key-serializer: org.apache.kafka.common.serialization.StringSerializer value-serializer: org.apache.kafka.common.serialization.StringSerializer bootstrap-servers: localhost:9092app: topic: foo: foo.t 可以仅关注spring.kafka和app.topic节点 更多spring.kafka配置信息请查看官网文档 三、代码 主要代码结构 消费者代码 12345678910111213141516171819202122package com.xxx.kafka.consumer;import lombok.extern.slf4j.Slf4j;import org.springframework.kafka.annotation.KafkaListener;import org.springframework.messaging.handler.annotation.Payload;import org.springframework.stereotype.Service;/** * @Author ：yaxuSong * @Description: * @Date: 17:56 2018/4/23 * @Modified by: */@Slf4j@Servicepublic class Receiver &#123; @KafkaListener(topics = \"$&#123;app.topic.foo&#125;\") public void listen(@Payload String message) &#123; log.info(\"received message='&#123;&#125;'\", message); &#125;&#125; 生产者代码 1234567891011121314151617181920212223242526272829package com.xxx.kafka.producer;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Value;import org.springframework.kafka.core.KafkaTemplate;import org.springframework.stereotype.Service;/** * @Author ：yaxuSong * @Description: * @Date: 17:57 2018/4/23 * @Modified by: */@Service@Slf4jpublic class Sender &#123; @Autowired private KafkaTemplate&lt;String, String&gt; kafkaTemplate; @Value(\"$&#123;app.topic.foo&#125;\") private String topic; public void send(String message)&#123; log.info(\"sending message='&#123;&#125;' to topic='&#123;&#125;'\", message, topic); kafkaTemplate.send(topic, message); &#125;&#125; 测试代码 1234567891011121314151617181920212223242526272829package com.xxx.controller;import com.xxx.controller.entry.ResMsg;import com.xxx.Sender;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;/** * @Author ：yaxuSong * @Description: * @Date: 11:21 2018/4/21 * @Modified by: */@Slf4j@RequestMapping(\"test\")@RestControllerpublic class TestController &#123; @Autowired private Sender sender; @RequestMapping(\"send\") public ResMsg send(String content)&#123; sender.send(\"Spring Kafka and Spring Boot Send Message:\"+content); return ResMsg.success(); &#125;&#125; 四、简单的测试 运行项目 测试发送 查看接收结果： 五、SpringBoot-Demo 本人最近使用阿里云的Kafka发现没有SpringBoot的Demo便写了一个。 代码地址：https://github.com/songyaxu/kafka-springboot-demo 本文参考地址https://docs.spring.io/spring-kafka/docs/2.1.5.RELEASE/reference/html/","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://weafteam.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://weafteam.github.io/tags/JAVA/"}]},{"title":"chapter-06-AIR","slug":"2018-04-16/chapter-06-AIR","date":"2018-04-22T06:31:24.000Z","updated":"2018-05-06T03:33:57.555Z","comments":true,"path":"posts/3cae9921/","link":"","permalink":"http://weafteam.github.io/posts/3cae9921/","excerpt":"","text":"TensorFlow 基础（3） hello,大家好，几天我们继续学习基础知识，为我们以后建立模型打下基础。主要是最近有点忙，所以每一周的内容会少一些，请大家谅解，随后慢慢加快进度。 这一次我们讲一下batch的概念，以及一些基本的操作，在之前的文章中，我们也讲过batch这个概念的。大家应该不会陌生 Working with Batch and Stochastic Training 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091import tensorflow as tfimport matplotlib.pyplot as pltimport numpy as npfrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()# 随机梯度训练# 生成数据x_vals = np.random.normal(1., 0.1, 100)y_vals = np.repeat(10., 100)x_data = tf.placeholder(shape = [1], dtype = tf.float32)y_target = tf.placeholder(shape = [1], dtype = tf.float32)# A就相当于权重咯A = tf.Variable(tf.random_normal(shape = [1]))my_output = tf.multiply(x_data, A)# 注意我们上一次的loss函数哦loss = tf.square(my_output - y_target)my_opt = tf.train.GradientDescentOptimizer(0.02) # 0.02就是学习率train_step = my_opt.minimize(loss)init = tf.global_variables_initializer()sess.run(init)# 开始训练模型loss_stochastic = []for i in range(100): rand_index = np.random.choice(100) # 随机选取一个样本进行训练 rand_x = [x_vals[rand_index]] rand_y = [y_vals[rand_index]] sess.run(train_step, feed_dict = &#123;x_data: rand_x, y_target: rand_y&#125;) if (i + 1) % 5 == 0: print('Step #' + str(i + 1) + ' A = ' + str(sess.run(A))) temp_loss = sess.run(loss, feed_dict = &#123;x_data: rand_x, y_target: rand_y&#125;) print('Loss = ' + str(temp_loss)) loss_stochastic.append(temp_loss) # batch train ops.reset_default_graph()sess = tf.Session()batch_size = 25x_vals = np.random.normal(1, 0.1, 100)y_vals = np.repeat(10., 100)x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32) # 看出来变化了吧？y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32) # 这里也是A = tf.Variable(tf.random_normal(shape=[1,1]))my_output = tf.matmul(x_data, A)# 是不是l2lossloss = tf.reduce_mean(tf.square(my_output - y_target))# 这里已经强调过很多遍了init = tf.global_variables_initializer()sess.run(init)# 这里是优化器my_opt = tf.train.GradientDescentOptimizer(0.02)train_step = my_opt.minimize(loss)loss_batch = []# Run Loopfor i in range(100): rand_index = np.random.choice(100, size=batch_size) # 看出来区别了么？ rand_x = np.transpose([x_vals[rand_index]]) rand_y = np.transpose([y_vals[rand_index]]) sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) if (i+1)%5==0: print('Step #' + str(i+1) + ' A = ' + str(sess.run(A))) temp_loss = sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) print('Loss = ' + str(temp_loss)) loss_batch.append(temp_loss)plt.plot(range(0, 100, 5), loss_stochastic, 'b-', label='Stochastic Loss')plt.plot(range(0, 100, 5), loss_batch, 'r--', label='Batch Loss, size=20')plt.legend(loc='upper right', prop=&#123;'size': 11&#125;)plt.show() 让你感受一些，batch训练的loss收敛： 你就说上面的你理解没理解，没理解要好好理解理解了~！！！ 这个就是结合了，把所有上面介绍的基础结合在一起，你说说是不是很棒 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# 联合所有的基础操作，弄一个分类的例子，期不期待import matplotlib.pyplot as pltimport numpy as npfrom sklearn import datasetsimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()iris = datasets.load_iris() # 将鸢尾花数据集下下来 总共是四个属性的数据集，根据四个种类，预测种类，总共三类binary_target = np.array([1. if x==0 else 0. for x in iris.target]) # 将label作为二分类问题iris_2d = np.array([[x[2], x[3]] for x in iris.data]) # 将后两个属性取出来用作种类预测batch_size = 20sess = tf.Session()x1_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)x2_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)A = tf.Variable(tf.random_normal(shape=[1, 1]))b = tf.Variable(tf.random_normal(shape=[1, 1]))my_mult = tf.matmul(x2_data, A)my_add = tf.add(my_mult, b)my_output = tf.subtract(x1_data, my_add) # 你能不能自己使用公式写出我们预测种类的公式呢？xentropy = tf.nn.sigmoid_cross_entropy_with_logits(logits = my_output, labels = y_target)my_opt = tf.train.GradientDescentOptimizer(0.05)train_step = my_opt.minimize(xentropy)init = tf.global_variables_initializer()sess.run(init)for i in range(1000): rand_index = np.random.choice(len(iris_2d), size=batch_size) #rand_x = np.transpose([iris_2d[rand_index]]) rand_x = iris_2d[rand_index] rand_x1 = np.array([[x[0]] for x in rand_x]) rand_x2 = np.array([[x[1]] for x in rand_x]) #rand_y = np.transpose([binary_target[rand_index]]) rand_y = np.array([[y] for y in binary_target[rand_index]]) sess.run(train_step, feed_dict=&#123;x1_data: rand_x1, x2_data: rand_x2, y_target: rand_y&#125;) if (i+1)%200==0: print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)) + ', b = ' + str(sess.run(b)))# Pull out slope/intercept[[slope]] = sess.run(A)[[intercept]] = sess.run(b)# Create fitted linex = np.linspace(0, 3, num=50)ablineValues = []for i in x: ablineValues.append(slope*i+intercept)# Plot the fitted line over the datasetosa_x = [a[1] for i,a in enumerate(iris_2d) if binary_target[i]==1]setosa_y = [a[0] for i,a in enumerate(iris_2d) if binary_target[i]==1]non_setosa_x = [a[1] for i,a in enumerate(iris_2d) if binary_target[i]==0]non_setosa_y = [a[0] for i,a in enumerate(iris_2d) if binary_target[i]==0]plt.plot(setosa_x, setosa_y, 'rx', ms=10, mew=2, label='setosa')plt.plot(non_setosa_x, non_setosa_y, 'ro', label='Non-setosa')plt.plot(x, ablineValues, 'b-')plt.xlim([0.0, 2.7])plt.ylim([0.0, 7.1])plt.suptitle('Linear Separator For I.setosa', fontsize=20)plt.xlabel('Petal Length')plt.ylabel('Petal Width')plt.legend(loc='lower right')plt.show() 上面是分类的结果图 验证模型： 接下来，我们会实现一个简单的回归模型和分类模型，分别做出他们的测试样例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113# 回归模型import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()batch_size = 25x_vals = np.random.normal(1, 0.1, 100)y_vals = np.repeat(10., 100)x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)# 将数据分为训练集80%和测试集20%train_indices = np.random.choice(len(x_vals), round(len(x_vals) * 0.8), replace = False)test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))x_vals_train = x_vals[train_indices]x_vals_test = x_vals[test_indices]y_vals_train = y_vals[train_indices]y_vals_test = y_vals[test_indices]# 下面这些我们是不是写了好多遍了！！！A = tf.Variable(tf.random_normal(shape=[1,1]))my_output = tf.matmul(x_data, A)# 还记得我么？你也见过我好多次了，为什么加reduce_mean? 你也知道batch可不是一个数据样本呀~loss = tf.reduce_mean(tf.square(my_output - y_target))# 创建优化器my_opt = tf.train.GradientDescentOptimizer(0.02)train_step = my_opt.minimize(loss)init = tf.global_variables_initializer()sess.run(init)# 算了，我都不想写了，你说说我们写了多少遍了，该会了for i in range(100): rand_index = np.random.choice(len(x_vals_train), size=batch_size) rand_x = np.transpose([x_vals_train[rand_index]]) rand_y = np.transpose([y_vals_train[rand_index]]) sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) if (i+1)%25==0: print('Step #' + str(i+1) + ' A = ' + str(sess.run(A))) print('Loss = ' + str(sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)))# 验证呀，一般训练出来的模型，我们要在测试集上面跑的很好，在测试集上面的准确率好了，没用，因为有过拟合的嫌疑，就是太认真了。泛化能力太差。# 验证回归模型，那就要使用loss去衡量这个模型的好坏了mse_test = sess.run(loss, feed_dict=&#123;x_data: np.transpose([x_vals_test]), y_target: np.transpose([y_vals_test])&#125;)mse_train = sess.run(loss, feed_dict=&#123;x_data: np.transpose([x_vals_train]), y_target: np.transpose([y_vals_train])&#125;)print('MSE on test:' + str(np.round(mse_test, 2)))print('MSE on train:' + str(np.round(mse_train, 2)))# 分类工作，小伙伴是不是一直觉得分类工作怎么能做，那是因为使用概率做的，也就是说，比如概率大于某一个阈值0.5 我们就分为某一类，如果小于我们就分为另一种，这是二分类，那么多分类怎么办，那么就要引入one-hot编码，这个我们后来慢慢深入。先来看例子ops.reset_default_graph()sess = tf.Session()batch_size = 25# 一毛一样x_vals = np.concatenate((np.random.normal(-1, 1, 50), np.random.normal(2, 1, 50)))y_vals = np.concatenate((np.repeat(0., 50), np.repeat(1., 50)))x_data = tf.placeholder(shape=[1, None], dtype=tf.float32)y_target = tf.placeholder(shape=[1, None], dtype=tf.float32)train_indices = np.random.choice(len(x_vals), round(len(x_vals)*0.8), replace=False)test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))x_vals_train = x_vals[train_indices]x_vals_test = x_vals[test_indices]y_vals_train = y_vals[train_indices]y_vals_test = y_vals[test_indices]A = tf.Variable(tf.random_normal(mean=10, shape=[1]))my_output = tf.add(x_data, A) # 我们直接使用了一个加法操作# 关键还是loss函数xentropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = my_output, labels = y_target))my_opt = tf.train.GradientDescentOptimizer(0.05)train_step = my_opt.minimize(xentropy)init = tf.global_variables_initializer()sess.run(init)# Run loopfor i in range(1800): rand_index = np.random.choice(len(x_vals_train), size=batch_size) rand_x = [x_vals_train[rand_index]] rand_y = [y_vals_train[rand_index]] sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) if (i + 1) % 200 == 0: print('Step #' + str(i+1) + ' A = ' + str(sess.run(A))) print('Loss = ' + str(sess.run(xentropy, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)))# 在测试集上面做测试y_prediction = tf.squeeze(tf.round(tf.nn.sigmoid(tf.add(x_data, A))))correct_prediction = tf.equal(y_prediction, y_target)accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))acc_value_test = sess.run(accuracy, feed_dict=&#123;x_data: [x_vals_test], y_target: [y_vals_test]&#125;)acc_value_train = sess.run(accuracy, feed_dict=&#123;x_data: [x_vals_train], y_target: [y_vals_train]&#125;)print('Accuracy on train set: ' + str(acc_value_train))print('Accuracy on test set: ' + str(acc_value_test))# 画出分类结果A_result = -sess.run(A)bins = np.linspace(-5, 5, 50)plt.hist(x_vals[0:50], bins, alpha=0.5, label='N(-1,1)', color='white')plt.hist(x_vals[50:100], bins[0:50], alpha=0.5, label='N(2,1)', color='red')plt.plot((A_result, A_result), (0, 8), 'k--', linewidth=3, label='A = '+ str(np.round(A_result, 2)))plt.legend(loc='upper right')plt.title('Binary Classifier, Accuracy=' + str(np.round(acc_value_test, 2)))plt.show() 总结：这次下来我们就把TensorFlow的所有基础内容讲完了，有什么问题，可以给我发邮件：air@weaf.top 希望大家把这些基础知识好好稳固一下，务必牢记于心，随后我们就会很顺利。 下一次我们就会开始一些基本算法~，基础学完不得好好练习一下么？","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]},{"title":"唯密文解密（针对Vigenere加密）","slug":"2018-04-16/唯密文解密（针对Vigenere加密）","date":"2018-04-21T01:38:12.000Z","updated":"2018-04-23T11:00:19.566Z","comments":true,"path":"posts/899ccb0/","link":"","permalink":"http://weafteam.github.io/posts/899ccb0/","excerpt":"","text":"上次说到了Vigenere加密以及解密的算法，但是如何破译这样的密码，也是很有意思的，这篇博客就是实现一个这样的破译，主要针对的是通过Vigenere加密的密文，那么就开始吧~ 任务要求： a.编程实现Vigenere加密/解密系统，并分析和评估该算法的安全性。 b.编程实现唯密文破译系统，能够破译密钥为2到4个字符的Vigenere密文，并分析如何加快破译速度。 时间要求： 布置任务后，在3周之内完成。 提交结果：已设计并测试好的程序，包括源码、可执行程序、测试数据集、实验报告。 原理介绍： 按照我们之前的说法，我们先介绍一下Caesar加密的缺点。对于一个稍微有点点密码学功底的人来说，Caesar密码的安全强度几乎为零，正如我们是上篇博客所讲的Caesar加密，加密的密钥充其量也就24个，也就是说，不管移动多少个字符，最多进行24次猜解就可以破译出来。 当然，这只是一种解密方法，也是比较笨的一种方法，而且这种方法并不适用于我们的Vigenere密码破解，因为我们没办法列举出所有的情况。 这里我们介绍破解Caesar密码的另一类方法，称为（字母）频度分析法。 假设大家都知道，英语中的字母出现概率是有差别的，其实对于一种特定的自然语言，如果文本足够长，那么各个字母出现的概率就是相对稳定的，具体的概率统计如下图所示： 这样我们根据以上的频度表，以及根据我们的Caesar密文中的统计出来的各个词的拼读，对应一下就可以找到密文对应的明文，再然后对应密文与明文就可以找到相应的加密的密钥。很简单吧，其实能想到这个想法并不简单的。 由上一篇的博客介绍，你应该知道了Vigenere密码分解之后其实就是多个Caesar密码。所以我们如果知道密钥的长度，每隔这个长度将原来的Vigenere密文分解为多个Caesar密文，再做上述的工作，是不是就完成了我们的破译工作？思路就是这样，但是怎么确定我们的密钥长度？那么我们接下来就讲讲怎么解决这个问题吧。 确定密钥长度 这个在网上搜集到的资料其实是有两种方法的。分别称为Kasiski测试法和Friedman测试法，但是本文给到的代码是基于第二个方法的，不过在此之前，我们还是先讲讲这两个方法的思路吧。 Kasiski测试法 Kasiski测试法是由Friedrich Kasiski于1863年给出了其描述，然而早在约1854年这一方法就由Charles Babbage首先发现。 它的思想是基于这样的一个事实：两个相同的明文段加密成相同的密文段，它们之间的距离为Length，那么密钥的长度就是距离Length的约数。 而当密文的长度很长时，我们便可以多找几组这样拥有重复密文段，找出他们间距的相同约数就是密钥的长度。 关于这个方法的代码，本文并未涉及，大家有兴趣可自行查阅资料实现。 另：机智的你也许发现了，我们这种方法其实并没有涉及到我们刚才说的统计频度，所以我们的重点不是这个方法，接下来就是本文的重点了。 Friedman测试法 首先我们讲一个概念：重合指数（IC，index of coincidence）。百度一下这个概念的话，搜到的结果可能不是令人很满意，我也是找了很多资料，感觉如下的概念说的很清楚，分享给大家。 重合指数表示：两个随机选出的字母是相同的概率，对于我们的英文字母来说，即以上概率即为随机选出两个A的概率+随机选出两个B的概率+….+随机选出两个Z的概率。 前人也为我们统计出了这个数字，为0.65。 即 P(A)^2 + P(B)^2 + P(C)^2 + … + P(Z)^2 = 0.65. 而利用这一概念推测密钥长度的原理为：对于一个Caesar密码的序列，由于所有字母的位移程度是一样的，所以密文的重合指数等于原文的重合指数。 将这一概念迁移到我们的Vigenere密文上，我们只要计算不同密钥长度下的重合指数，只要重合指数接近期望的0.65时，我们便可以推测当前的长度就是我们的密钥长度。 举个例子： 密文为：AAABBCCDDDDEEEFG 首先我们测试密钥长度=1，首先统计上述密文中每个字母出现的次数（A-Z）： A：3 B：2 C：2 D：4 E：3 F:1 G:1 H:0 … Z：0 然后我们根据上述公式计算重合指数P，如果 P ！= 0.65，我们就尝试密钥长度2。 假设为2的话，将上述的密文分成两组： 组1：A A B C D D E F 组2：A B C D D E E G 再分别计算重合指数，如果这两个的重合指数都接近于0.65，那么我们就可以基本确定密钥的长度为2了。如果不是，那么继续往下分。 理论上来说，我们得到的密文长度越长，通过这个方法分析得到的效果会更好，实际上在我测试的结果中，也确实符合刚才的说法。 实现 Friedman测试法确定密钥长度 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859// Friedman测试法确定密钥长度 public int Friedman(String ciphertext) &#123; int keyLength = 1; // 猜测密钥长度 double[] IC; // 重合指数 double average; // 平均重合指数 ArrayList&lt;String&gt; cipherGroup; // 密文分组 while (true) &#123; IC = new double[keyLength]; cipherGroup = new ArrayList&lt;String&gt;(); average = 0; // 1 先根据密钥长度分组 for (int i = 0; i &lt; keyLength; ++i) &#123; StringBuffer temporaryGroup = new StringBuffer(); for (int j = 0; i + j * keyLength &lt; ciphertext.length(); ++j) &#123; temporaryGroup.append(ciphertext.charAt(i + j * keyLength)); &#125; cipherGroup.add(temporaryGroup.toString()); &#125; // 2 再计算每一组的重合指数 for (int i = 0; i &lt; keyLength; ++i) &#123; String subCipher = new String(cipherGroup.get(i)); // 子串 HashMap&lt;Character, Integer&gt; occurrenceNumber = new HashMap&lt;Character, Integer&gt;(); // 字母及其出现的次数 // 2.1 初始化字母及其次数键值对 for (int h = 0; h &lt; 26; ++h) &#123; occurrenceNumber.put((char) (h + 65), 0); &#125; // 2.2 统计每个字母出现的次数 for (int j = 0; j &lt; subCipher.length(); ++j) &#123; occurrenceNumber.put(subCipher.charAt(j), occurrenceNumber.get(subCipher.charAt(j)) + 1); &#125; // 2.3 计算重合指数 double denominator = Math.pow((double) subCipher.length(), 2); for (int k = 0; k &lt; 26; ++k) &#123; double o = (double) occurrenceNumber.get((char) (k + 65)); IC[i] += o * (o - 1); &#125; IC[i] /= denominator; &#125; // 3 判断退出条件,重合指数的平均值是否大于0.065 for (int i = 0; i &lt; keyLength; ++i) &#123; average += IC[i]; &#125; average /= (double) keyLength; if (average &gt;= 0.06) &#123; break; &#125; else &#123; keyLength++; &#125; &#125; // while--end return keyLength; &#125;// Friedman--end 破译密文 这里给出来的是打印出来了具体的密钥和明文，实际上可以直接写一个类，类中设计两个属性值，一个密钥属性，一个明文属性，直接赋值下就可以了。相信机智的你可以完成这个操作。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public void decryptCipher(int keyLength, String ciphertext) &#123; int[] key = new int[keyLength]; ArrayList&lt;String&gt; cipherGroup = new ArrayList&lt;String&gt;(); double[] probability = new double[] &#123; 0.082, 0.015, 0.028, 0.043, 0.127, 0.022, 0.02, 0.061, 0.07, 0.002, 0.008, 0.04, 0.024, 0.067, 0.075, 0.019, 0.001, 0.06, 0.063, 0.091, 0.028, 0.01, 0.023, 0.001, 0.02, 0.001 &#125;; // 1 先根据密钥长度分组 for (int i = 0; i &lt; keyLength; ++i) &#123; StringBuffer temporaryGroup = new StringBuffer(); for (int j = 0; i + j * keyLength &lt; ciphertext.length(); ++j) &#123; temporaryGroup.append(ciphertext.charAt(i + j * keyLength)); &#125; cipherGroup.add(temporaryGroup.toString()); &#125; // 2 确定密钥 for (int i = 0; i &lt; keyLength; ++i) &#123; double MG; // 重合指数 int flag; // 移动位置 int g = 0; // 密文移动g个位置 HashMap&lt;Character, Integer&gt; occurrenceNumber; // 字母出现次数 String subCipher; // 子串 while (true) &#123; MG = 0; flag = 65 + g; subCipher = new String(cipherGroup.get(i)); occurrenceNumber = new HashMap&lt;Character, Integer&gt;(); // 1.1 初始化字母及其次数 for (int h = 0; h &lt; 26; ++h) &#123; occurrenceNumber.put((char) (h + 65), 0); &#125; // 1.2 统计字母出现次数 for (int j = 0; j &lt; subCipher.length(); ++j) &#123; occurrenceNumber.put(subCipher.charAt(j), occurrenceNumber.get(subCipher.charAt(j)) + 1); &#125; // 1.3 计算重合指数 for (int k = 0; k &lt; 26; ++k, ++flag) &#123; double p = probability[k]; flag = (flag == 91) ? 65 : flag; double f = (double) occurrenceNumber.get((char) flag) / subCipher.length(); MG += p * f; &#125; // 1.4 判断退出条件 if (MG &gt;= 0.055) &#123; key[i] = g; break; &#125; else &#123; ++g; &#125; &#125; // while--end &#125; // for--end // 3 打印密钥 StringBuffer keyString = new StringBuffer(); for (int i = 0; i &lt; keyLength; ++i) &#123; keyString.append((char) (key[i] + 65)); &#125; System.out.println(&quot;\\n密钥为: &quot; + keyString.toString()); // 4 解密 StringBuffer plainBuffer = new StringBuffer(); for (int i = 0; i &lt; ciphertext.length(); ++i) &#123; int keyFlag = i % keyLength; int change = (int) ciphertext.charAt(i) - 65 - key[keyFlag]; char plainLetter = (char) ((change &lt; 0 ? (change + 26) : change) + 65); plainBuffer.append(plainLetter); &#125; System.out.println(&quot;\\n明文为：\\n&quot; + plainBuffer.toString().toLowerCase()); &#125; 参考资料： 维吉尼亚密码及其破解 【密码学】维吉尼亚密码加解密原理及其破解算法Java实现 以上是本次博客的全部内容，感谢驻足~","categories":[{"name":"密码学","slug":"密码学","permalink":"http://weafteam.github.io/categories/密码学/"}],"tags":[{"name":"密码学","slug":"密码学","permalink":"http://weafteam.github.io/tags/密码学/"}]},{"title":"Redis在SpringBoot 2.0中的整合","slug":"2018-04-16/SpringBoot-integration-with-Redis","date":"2018-04-18T09:57:12.000Z","updated":"2018-05-06T03:27:58.954Z","comments":true,"path":"posts/2b2e3e2f/","link":"","permalink":"http://weafteam.github.io/posts/2b2e3e2f/","excerpt":"今天开始给大家分享Java相关的技术开发知识，在以后的开发和学习中，还希望大家多多指教，对于我发表的相关内容，如有错误，请大家指出来，一起学习。 更要记住这句话：Stay Hungry, Stay Foolish. 一、Redis的安装 为了方便教程这里先简单介绍Redis的安装。 1. windows平台的安装 现在官网已经不提供windows平台的下载，所以只能去github上下载安装 github下载网址 进入之后选择好版本点击msi下载 然后双击安装。 默认是直接运行的。 可以通过控制台访问如 具体语法可以在相关网上查阅。 ### 2. Linux平台的安装 直接到官网下载 Redis.io 解压并安装 1234wget http://download.redis.io/releases/redis-4.0.9.tar.gztar xzf redis-4.0.9.tar.gzcd redis-4.0.9make","text":"今天开始给大家分享Java相关的技术开发知识，在以后的开发和学习中，还希望大家多多指教，对于我发表的相关内容，如有错误，请大家指出来，一起学习。 更要记住这句话：Stay Hungry, Stay Foolish. 一、Redis的安装 为了方便教程这里先简单介绍Redis的安装。 1. windows平台的安装 现在官网已经不提供windows平台的下载，所以只能去github上下载安装 github下载网址 进入之后选择好版本点击msi下载 然后双击安装。 默认是直接运行的。 可以通过控制台访问如 具体语法可以在相关网上查阅。 ### 2. Linux平台的安装 直接到官网下载 Redis.io 解压并安装 1234wget http://download.redis.io/releases/redis-4.0.9.tar.gztar xzf redis-4.0.9.tar.gzcd redis-4.0.9make 服务端运行脚本 1src/redis-server 客户端运行脚本 1src/redis-cli 3. SpringBoot2.0相关配置 pom文件加入以下依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 我这里SpringBoot的配置文件使用的是YAML。 在相应环境中配置Redis ##### application-local.yml 1234567891011121314151617spring: datasource: name: test driverClassName: com.mysql.jdbc.Driver url: jdbc:mysql://127.0.0.1:3306/local?useUnicode=true&amp;characterEncoding=UTF-8 username: root password: root redis: database: 0 host: localhost port: 6379 jedis: pool: min-idle: 0 max-idle: 8 max-active: 8 max-wait: -1ms 4.代码级别配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import com.xxx.controller.entry.entity.AccessToken;import org.springframework.cache.annotation.CachingConfigurerSupport;import org.springframework.cache.annotation.EnableCaching;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.redis.cache.RedisCacheManager;import org.springframework.data.redis.connection.RedisConnectionFactory;import org.springframework.data.redis.connection.jedis.JedisConnectionFactory;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.data.redis.core.convert.KeyspaceConfiguration;import org.springframework.data.redis.repository.configuration.EnableRedisRepositories;import org.springframework.data.redis.serializer.Jackson2JsonRedisSerializer;import org.springframework.data.redis.serializer.StringRedisSerializer;import java.util.Collections;/** * @Author ：yaxuSong * @Description: * @Date: 18:35 2018/4/17 * @Modified by: */@Configuration@EnableCaching//增加Respository支持，并使其支持@TimeToLive@EnableRedisRepositories(keyspaceConfiguration = RedisCacheConfig.MyKeyspaceConfiguration.class)public class RedisCacheConfig extends CachingConfigurerSupport&#123; @Bean public RedisCacheManager cacheManager(RedisConnectionFactory connectionFactory) &#123; return RedisCacheManager.builder(connectionFactory).build(); &#125;// @Bean// public RedisConnectionFactory connectionFactory() &#123;// return new JedisConnectionFactory();// &#125; @Bean public RedisTemplate&lt;?, ?&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) &#123; RedisTemplate&lt;byte[], byte[]&gt; template = new RedisTemplate&lt;byte[], byte[]&gt;(); template.setConnectionFactory(redisConnectionFactory); return template; &#125;// @Bean// public RedisTemplate&lt;Object, Object&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory)&#123;// RedisTemplate&lt;Object, Object&gt; redisTemplate = new RedisTemplate&lt;Object, Object&gt;();// redisTemplate.setConnectionFactory(redisConnectionFactory);// redisTemplate.setKeySerializer(new StringRedisSerializer());//key序列化// redisTemplate.setValueSerializer(new Jackson2JsonRedisSerializer(Object.class)); //value序列化// redisTemplate.afterPropertiesSet();// return redisTemplate;// &#125; public static class MyKeyspaceConfiguration extends KeyspaceConfiguration &#123; @Override protected Iterable&lt;KeyspaceSettings&gt; initialConfiguration() &#123; return Collections.singleton(new KeyspaceSettings(AccessToken.class, \"accessToken\")); &#125; &#125;&#125; 缓存对象AccessToken 1234567891011121314151617181920import lombok.Data;import org.springframework.data.annotation.Id;import org.springframework.data.redis.core.RedisHash;import org.springframework.data.redis.core.TimeToLive;/** * @Author ：yaxuSong * @Description: * @Date: 14:26 2018/4/18 * @Modified by: */@RedisHash(\"accessToken\")@Datapublic class AccessToken &#123; @Id String id; String accessToken; @TimeToLive Long expire;&#125; 创建Respository 123456789101112131415import com.xxx.controller.entry.entity.AccessToken;import org.springframework.data.repository.CrudRepository;import org.springframework.stereotype.Repository;/** * @Author ：yaxuSong * @Description: * @Date: 14:34 2018/4/18 * @Modified by: */@Repository// 继承自CURD，里边有最基本的方法public interface AccessTokenRepository extends CrudRepository&lt;AccessToken, String&gt; &#123;&#125; 接下来完成自己的业务服务类 12345678910111213141516import com.xxx.controller.entry.entity.AccessToken;/** * @Author ：yaxuSong * @Description: * @Date: 14:50 2018/4/18 * @Modified by: */public interface AccessTokenService &#123; AccessToken save(AccessToken accessToken); void delete(AccessToken accessToken); AccessToken get(String id);&#125; 业务服务类的实现 123456789101112131415161718192021222324252627282930313233343536import com.xxx.controller.entry.entity.AccessToken;import com.xxx.dao.repository.AccessTokenRepository;import com.xxx.service.AccessTokenService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import java.util.Optional;/** * @Author ：yaxuSong * @Description: * @Date: 14:52 2018/4/18 * @Modified by: */@Service(\"accessTokenService\")public class AccessTokenServiceImpl implements AccessTokenService &#123; @Autowired private AccessTokenRepository repo; @Override public AccessToken save(AccessToken accessToken) &#123; return repo.save(accessToken); &#125; @Override public void delete(AccessToken accessToken) &#123; repo.delete(accessToken); &#125; @Override public AccessToken get(String id) &#123; Optional&lt;AccessToken&gt; accessToken = repo.findById(id); return accessToken.orElse(null); &#125;&#125; 以上完成了整个整合过程。 ### 5. 简单的测试 12345678910111213141516171819202122232425262728293031323334import com.xxx.controller.entry.entity.AccessToken;import com.xxx.service.AccessTokenService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;/** * @Author ：yaxuSong * @Description: * @Date: 15:16 2018/4/18 * @Modified by: */@RestController@RequestMapping(\"test\")public class TestController &#123; @Autowired private AccessTokenService accessTokenService; @RequestMapping(\"add\") public String test()&#123; AccessToken accessToken = new AccessToken(); accessToken.setAccessToken(\"dadaadadsdadewqeqfskksdbfdbkfsdkdajdhwke2elhsbcslc/DNDAWDAWWAFEWFSD23E2342\"); accessToken.setExpire(60L); //单位 秒 AccessToken at = accessTokenService.save(accessToken); return \"成功\"+\"键值为：\"+at.getId(); &#125; @RequestMapping(\"get\") public String get(String id)&#123; AccessToken accessToken = accessTokenService.get(id); return accessToken==null?\"已过期\":accessToken.toString(); &#125;&#125; 测试结果： 我这里添加了一个过期时间为60s的token。 我们通过查看可以看到时间的变化 第一次查询： 第二次查询： 第三次查询： 我们查看下本地Rdis所有键值情况： 过一段时间后查询： 我们发现之前还存在键值id为c07cde6a-aec7-40f3-ad39-41862209bc9f的，但是内容没有了。 后来查询的就被删除了（过期后不会直接删除，会稍有延迟，只有id存在，其他都已被删除） 我们看到键值为：d2b97d54-1c8b-4803-8f60-6aaf3384fc32的是我之前存的TTL=7200s的。 至此所有相关的内容就介绍完了。 本文参考地址：Spring-data-redis","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://weafteam.github.io/categories/JAVA/"}],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"http://weafteam.github.io/tags/JAVA/"}]},{"title":"Linux下MySQL安装","slug":"2018-04-09/three-method-for-install-mysql","date":"2018-04-17T04:07:12.000Z","updated":"2018-04-23T11:00:19.558Z","comments":true,"path":"posts/2cd32dfc/","link":"","permalink":"http://weafteam.github.io/posts/2cd32dfc/","excerpt":"接下来我将介绍3种方法安装MySQL 第一种 一、查看是否安装了MySQL 使用命令： 1rpm -qa|grep -i mysql 如果使用centos，可能会出现冲突，解决冲突需要卸载mariadb 首先查看是否安装了Mariadb 1rpm -qa|grep mariadb","text":"接下来我将介绍3种方法安装MySQL 第一种 一、查看是否安装了MySQL 使用命令： 1rpm -qa|grep -i mysql 如果使用centos，可能会出现冲突，解决冲突需要卸载mariadb 首先查看是否安装了Mariadb 1rpm -qa|grep mariadb 然后卸载 1rpm -e mariadb-libs-5.5.56-2.el7.x86_64 强制卸载(可选): 1rpm -e --nodeps mariadb-libs-5.5.56-2.el7.x86_64 二、如果安装了需删除已安装版本 —— 删除命令： 12rpm -e --nodeps 包名( rpm -ev mysql-4.1.12-3.RHEL4.1 ) 删除老版本mysql的开发头文件和库 命令： 12rm -fr /usr/lib/mysqlrm -fr /usr/include/mysql 注意：卸载后/var/lib/mysql中的数据及/etc/my.cnf不会删除，如果确定没用后就手工删除 12rm -f /etc/my.cnfrm -fr /var/lib/mysql 三、安装mysql准备环境 我自mysql官网下载通用的Linux版本安装包 mysql-5.7.20-linux-glibc2.12-x86_64.tar.gz 将下载好的包放在 /usr/local 目录下，或者执行命令： 12cd /usr/localwget https://cdn.mysql.com//Downloads/MySQL-5.7/mysql-5.7.20-linux-glibc2.12-x86_64.tar.gz 解压下载的文件 1tar -zxvf mysql-5.7.20-linux-glibc2.12-x86_64.tar.gz 将解压之后的所有文件移动到/usr/local/mysql 1mv ./mysql-5.7.20-linux-glibc2.12-x86_64/* ./mysql 为mysql创建系统用户(可选，新版本会自动创建相应用户) 12groupadd mysqluseradd -r -g mysql mysql //-r参数表示mysql用户是系统用户，不可用于登录系统 并变更mysql安装目录的所属用户和用户组 12chown -R mysql:mysql mysql// -R 迭代处理 四、 安装和初始化 —— 初始化数据库 1./bin/mysqld --initialize --user=mysql --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data --lc_messages_dir=/usr/local/mysql/share --lc_messages=en_US 记录刚刚输出的密码： jk9tEao&lt;94MC 配置/etc/my.cnf my.cnf 五、 启动登录和设置密码 切换到mysql安装目录的bin下启动 1./mysqld_safe --user=mysql 启动后可能无法使用当前窗口 登录进去设置新的密码： 1./mysql -u root -p 12set password=password(\"root\");flush privileges; 六、 添加到服务 切换到 support-files目录下，并执行以下命令 1cp mysql.server /etc/init.d/mysql 然后停止当前进程，使用服务启动mysql 1service mysql start 并添加mysql环境变量 在 /etc/profile 的文件末尾追加： export PATH=$PATH:/usr/local/mysql/bin 保存后执行 1source /etc/profile 最后使用新密码登录到mysql 第二种 一、查看是否安装了MySQL 使用命令： 1rpm -qa|grep -i mysql 如果使用centos，可能会出现冲突，解决冲突需要卸载mariadb 首先查看是否安装了Mariadb 1rpm -qa|grep mariadb 然后卸载 1rpm -e mariadb-libs-5.5.56-2.el7.x86_64 强制卸载(可选): 1rpm -e --nodeps mariadb-libs-5.5.56-2.el7.x86_64 二、如果安装了需删除已安装版本 —— 删除命令： 12rpm -e --nodeps 包名( rpm -ev mysql-4.1.12-3.RHEL4.1 ) 删除老版本mysql的开发头文件和库 命令： 12rm -fr /usr/lib/mysqlrm -fr /usr/include/mysql 注意：卸载后/var/lib/mysql中的数据及/etc/my.cnf不会删除，如果确定没用后就手工删除 12rm -f /etc/my.cnfrm -fr /var/lib/mysql 三、准备安装的环境 wget https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-5.7.20-1.el7.x86_64.rpm-bundle.tar 或者用自己准备好的包 mysql-5.7.20-1.el7.x86_64.rpm-bundle.tar 解压包 1tar -xvf mysql-5.7.20-1.el7.x86_64.rpm-bundle.tar 然后按照以下顺序安装 1234rpm -ivh mysql-community-common-5.7.20-1.el7.x86_64.rpm rpm -ivh mysql-community-libs-5.7.20-1.el7.x86_64.rpm rpm -ivh mysql-community-client-5.7.20-1.el7.x86_64.rpm rpm -ivh mysql-community-server-5.7.20-1.el7.x86_64.rpm 四、启动并修改密码 安装完成后就可以启动服务了 1service mysqld start 启动后查看配置文件 1vi /etc/my.cnf 找打log文件 进入查找默认root密码 1cat /var/log/mysqld.log 使用一下命令登录并修改密码 1mysql -uroot -p 修改密码 12SET PASSWORD FOR &apos;root&apos;@&apos;localhost&apos; = PASSWORD(&apos;newpass&apos;);FLUSH PRIVILEGES; 第三种 一、查看是否安装了MySQL数据库 1rpm -qa|grep mysql 卸载 1rpm -e --nodeps mysql-libs-5.1.71-1.el6.x86_64 二、安装 安装一下包 123456rpm -ivh MySQL-devel-5.6.23-1.linux_glibc2.5.x86_64.rpmrpm -ivh MySQL-client-5.6.23-1.linux_glibc2.5.x86_64.rpmrpm -ivh MySQL-server-5.6.23-1.linux_glibc2.5.x86_64.rpmrpm -ivh MySQL-embedded-5.6.23-1.linux_glibc2.5.x86_64.rpmrpm -ivh MySQL-shared-5.6.23-1.linux_glibc2.5.x86_64.rpmrpm -ivh MySQL-shared-compat-5.6.23-1.linux_glibc2.5.x86_64.rpm 三、启动登录设置密码 使用以下命令开启服务 service mysql start 获取初始密码： 使用root登录 mysql -uroot -p 然后试用一下命令设置密码 SET PASSWORD = PASSWORD(&#39;123456&#39;);","categories":[{"name":"Linux","slug":"Linux","permalink":"http://weafteam.github.io/categories/Linux/"}],"tags":[{"name":"Linux运维","slug":"Linux运维","permalink":"http://weafteam.github.io/tags/Linux运维/"}]},{"title":"chapter-05-AIR","slug":"2018-04-09/chapter-05-AIR","date":"2018-04-15T14:14:23.000Z","updated":"2018-04-23T11:00:19.554Z","comments":true,"path":"posts/7b0ee3f1/","link":"","permalink":"http://weafteam.github.io/posts/7b0ee3f1/","excerpt":"","text":"TensorFlow 基础（2） 今天有和大家见面了，今天的文章可能内容有点少，这周有很多事情，所以少写点。下一周我尽量多写点。弥补大家。那么我们今天闲话少说，直接开始今天的TensorFlow的基础介绍。接着上一节继续讲起。 Loss Functions 今天这个开头就是最常用的损失函数的实现，使用。主要涉及到两种损失函数的设计，数值预测的回归损失函数，还有分类的损失函数设计。那么我们直接开始我们的实现，有什么难点我会注释。 12345678910111213141516171819202122232425262728293031323334353637383940# 首先我们像往常一场导入我们需要的模块import tensorflow as tfimport matplotlib.pyplot as pltfrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()x_vals = tf.linspace(-1., 1., 500)target = tf.constant(0.)# l2 loss 和l2范数差一个平方根l2_y_vals = tf.square(target - x_vals)l2_y_out = sess.run(l2_y_vals)# l1 loss 就是l1范数l1_y_vals = tf.abs(target - x_vals)l1_y_out = sess.run(l1_y_vals)# Pseudo-Huber loss 为了让loss更加的光滑一些# 具体看公式一delta = tf.constant(0.25)phuber1_y_vals = tf.multiply(tf.square(delta), tf.sqrt(1. + tf.square((target - x_vals) / delta)) - 1.)phuber1_y_out = sess.run(phuber1_y_vals)delta2 = tf.constant(5.)phuber2_y_vals = tf.multiply(tf.square(delta2), tf.sqrt(1. + tf.square((target - x_vals)/delta2)) - 1.)phuber2_y_out = sess.run(phuber2_y_vals)# 画出这些回归损失函数x_array = sess.run(x_vals)plt.plot(x_array, l2_y_out, 'b-', label='L2 Loss')plt.plot(x_array, l1_y_out, 'r--', label='L1 Loss')plt.plot(x_array, phuber1_y_out, 'k-.', label='P-Huber Loss (0.25)')plt.plot(x_array, phuber2_y_out, 'g:', label='P-Huber Loss (5.0)')plt.ylim(-0.2, 0.4)plt.legend(loc='lower right', prop=&#123;'size': 11&#125;)plt.show()# 你能从后面的两个损失函数中得到什么规律呢？ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import tensorflow as tffrom tensorflow.python.framework import opsimport matplotlib.pyplot as pltops.reset_default_graph()sess = tf.Session()# Various predicted X valuesx_vals = tf.linspace(-3., 5., 500)# Target of 1.0target = tf.constant(1.)targets = tf.fill([500,], 1.)# 分类损失函数# Hinge Loss 合页损失函数# 具体请见公式二hinge_y_vals = tf.maximum(0., 1. - tf.multiply(target, x_vals))hinge_y_out = sess.run(hinge_y_vals)# 交叉熵损失xentropy_y_vals = - tf.multiply(target, tf.log(x_vals)) - tf.multiply((1. - target), tf.log(1. - x_vals))xentropy_y_out = sess.run(xentropy_y_vals)# sigmoid 交叉熵x_val_input = tf.expand_dims(x_vals, 1)target_input = tf.expand_dims(targets, 1)xentropy_sigmoid_y_vals = tf.nn.softmax_cross_entropy_with_logits(logits = x_val_input, labels = target_input)xentropy_sigmoid_y_out = sess.run(xentropy_sigmoid_y_vals)# 权重softmax 交叉熵损失函数weight = tf.constant(0.5)xentropy_weighted_y_vals = tf.nn.weighted_cross_entropy_with_logits(x_vals, targets, weight)xentropy_weighted_y_out = sess.run(xentropy_weighted_y_vals)# 画出这些损失函数x_array = sess.run(x_vals)plt.plot(x_array, hinge_y_out, 'b-', label='Hinge Loss')plt.plot(x_array, xentropy_y_out, 'r--', label='Cross Entropy Loss')plt.plot(x_array, xentropy_sigmoid_y_out, 'k-.', label='Cross Entropy Sigmoid Loss')plt.plot(x_array, xentropy_weighted_y_out, 'g:', label='Weighted Cross Entropy Loss (x0.5)')plt.ylim(-1.5, 3)#plt.xlim(-1, 3)plt.legend(loc='lower right', prop=&#123;'size': 11&#125;)plt.show()# 具体损失函数是干嘛用的，那就是为了具体的数据预测给提供一个最优化的目标，为了让每一类任务有一个最小化目标而构造出来的loss函数，在机器学习里面最重要的其实有一项就是损失函数的设计，设计一个好的损失函数，会让我们的网络更加的稳定，更加容易收敛和收敛到一个相对最优值# 没有掌握这些基本概念的，希望自己先找一些这方面的知识来看一看，然后再理解的写代码，这样会事半功倍。 公式一： \\[ L_{\\delta}(i) = {\\delta}^2 (\\sqrt{1 + (a/{\\delta})^2} - 1) \\] 公式二： \\[ max(0, 1 - (pre - y)) \\] 公式三： \\[ L = -actual * (log(pre)) - (1- actual)(log(1-pre)) \\] 公式四： \\[ L = -actual * (log(sigmoid(pre))) - (1- actual)(log(1- sigmoid(pre))) \\] 公式五： \\[ L = -actual * (log(pre)) * weights - (1-actual)(log(1-pre)) \\] Back Propagation 这个地方不要紧张，我这里给你推荐一个网站，上面有很好理解这个算法的解释。 机器学习基础以及反向传播算法介绍 12345678910111213141516171819202122232425262728293031323334353637383940# 下面是一个回归的例子# 老样子，我们创建tensorflow的会话，使用默认的计算图import tensorflow as tfimport numpy as npimport matplotlib.pyplot as pltfrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()# 一个回归的例子# 创建数据x_vals = np.random.normal(1, 0.1, 100) # x数据y_vals = np.repeat(10., 100) # y 数据x_data = tf.placeholder(shape=[1], dtype=tf.float32) # 占位符y_target = tf.placeholder(shape=[1], dtype=tf.float32) # label（真值）A = tf.Variable(tf.random_normal(shape=[1]))my_output = tf.multiply(x_data, A)# 使用l2 lossloss = tf.square(my_output - y_target)init = tf.global_variables_initializer()sess.run(init)# 创建了一个反向传播优化器my_opt = tf.train.GradientDescentOptimizer(0.02)train_step = my_opt.minimize(loss) # 最小化loss# 开始我们的迭代训练for i in range(100): rand_index = np.random.choice(100) rand_x = [x_vals[rand_index]] rand_y = [y_vals[rand_index]] sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) if (i+1)%25==0: print('Step #' + str(i+1) + ' A = ' + str(sess.run(A))) print('Loss = ' + str(sess.run(loss, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;))) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# 下面是一个分类的例子import tensorflow as tfimport numpy as npimport matplotlib.pyplot as pltfrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()# 分类的例子# 创建数据x_vals = np.concatenate((np.random.normal(-1, 1, 50), np.random.normal(3, 1, 50)))y_vals = np.concatenate((np.repeat(0., 50), np.repeat(1., 50)))# 创建占位符x_data = tf.placeholder(shape=[1], dtype=tf.float32)y_target = tf.placeholder(shape=[1], dtype=tf.float32)A = tf.Variable(tf.random_normal(mean=10, shape=[1]))my_output = tf.add(x_data, A)my_output_expanded = tf.expand_dims(my_output, 0)y_target_expanded = tf.expand_dims(y_target, 0)# 是不是使用的是对应的分类损失函数呀 sigmoid cross entropyxentropy = tf.nn.sigmoid_cross_entropy_with_logits(logits = my_output_expanded, labels = y_target_expanded)my_opt = tf.train.GradientDescentOptimizer(0.05)train_step = my_opt.minimize(xentropy)init = tf.global_variables_initializer()sess.run(init)for i in range(1400): rand_index = np.random.choice(100) rand_x = [x_vals[rand_index]] rand_y = [y_vals[rand_index]] sess.run(train_step, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;) if (i+1)%200==0: print('Step #' + str(i+1) + ' A = ' + str(sess.run(A))) print('Loss = ' + str(sess.run(xentropy, feed_dict=&#123;x_data: rand_x, y_target: rand_y&#125;)))# 测试 predictions = []for i in range(len(x_vals)): x_val = [x_vals[i]] prediction = sess.run(tf.round(tf.sigmoid(my_output)), feed_dict=&#123;x_data: x_val&#125;) predictions.append(prediction[0]) accuracy = sum(x==y for x,y in zip(predictions, y_vals))/100.print('Ending Accuracy = ' + str(np.round(accuracy, 2))) o^o,今天我们就讲到这里，下节我们再见，总的来说，就是在回归和分类问题中，设计相对应的loss函数，然后使用反向传播优化器起优化loss，使得loss逐渐减小","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]},{"title":"asyncio 不完全指北（一）","slug":"2018-04-09/guide-to-asyncio-1","date":"2018-04-14T19:37:56.000Z","updated":"2018-05-06T03:27:58.934Z","comments":true,"path":"posts/b496f296/","link":"","permalink":"http://weafteam.github.io/posts/b496f296/","excerpt":"","text":"前言 众所周知，Python 的并发编程主要由线程、进程和协程三个组件组成，我们可以使用 Python 模块 threading、multiprocessing 和 yield 句法去操纵它们。后来，又有了更高层的封装：concurrent.futures 和 asyncio 模块。concurrent.futures是对 threading 和 multiprocessing 的封装，不是这篇文章的重点；asyncio 是 Python 中的重大变化，也代表了未来的发展趋势，所以这篇文章打算讲讲 asyncio。 什么是 asyncio asyncio 一开始是 Python 的作者 Guido van Rossum 在 Python 仓库之外开发的，代号为“Tulip”，在 Python 3.4 时加入标准库。asyncio模块使用事件循环驱动的协程实现并发，提供了基于协程来构建并发程序的工具。作为对比，threading 模块通过应用级线程实现并发；multiprocessing 模块使用系统级进程实现并发；而 asyncio使用单线程、单进程，其中应用程序的各个部分在事件循环的驱动下进行协作，在最佳时间显式切换任务。asyncio 不但支持通常情况下出现阻塞型 IO 时的上下文切换，还支持调度，来让代码在指定的将来时间运行，并且还可以让一个协程等待另一个协程完成。 asyncio 中的几个概念 事件循环：asyncio 提供的框架以事件循环为中心，它是负责有效处理 I / O 事件、系统事件和应用程序上下文切换的第一类对象。Python 提供了几个循环实现，通常会自动选择合理的默认值，但也可以选择特定的事件循环实现。同样也有一些第三方的实现，例如 uvloop。应用程序将要执行的代码注册到事件循环中，代表允许事件循环在必要时对代码进行调用。当调用结束，或无法继续时，应用程序会让出控制权，交还给事件循环。 协程（coroutine ）：将控制权交还给事件循环的机制来自于 Python 的协程，这是一种特殊的函数，它将控制权交还给调用方而不会丢失本身状态。协程类似于生成器函数，事实上可以在 Python 3.5 之前的版本中用生成器实现协程。asyncio 还为 Protocols 和 Transports 提供了基于类的抽象层，使用回调的代码风格。在基于类的模型和协程模型中，通过重新进入事件循环来显式更改上下文将取代 Python 线程实现中的隐式上下文更改。 future：future是一种对象，表示待完成的操作的结果。事件循环可以监视 future 对象直到它完成，从而允许应用程序的一部分等待另一部分完成某些工作。除了 future，asyncio 还包括其他并发原语，例如锁和信号量。通常情况下，我们不应该自行创建 future，只能通过并发框架（例如 asyncio）来实例化。原因是 future代表终将发生的事情，而某件事的发生，是通过安排好这件事的执行时间来确定的。只有当我们把某件事交给事件循环处理时，事件循环才会给这件事排期，从而创建一个 future对象。 任务（Task）：任务是 future的子类，它包装并管理协程的执行。任务可以通过事件循环进行调度，以便在它们需要的资源可用时运行，并生成可由其他协程使用的结果。 使用协程处理多任务协作 协程是为并发设计的语言概念。协程函数在调用时创建协程对象，然后调用方可以使用协程的 send () 方法运行函数。协程可以在另一个协程中使用 await关键字暂停执行。当它被暂停时，协程的状态被保持，允许它在下次被唤醒时恢复到它停止的位置。 启动协程 启动一个协程最简单的方式是将一个协程传递给事件循环的 run_until_complete() 方法： 123456789101112131415import asyncioasync def coroutine(): print('in coroutine')event_loop = asyncio.get_event_loop()try: print('starting coroutine') print('entering event loop') event_loop.run_until_complete(coroutine())finally: print('closing event loop') event_loop.close() 1234starting coroutineentering event loopin coroutineclosing event loop 首先，我们通过 asyncio.get_event_loop() 获取了一个默认事件循环的引用。run_until_complete() 方法接受一个协程对象，并用它启动事件循环，然后在协程通过 return 结束时停止事件循环。 协程的返回值 协程的返回值返回给启动并等待它的程序： 1234567891011121314import asyncioasync def coroutine(): print('in coroutine') return 'result'event_loop = asyncio.get_event_loop()try: return_value = event_loop.run_until_complete(coroutine()) print(f'it returned: &#123;return_value!r&#125;')finally: event_loop.close() 12in coroutineit returned: 'result' 链式调用协程 一个协程可以启动另一个协程并等待它返回结果。下面的示例包含两个阶段，它们必须按顺序执行，但可以与另外的操作同时运行： 12345678910111213141516171819202122232425262728import asyncioasync def phase1(): print('in phase1') return 'result1'async def phase2(arg): print('in phase2') return f'result2 derived from &#123;arg&#125;'async def main(): print('in main') print('waiting for result1') result1 = await phase1() print('waiting for result2') result2 = await phase2(result1) return (result1, result2)event_loop = asyncio.get_event_loop()try: return_value = event_loop.run_until_complete(main()) print(f'return value: &#123;return_value!r&#125;')finally: event_loop.close() 123456in mainwaiting for result1in phase1waiting for result2in phase2return value: ('result1', 'result2 derived from result1') 在这里使用了 await 关键字，并没有将新的协程添加到事件循环中。因为控制流已经在由事件循环管理的协程内部，所以不需要通知事件循环管理新的协程。 使用生成器语法 async 和 await 关键字出现于 Python 3.5，对于 Python 3.5 之前的版本，可以使用 asyncio.coroutine 装饰器和 yield from 来实现相同的功能： 12345678910111213141516171819202122232425262728293031import asyncio@asyncio.coroutinedef phase1(): print('in phase1') return 'result1'@asyncio.coroutinedef phase2(arg): print('in phase2') return f'result2 derived from &#123;arg&#125;'@asyncio.coroutinedef main(): print('in main') print('waiting for result1') result1 = yield from phase1() print('waiting for result2') result2 = yield from phase2(result1) return (result1, result2)event_loop = asyncio.get_event_loop()try: return_value = event_loop.run_until_complete(main()) print(f'return value: &#123;return_value!r&#125;')finally: event_loop.close() 参考资料 Asynchronous Concurrency Concepts Cooperative Multitasking with Coroutines","categories":[],"tags":[]},{"title":"Vigenere密码加密解密","slug":"2018-04-09/Vigenere密码加密解密","date":"2018-04-14T14:11:24.000Z","updated":"2018-04-23T11:00:19.551Z","comments":true,"path":"posts/8b092926/","link":"","permalink":"http://weafteam.github.io/posts/8b092926/","excerpt":"","text":"今天换个口味，写点原来从没接触过的东西–密码学。前一阵信息安全课上留了一个作业，实现Vigenere加密解密，借着机会写篇博客。这次博客由于比较仓促，这次只写加密解密系统的实现，不涉及唯密文破解。 任务要求： a.编程实现Vigenere加密/解密系统，并分析和评估该算法的安全性。 b.编程实现唯密文破译系统，能够破译密钥为2到4个字符的Vigenere密文，并分析如何加快破译速度。 时间要求： 布置任务后，在3周之内完成。 提交结果：已设计并测试好的程序，包括源码、可执行程序、测试数据集、实验报告。 原理介绍： 先普及下Caesar密码，作为单密码简单替换密码届的扛把子，他有着不可动摇的地位，它的原理很简单，对于需要加密的每个字符都进行相同大小的平移。先给出Caesar密码加密的字符对应表，如下： 举个例子吧：明文为China，它对应的数字应为2 7 8 13 0.比如我们平移距离为3，那么加密之后的密文应该为FKLQD，简单到炸。 我们先不谈上述加密方法的缺点，这些我们放到唯密文解密中聊。有了以上的基础之后我们再聊Vigenere加密以及解密，它是使用一系列凯撒密码组成密码字母表的加密算法，属于多表密码的一种简单形式。同样先给出它的密码加密字符对应表（自己画太麻烦了，我就在百科上扒了一个图，溜。。）： 上图中的维吉尼亚表的第一列代表着密钥字母（这是有别于Caesar密码的地方），第一行代表着明文字母，行列分别使用当前需要加密字符和当前的密钥字符确定当前明文字符对应着的密文字符。 这里我们说一下，一般情况下，我们给出的密钥是短于我们的明文长度的。所以我们做Vigenere加密的时候第一步做的就是对照明文长度，补齐密钥字串。 说了这么多，举个例子说下吧： 例如我们的明文字串为：data security 密钥：best 按照上述的规则，第b行，第d列，对应的字符为E，….. 加密之后的密文应为：EELTTIUNSMLR（不区分大小写）。 实现 第一步：使得密钥字符串长度与明文长度相同 1234567891011121314151617181920public String dealKey(String str,String Key)&#123; Key=Key.toUpperCase();// 将密钥转换成大写 Key=Key.replaceAll(\"[^A-Z]\", \"\");//去除所有非字母的字符 StringBuilder stringBuilder = new StringBuilder(Key); String newKey=\"\"; if(sstringBuilder.length()!=str.length())&#123; //如果密钥长度与str不同，则需要生成密钥字符串 if(stringBuilder.length()&lt;str.length())&#123; //如果密钥长度比str短，则以不断重复密钥的方式生成密钥字符串 while(stringBuilder.length()&lt;str.length())&#123; stringBuilder.append(Key); &#125; &#125; //此时，密钥字符串的长度大于或等于str长度 //将密钥字符串截取为与str等长的字符串 newKey=stringBuilder.substring(0, str.length()); &#125; return newKey; &#125; 第二步：加密 其实我们不用将上述的Vigenere密码表列出来，一是列出来费时费力费空间，二是一个简单的取余操作就能解决这个事情。 12345678910111213141516private String PwTable = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\";public String Encryption(String P,String K)&#123; P = P.toUpperCase();// 将明文转换成大写 P = P.replaceAll(\"[^A-Z]\", \"\");//去除所有非字母的字符 K = dealKey(P,K); int len = K.length(); StringBuilder stringBuilder = new StringBuilder(); for(int i = 0;i &lt; len;i++)&#123; int row=PwTable.indexOf(K.charAt(i));//行号 int col=PwTable.indexOf(P.charAt(i));//列号 int index = (row+col)%26; stringBuilder.append(PwTable.charAt(index)); &#125; return stringBuilder.toString(); &#125; 第三步：解密 这个过程其实是比较有趣的，我们取密钥所在行为解密的行号，密文所在列作为我们的列号，我们需要分两种情况考虑： 首先说一下第一种情况，将上述的密码表从主对角线分开，一是密文在我们的密码表的右上部，这种情况比较简单，其实就是一个加密过程的逆过程，我们此时的密文字符在密码表中的位置肯定是不比其对应的明文靠后的（理解这句话，这种情况其实也就明白了，再简单点讲就是PwTable.indexof(密文)&gt;PwTable.indexof(对应的明文)），此时我们只要让列号减去行号，然后将结果做indexof操作就能得到相应的明文。 如果你理解了第一种情况，第二种情况也就好理解了，此时我们的密文字符在我们的密码表中的位置肯定是不比其对应的明文的位置靠前的，我们需要将列号加一圈字符表再去减行号。 理解之后下面的这些代码应该就不难理解了。 12345678910111213141516171819public String Decryption(String C,String K)&#123; C=C.toUpperCase();// 将密文转换成大写 C=C.replaceAll(\"[^A-Z]\", \"\");//去除所有非字母的字符 K=dealKey(C,K); int len = K.length(); StringBuilder stringBuilder=new StringBuilder(); for(int i = 0;i&lt;len;i++)&#123; int row = PwTable.indexOf(K.charAt(i));//行号 int col = PwTable.indexOf(C.charAt(i));//列号 int index; if(row&gt;col)&#123; index=col+26-row; &#125;else&#123; index=col-row; &#125; stringBuilder.append(PwTable.charAt(index)); &#125; return sb.toString(); &#125; 以上是本篇博客的全部内容，希望对你有所帮助，感谢驻足~","categories":[{"name":"密码学","slug":"密码学","permalink":"http://weafteam.github.io/categories/密码学/"}],"tags":[{"name":"密码学","slug":"密码学","permalink":"http://weafteam.github.io/tags/密码学/"}]},{"title":"如何理解丘奇计数","slug":"2018-04-02/how-to-understand-church-numerals","date":"2018-04-07T19:39:29.000Z","updated":"2018-04-23T11:00:19.548Z","comments":true,"path":"posts/cd89a3b2/","link":"","permalink":"http://weafteam.github.io/posts/cd89a3b2/","excerpt":"","text":"前言 不想写 Python 了，这次换个主题：丘奇计数，又名 lambda 演算的自然数表示法。 什么是 lambda 演算 lambda 演算（也称为 λ 演算）是数学逻辑中的一种形式系统，它基于函数抽象和应用，使用变量绑定和替换来表示计算。 没错，上面这句话来自维基百科，基本上是一句正确的废话，看完了也不知道什么是 lambda 演算。不过这篇文章的重点不在 lambda 演算上，希望你已经了解了一些关于 lambda 演算的知识。如果有机会下一篇再展开说（可能 什么是自然数 在计算机科学和集合论中，我们把非负整数 \\((0, 1, 2, 3, 4...)\\) 称为自然数。皮亚诺给出了自然数的严格定义： \\(0\\) 是自然数； 如果 \\(n\\) 是自然数，那么 \\(n+1\\) 也是自然数（\\(n+1\\) 代表 \\(n\\) 的后继）； \\(0\\) 不是任何一个数的后继； 如果 \\(m\\) 与 \\(n\\) 都是自然数且 \\(m\\neq n\\)，那么 \\(n+1 \\neq m+1\\)； 设 \\(P(n)\\) 为关于自然数 \\(n\\) 的一个性质，如果 \\(P(0)\\) 正确， 且假设 \\(P(n)\\) 正确，则 \\(P(n+1)\\) 亦正确。那么 \\(P(n)\\) 对一切自然数 \\(n\\) 都正确。 存在一个集合 \\(N\\)，称其元素为自然数，当且仅当这些元素满足公理 1 - 5（也就是皮亚诺公理）。 在自然数集合上可以定义一组运算：加法、乘法等等，这里用加法举个例子： 1234def add(m, n): if n == 0: return m return add(m, n - 1) + 1 可以看出加法是由两条规则递归定义的： $ m + 0 = m$ \\(m + (n + 1) = (m + n) + 1\\) lambda 演算的自然数表示法 自然数当然不止可以用皮亚诺公理定义，丘奇首先把自然数和自然数上的运算定义在了 lambda 演算上，所以称之为丘奇计数。 下面就要开始丘奇计数的定义了，由于 lambda 演算的样子不太友好，所以还是用 Python 表示。 首先定义 0： 1zero = lambda f: lambda x: x 先不管它为什么是 0，让我们看看这个语句。它定义了一个接受一个参数 f 的函数，返回一个函数，这个函数接受一个参数 x，然后返回它。似乎很简单，但是它为什么是 0？ 把它放在一边，看看 1 的定义： 1one = lambda f: lambda x: f(x) 这个语句是什么意思呢？定义了一个接受一个参数 f 的函数，返回一个函数，这个函数接受一个参数 x，然后返回 f(x) 的调用结果。好像有些规律了，再看看 2： 1two = lambda f: lambda x: f(f(x)) 定义了一个接受一个参数 f 的函数，返回一个函数，这个函数接受一个参数 x，然后返回 f(f(x)) 的调用结果。 现在可以清楚的看到，每个自然数的后继都多调用了一次 f，自然数被表示为 f 的调用次数。 于是，我们可以很轻易的写出后继函数： 1succ = lambda n: lambda f: lambda x: f(n(f)(x)) 这个函数接受一个参数 n（也就是上面被定义的 0，1，2 等等），返回一个函数，这个函数在 n 的基础上多执行了一次 f，达到了求 n 的后继的目的。 现在让我们忘记 1 的定义，用 0 和后继重新定义一次： 12345678zero = lambda f: lambda x: xsucc = lambda n: lambda f: lambda x: f(n(f)(x))one = succ(zero) = (lambda n: lambda f: lambda x: f(n(f)(x)))(lambda f: lambda x: x) = lambda f: lambda x: f(((lambda f: lambda x: x)(f))(x)) = lambda f: lambda x: f((lambda x: x)(x)) = lambda f: lambda x: f(x) 和我们预想的完全一致。 加法 接下来试着定义一下加法，加法是两个数相加返回一个数（也就是说，加法是定义在自然数上的幺半群），所以签名长这样： 1add = lambda m: lambda n: lambda f: lambda x: ... 函数体呢？我们推广一下后继函数：后继函数在n的基础上多调用了一次 f，相当于 +1；那加法相当于 +m，也就是多调用 m 次 f，于是可以得出： 1add = lambda m: lambda n: lambda f: lambda x: m(f)(n(f)(x)) 乘法 乘法的签名也是一样： 1mul = lambda m: lambda n: lambda f: lambda x: ... 我们都知道乘法是从加法推广而来的：m * n 相当于加 m 次 n，所以可以使用加法的定义： 1mul = lambda m: lambda n: lambda f: lambda x: m(add(n))(zero)(f)(x) 上面的写法正确，不过太丑了，可以化简为： 1mul = lambda m: lambda n: lambda f: lambda x: m(n(f))(x) 求幂 求幂的签名也一样： 1pow = lambda m: lambda n: lambda f: lambda x: ... 求幂是由乘法推广而来的：mn 相当于乘 n 次 m，所以可以使用乘法的定义： 1pow = lambda m: lambda n: lambda f: lambda x: n(mul(m))(one)(f)(x) 同样可以化简为： 1pow = lambda m: lambda n: lambda f: lambda x: n(m)(f)(x) 结语 机智的同学一定发现我们并没有实现减法，这是因为减法的实现太复杂了。至于为什么减法的实现很复杂，以及如何实现减法，这里有一篇参考资料 ，有兴趣的话可以自行了解一下。","categories":[],"tags":[]},{"title":"chapter-04-AIR","slug":"2018-04-02/chapter-04-AIR","date":"2018-04-05T02:13:07.000Z","updated":"2018-04-23T11:00:19.546Z","comments":true,"path":"posts/466eca41/","link":"","permalink":"http://weafteam.github.io/posts/466eca41/","excerpt":"","text":"TensorFLow 基础（1） hi,又和大家见面了，上一次我们讲了建立模型步骤和一些基础的概念（Tensor、Placeholder），那么我们这次就继续我们的矩阵操作，因为在TensorFlow处理一些数学问题的时候，往往都是通过矩阵来存储数据，通过特定的矩阵运算，我们实现数据的处理，从而得到一些数据的特性。还有一些其他的Tensorflow的概念，我希望大家能坚持下去，只要将这些基础的概念学会，那么以后运用TensorFlow就会得心应手。 和TensorFlow一起工作的Matrices 12345678910111213141516171819202122232425262728293031# 矩阵和矩阵操作import tensorflow as tfimport numpy as npfrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()identity_matrix = tf.diag([1., 1., 1])print(sess.run(identity_matrix)) #注意这个地方，如果是TensorFlow的Tensor，# 那么使用sess的run方法才能将结果显示# 或者下面这种方式print(identity_matrix.eval(session = sess))A = tf.truncated_normal([2, 3]) # 2 * 3 大小 均值0 方差为1.print(sess.run(A))B = tf.fill([2, 3], 5.) # 2 * 3 使用5.填充print(sess.run(B))C = tf.random_uniform([3, 2]) # 3 * 2 随机初始化print(sess.run(C))D = tf.convert_to_tensor(np.array([[1., 2., 3.], [-3., -7., -1.], [0., 5., -2.]]))print(sess.run(D))print(sess.run(A+B)) # 加法和 tf.add() 一样print(sess.run(B-B)) # 减法和 tf.subtract() 一样print(sess.run(tf.matmul(B, identity_matrix))) # 矩阵乘法print(sess.run(tf.transpose(C))) # 矩阵转置print(sess.run(tf.matrix_determinant(D))) # 计算行列式print(sess.run(tf.matrix_inverse(D))) # 矩阵的逆print(sess.run(tf.cholesky(identity_matrix))) # cholesk分解（平方根分解）eigenvalues, eigenvectors = sess.run(tf.self_adjoint_eig(D)) # 求特征向量和特征值print(eigenvalues)print(eigenvectors) Math Operation（数学操作） 123456789101112131415161718192021222324252627282930import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()# math operationprint(sess.run(tf.div(3, 4)))print(sess.run(tf.truediv(3, 4)))print(sess.run(tf.floordiv(3., 4.)))print(sess.run(tf.mod(22., 5.)))print(sess.run(tf.cross([1., 0., 0.], [0., 1., 0.])))# Trig operationprint(sess.run(tf.sin(3.1416)))print(sess.run(tf.cos(3.1416)))print(sess.run(tf.div(tf.sin(3.1416 / 4.), tf.cos(3.1416 / 4.))))# custom operation# f(x) = 3 * x^2 - X + 10test_nums = range(15) # 生成一个listdef custom_polynomial(x_val): return (tf.subtract(3 * tf.square(x_val), x_val) + 10)print(sess.run(custom_polynomial(11)))# list expendexpected_output = [3 * x * x - x + 10 for x in test_nums]print(expected_output)for num in test_nums: print(sess.run(custom_polynomial(num))) Activation Function（激活函数） 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 激活函数主要是为了让神经网络模型具有非线性的特性import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()x_vals = np.linspace(start = -10, stop = 10, num = 100)# Relu Activation-&gt; max(0, x)print(sess.run(tf.nn.relu([-3., 3., 10.])))y_relu = sess.run(tf.nn.relu(x_vals))# Relu6 Activation-&gt; min(max(x, 0), 6)print(sess.run(tf.nn.relu6([-3., 3., 10.])))y_relu6 = sess.run(tf.nn.relu6(x_vals))# Sigmoidactivation-&gt; 见公式1print(sess.run(tf.nn.sigmoid([-1., 0., 1.])))y_sigmoid = sess.run(tf.nn.sigmoid(x_vals))# Hyper Tangent activation-&gt;见公式2print(sess.run(tf.nn.tanh([-1., 0., 1.])))y_tanh = sess.run(tf.nn.tanh(x_vals))# softsign activation-&gt;见公式3print(sess.run(tf.nn.softsign([-1., 0., 1.])))y_softsign = sess.run(tf.nn.softsign(x_vals))# softplus activation-&gt;见公式4print(sess.run(tf.nn.softplus([-1., 0., 1.])))y_softplus = sess.run(tf.nn.softplus(x_vals))# Exponential linear activation-&gt;见公式5print(sess.run(tf.nn.elu([-1., 0., 1.])))y_elu = sess.run(tf.nn.elu(x_vals))plt.plot(x_vals, y_softplus, 'r--', label='Softplus', linewidth=2)plt.plot(x_vals, y_relu, 'b:', label='ReLU', linewidth=2)plt.plot(x_vals, y_relu6, 'g-.', label='ReLU6', linewidth=2)plt.plot(x_vals, y_elu, 'k-', label='ExpLU', linewidth=0.5)plt.ylim([-1.5,7])plt.legend(loc='upper left')plt.show()plt.plot(x_vals, y_sigmoid, 'r--', label='Sigmoid', linewidth=2)plt.plot(x_vals, y_tanh, 'b:', label='Tanh', linewidth=2)plt.plot(x_vals, y_softsign, 'g-.', label='Softsign', linewidth=2)plt.ylim([-2,2])plt.legend(loc='upper left')plt.show()下图给出激活函数的曲线图 公式1： \\[ \\sigma(x)=\\frac{1}{1+e^{-x}} \\] 公式2： \\[ f(x)=\\frac{e^x-e^{-x}}{e^x + e^{-x}} \\] 公式3： \\[ f(x) = \\frac{1}{1 + |x|} \\] 公式4 \\[ f(x) = \\log(1 + e^x) \\] 公式5： \\[ elu(x) = \\begin{cases} x &amp; x &gt; 0 \\\\ \\alpha(exp(x) - 1) &amp; x \\leq 0 \\end{cases} \\] Operations on a Computational Graph 1234567891011121314151617181920212223242526272829import osimport matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()# 创建的数据是要喂给下面的placeholder的x_vals = np.array([1., 3., 5., 7., 9.])# 创建placeholderx_data = tf.placeholder(tf.float32)# 创建一个乘数m = tf.constant(3.)# 乘法prod = tf.multiply(x_data, m)for x_val in x_vals: print(sess.run(prod, feed_dict = &#123;x_data: x_val&#125;))#下面将数据计算图输出到文件里面，供我们后来启动tensorboard使用merged = tf.summary.merge_all(key = 'summary')if not os.path.exists('tensorboard_logs/'): os.makedirs('tensorboard_logs/')my_writer = tf.summary.FileWriter('./tensorboard_logs/', sess.graph) Layering Nested Operations 123456789101112131415161718192021222324252627282930313233343536373839404142import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tfimport osfrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()# 创建数据为了feedmy_array = np.array([[1., 3., 5., 7., 9.], [-2., 0., 2., 4., 6.], [-6., -3., 0., 3., 6.]])# 复制x_vals = np.array([my_array, my_array + 1])# 声明placeholderx_data = tf.placeholder(tf.float32, shape = [3, 5])# 声明常数来操作m1 = tf.constant([[1.], [0.], [-1.], [2.], [4]])m2 = tf.constant([[2.]])a1 = tf.constant([[10.]])# 声明操作prod1 = tf.matmul(x_data, m1)prod2 = tf.matmul(prod1, m2)add1 = tf.matmul(prod2, a1)# 打印验证结果for x_val in x_vals: print(sess.run(add1, feed_dict = &#123;x_data: x_val&#125;)) #下面将数据计算图输出到文件里面，供我们后来启动tensorboard使用merged = tf.summary.merge_all(key = 'summaries')if not os.path.exists('tensorboard_logs/'): os.makedirs('tensorflow_logs/')my_writer = tf.summary.FileWriter('tensorboard_logs/', sess.graph)#下图就是在操作过程中，tensorflow建立的图运算模型 Working With Multiple Layers 123456789101112131415161718192021222324252627282930313233343536373839404142434445import matplotlib.pyplot as pltimport numpy as npimport tensorflow as tfimport osfrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()x_shape = [1, 4, 4, 1]# 定义一个4 * 4 大小的随机矩阵x_val = np.randim.uniform(size = x_shape)x_data = tf.placeholder(tf.float32, shape = x_shape)# 定义一个空间移动窗口，也就是卷积操作的卷积核# 大小是2 * 2， 步长是 2# filter的值是一个固定的值0.25my_filter = tf.constant(0.25, shape = [1, 2, 2, 1])my_strides = [1, 2, 2, 1]mov_avg_layer = tf.nn.conv2d(x_data, my_filter, my_strides, padding = 'SAME', name = 'Moving_Avg_Window')# 第二层def custom_layer(input_matrix): input_matrix_sqeezed = tf.squeeze(input_matrix) A = tf.constant([1., 2.], [-1., 3.]) b = tf.constant(1., shape = [2, 2]) output = tf.add(tf.matmul(A, input_matrix_sqeezed), b) return tf.nn.relu(output)with tf.name_scope('custom_layer') as scope: custom_layer1 = custom_layer(mov_avg_layer)# 运行结果print(sess.run(mov_avg_layer, feed_dict = &#123;x_data: x_val&#125;))print(sess.run(custom_layer1, feed_dict = &#123;x_data: x_val&#125;))#下面将数据计算图输出到文件里面，供我们后来启动tensorboard使用merged = tf.summary.merge_all(key = 'summaries')if not os.path.exists('tensorboard_logs/'): os.makedirs('tensorboard_logs/')my_writer = tf.summary.FileWriter('tensorboard_logs', sess.graph)# 下图是计算图 总结：这一次，一开始主要讲了矩阵的一些操作，后续又进行了数学操作，激活函数，运算图、层内元素嵌套运算还有最好的多层运算，并给出了tensorboard的计算图结构。大家不仅仅要看一看，也要动手做一做哦。 一如既往的有什么问题可以直接联系milittle，air@weaf.top邮箱","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]},{"title":"Nginx和Git的离线安装","slug":"2018-03-26/offline-install-git-and-nginx","date":"2018-04-01T04:07:12.000Z","updated":"2018-04-23T11:00:19.540Z","comments":true,"path":"posts/a86291b8/","link":"","permalink":"http://weafteam.github.io/posts/a86291b8/","excerpt":"一、准备工作 一般情况下为了确保安装没有任何问题，我们先使用有网络环境的下安装的方法，去检测当前机器具体需要安装什么冬天链接库，然后按照提示缺失的库去下载相应的库。 我们按照正常的流程，去解压nginx 1tar -zxvf nginx-1.13.8.tar.gz 进入解压后的目录执行 12cd nginx-1.13.8./configure 出现以下错误： 我们按照有网络环境的方法去检测缺失的库及其版本。 1yum -y install gcc gcc-c++ autoconf automake make","text":"一、准备工作 一般情况下为了确保安装没有任何问题，我们先使用有网络环境的下安装的方法，去检测当前机器具体需要安装什么冬天链接库，然后按照提示缺失的库去下载相应的库。 我们按照正常的流程，去解压nginx 1tar -zxvf nginx-1.13.8.tar.gz 进入解压后的目录执行 12cd nginx-1.13.8./configure 出现以下错误： 我们按照有网络环境的方法去检测缺失的库及其版本。 1yum -y install gcc gcc-c++ autoconf automake make 显示如下： 我们下载号相应的库 下边提供几个下载的网址： http://mirrors.163.com/centos/6/os/x86_64/Packages/ http://rpmfind.net/ https://pkgs.org 下边是下载好的库 安装相应的库（集体安装情况具体分析）： 12345678910111213rpm -ivh mpfr-2.4.1-6.el6.x86_64.rpmrpm -ivh cpp-4.4.7-18.el6.x86_64.rpmrpm -Uvh tzdata-2016j-1.el6.noarch.rpmrpm -Uvh glibc-common-2.12-1.209.el6.x86_64.rpm glibc-2.12-1.209.el6.x86_64.rpm glibc-headers-2.12-1.209.el6.x86_64.rpm glibc-devel-2.12-1.209.el6.x86_64.rpm kernel-headers-2.6.32-696.el6.x86_64.rpmrpm -ivh libgomp-4.4.7-18.el6.x86_64.rpmrpm -Uvh libstdc++-4.4.7-18.el6.x86_64.rpmrpm -ivh libstdc++-devel-4.4.7-18.el6.x86_64.rpmrpm -ivh ppl-0.10.2-11.el6.x86_64.rpmrpm -ivh cloog-ppl-0.15.7-1.2.el6.x86_64.rpmrpm -Uvh libgcc-4.4.7-18.el6.x86_64.rpmrpm -ivh gcc-4.4.7-18.el6.x86_64.rpmrpm -ivh gcc-c++-4.4.7-18.el6.x86_64.rpmrpm -ivh automake-1.11.1-4.el6.noarch.rpm autoconf-2.63-5.1.el6.noarch.rpm 然后执行./configure 发现错误： 然后安装一下库 1234567rpm -Uvh pcre-7.8-7.el6.x86_64.rpmrpm -ivh pcre-devel-7.8-7.el6.x86_64.rpmrpm -Uvh zlib-1.2.3-29.el6.x86_64.rpmrpm -ivh zlib-devel-1.2.3-29.el6.x86_64.rpmrpm -i --force --nodeps krb5-devel-1.10.3-65.el6.x86_64.rpmrpm -Uvh openssl-1.0.1e-57.el6.x86_64.rpmrpm -ivh openssl-devel-1.0.1e-57.el6.x86_64.rpm 然后执行 123./configuremakemake install 安装完成 二、查看版本信息 ===== 根据安装完成的信息查看nginx. 三、简介 —– 不同操作系统的Linux的安装可能不太一样。 本教程使用的是CentOS或者RHEL。 四、准备环境 —- 如果有网络的情况下肯定相当容易： 123yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel gcc perl-ExtUtils-MakeMakeryum install git 若果yum源安装的版本较低，不执行yum install git命令，并按照四、五步骤操作。 如果是离线需要完成后续操作： 首先我们需要到官网下载相应的安装包 https://www.kernel.org/pub/software/scm/git/ 选择tar.gz 三、下载和安装依赖 —— 然后最麻烦的地方就是依赖动态链接库的下载。 需要下载的库可以到这两个网站上去找： fr2.rpmfind.net Linux Packages Search-pkgs.org 我这里提供了一些 1234567891011121314151617181920212223cpio-2.11-24.el7.x86_64.rpmcurl-7.29.0-42.el7.x86_64.rpmexpat-2.1.0-10.el7_3.x86_64.rpmexpat-devel-2.1.0-10.el7_3.x86_64.rpmgdbm-devel-1.10-8.el7.x86_64.rpmgettext-0.19.8.1-2.el7.x86_64.rpmgettext-devel-0.19.8.1-2.el7.x86_64.rpmkrb5-devel-1.15.1-8.el7.x86_64.rpmlibcurl-7.29.0-42.el7.x86_64.rpmlibcurl-devel-7.29.0-42.el7.x86_64.rpmlibdb-devel-5.3.21-20.el7.x86_64.rpmopenssl-1.0.2k-8.el7.x86_64.rpmopenssl-devel-1.0.2k-8.el7.x86_64.rpmperl-5.16.3-292.el7.x86_64.rpmperl-devel-5.16.3-292.el7.x86_64.rpmperl-ExtUtils-CBuilder-0.28.2.6-292.el7.noarch.rpmperl-ExtUtils-Install-1.58-292.el7.noarch.rpmperl-ExtUtils-MakeMaker-6.68-3.el7.noarch.rpmperl-ExtUtils-Manifest-1.61-244.el7.noarch.rpmperl-ExtUtils-ParseXS-3.18-3.el7.noarch.rpmsystemtap-sdt-devel-3.1-3.el7.x86_64.rpmzlib-1.2.7-17.el7.x86_64.rpmzlib-devel-1.2.7-17.el7.x86_64.rpm 这是git需要的一些库，需要安装的不是很多，但是安装的库也需要依赖。 123456789rpm -ivh perl-5.16.3-292.el7.x86_64.rpmrpm -ivh perl-devel-5.16.3-292.el7.x86_64.rpmrpm -ivh zlib-devel-1.2.7-17.el7.x86_64.rpmrpm -ivh libcurl-devel-7.29.0-42.el7.x86_64.rpmrpm -ivh curl-7.29.0-42.el7.x86_64.rpmrpm -ivh zlib-devel-1.2.7-17.el7.x86_64.rpmrpm -ivh openssl-devel-1.0.2k-8.el7.x86_64.rpmrpm -ivh perl-ExtUtils-MakeMaker-6.68-3.el7.noarch.rpmrpm -ivh gettext-devel-0.19.8.1-2.el7.x86_64.rpm 以上库需要安装，并需要安装对应依赖。 有时候有些包可能互相依赖，安装时可使用一下命令 1rpm -ivh perl-ExtUtils-MakeMaker-6.68-3.el7.noarch.rpm perl-ExtUtils-Install-1.58-292.el7.noarch.rpm zlib-devel-1.2.7-17.el7.x86_64.rpm 编译安装可能需要的包 12345678910rpm -ivh cloog-ppl-0.15.7-1.2.el6.x86_64.rpmrpm -ivh cpp-4.4.7-18.el6.x86_64.rpmrpm -ivh gcc-4.4.7-18.el6.x86_64.rpmrpm -ivh gcc-c++-4.4.7-18.el6.x86_64.rpmrpm -ivh libgcc-4.4.7-18.el6.x86_64.rpmrpm -ivh libgomp-4.4.7-18.el6.x86_64.rpmrpm -ivh libstdc++-4.4.7-18.el6.x86_64.rpmrpm -ivh libstdc++-devel-4.4.7-18.el6.x86_64.rpmrpm -ivh mpfr-2.4.1-6.el6.x86_64.rpmrpm -ivh ppl-0.10.2-11.el6.x86_64.rpm 有时会发生冲突可以使用枪支卸载，或者不考虑依赖安装。 12345rpm -e --nodeps mariadb-libs-5.5.56-2.el7.x86_64 //强制卸载rpm -i --force --nodeps krb5-devel-1.15.1-8.el7.x86_64.rpm //强制安装 --force可选 五、解压和安装 解压安装包 1tar -zxvf git-2.15.1.tar.gz 进入解压后的文件夹 执行一下命令 1./configure 检查没有任何出错 然后执行以下命令进行编译 1make 检查没有问题执行安装 1make install 检查没有任何出错 六、查看安装结果 —- 1git --version","categories":[{"name":"Linux","slug":"Linux","permalink":"http://weafteam.github.io/categories/Linux/"}],"tags":[{"name":"Linux运维","slug":"Linux运维","permalink":"http://weafteam.github.io/tags/Linux运维/"}]},{"title":"TensorFlow 建立网络模型","slug":"2018-03-26/chapter-03-AIR","date":"2018-03-31T10:23:35.000Z","updated":"2018-04-23T11:00:19.538Z","comments":true,"path":"posts/5b7df854/","link":"","permalink":"http://weafteam.github.io/posts/5b7df854/","excerpt":"","text":"TensorFlow 建立网络模型 上次一我们在fashion-mnist上面体验了一把，但是里面有一些建立模型和一些TensorFlow的基础概念都没有给大家讲，所以这节决定将这方面的知识介绍一些，上节是为了引起大家的注意，TensorFlow具有很强大的功能，我们只能后续慢慢的学习。 其实在上一次的实例中，有很多地方确实是很困惑的，如果没有接触过机器学习的小伙伴可能理解起来会有一些问题，那么我开头就稍微讲一下，机器学习有一些什么？就我现在了解的一些内容给大家介绍，有可能有一些不到位的地方，还请多多包涵： 其实机器学习，总的宗旨就是利用数据的特征来做识别和分类等任务 第一大类是分类工作，假设有一百类，经典的做法，就是使用神经网络提取一些数据的特征，然后利用softmax输出层进行不同种类概率的预测： \\[ softmax(i) = \\frac{X_i}{\\sum_{i=0,99}X_i} \\] 上面是softmax层计算的公式，从一百类里面找出每一类的概率值，然后按照概率值来预测输入数据是哪一种类型，就像上一次文章里面的fashion-mnist的数据一样，会预测出输出的类别。softmax(i)代表的就是这个种类的概率值，取最大值作为预测类别。 你可以把一个矩阵看成一个数据集合，一行是一个数据信息，就和我们的关系型数据一样，一行代表一个表的一条信息，那么每列就是每一行数据的一个属性，那么在机器学习里面就是数据的特征了，因为在网络模型中，每个特征都有对应的权重，那么，对于每个特征来说，对于最后的分类，识别等工作起的重要程度是不一样的。这也和我们的数据库信息差不多，有一些信息也是无关紧要的。有些信息可以主要决定这一行数据。 第二大类就是回归，回归可以看作是一个连续的分类，对于二维数据来说，其实就是根据你给出的数据来拟合一条线。对于三维来讲就是拟合一个平面。再高维就是超平面。 最近，也就是2018年3月31在加利福尼亚州山景城的计算机历史博物馆举办了第二届TensorFlow开发者峰会，会上有超过500名使用TensorFlow的用户，还有一些观众，大家有兴趣的话可以关注youtube的TensorFlow官方频道。可以查看开会的视频。 TensorFlow应用广泛，其中有使用TensorFlow来做开普勒任务分析的 也有使用TensorFlow预测心脏发作和中风概率 还有一些应用在现实当中的项目。 这让我们认识到TensorFlow对于实际领域中应用的越来越广泛，所以我们不学习是不是有点亏。这么好的开源项目。 上一次我们既然做过了一次服装类别识别，那么这次我主要从TensorFlow建立模型的步骤讲起：让大家再深入理解一下TensorFlow。 第一步也是很重要的一步，那就是导入数据。 第二步一般就是对数据进行的预处理，一般包括归一化数据，转换数据等操作。 第三步设置算法的超参数，一般也就是学习率，batch_size(批处理个数)，epoch(轮次)。这里举一个例子，假如你有10000条训练数据。那么，batch_size设置为100，那么你的一个epoch就迭代100次才能将所有数据训练一遍，每次输入数据是100条，因为一个epoch的意思就是训练完一次训练数据，所以一个epoch是迭代100次就可以结束一轮了。learning_rate一般设置为0.1-0.0001之间，但是也不排除一些特殊情况，主要是learning_rate设置的过小，反向传播更新参数的时候速度会很慢，设置的过大，会出现无法收敛的情况。 第四步设置变量和placeholders，变量是记录权重和偏置项信息的，一般在最小化loss函数的时候，反向传播算法会更新权重和偏置项，TensorFlow导入数据是通过placeholders来实现的，大家还记得我们上次的fashion-mnist识别，我的数据就是通过先定义placeholders，最后在Session运行的时候，在feed_dict这个字典参数里面将训练数据喂进去的。 1234a_var = tf.constant(42)x_input = tf.placeholder(tf.float32, [n_x, None], name=\"X\")x_output = tf.placeholder(tf.float32, [n_y, None], name=\"y\")# 定义输入数据的一些方式 第五步定义图模型，我们有了数据，初始化了变量和placeholders，那我们就需要定义一个图模型，来生成TensorFlow的图模型（计算图）我们必须告诉TensorFlow对我们的数据进行哪些操作，来让我们的模型具有预测能力（更加深入的运算我们在后续的博客里面会陆续讲到） 1h_pre_output = tf.add(tf.matmul(W, x_input) + B) 第六步声明loss函数，在上面计算图中我们定义了一些对我们数据的操作。那么我们需要验证我们预测的输出，和我们真实之间的差距，一般对于回归任务来讲的话，就是平方误差：这样就求得了平方误差。但是对于分类任务，那就是交叉熵误差。就像上一节我们用到的loss生成函数就是softmax这种方式。，交叉熵的公式后续用到再给大家介绍。 \\[ loss(i)=\\frac{1}{N}\\sum{_i}(y\\_pre_i-y\\_true_i)^2 \\] 12TensorFlow求法：loss = tf.reduce_mean(tf.square(y_pre - y_true)) 第七步声明了loss函数以后，我们需要使用BP算法也就是反向传播算法来更新权重和偏置项。在TenorFlow框架里面有好多这样的优化器，都在 tf.train这个模块里面。 12optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)这个就是我们上次使用的优化器，来优化我们的loss 最后一步那就是初始化会话Session()，开始训练模型 123with tf.Session() as session: session.run(init) ..... 由上面的步骤，大家再结合上一次的网络代码，是不是可以理解了TensorFlow在建立一个网络模型的时候的具体步骤。 其实在TensorFlow中还有一个很重要的概念，那就是Tensor，上次说过了它的概念，那么接下来我讲一下TensorFlow里面的Tensor。 1234567891011121314151617181920212223242526import tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()# 定义一个会话，记得，TensorFlow里面都是通过session来执行的sess = tf.Session()# 创建一个1 * 20的向量tensor_zeros = tf.zeros([1, 20])sess.run(tensor_zeros) # 你可以运行一下看看my_var = tf.Variable(tf.zeros([1, 20])) # 使用tenso来初始化变量sess.run(my_var.initializer) # 又一种运行变量初始化器的方式sess.run(my_var) #打印出来看看# tf.ones() 生成全是1# tf.zeros() 生成全是0# tf.constant() 生成一个常量Tensor# 如果我们想要通过一个已知的Tensor来创建另一个，则可以使用ones_like()和zeros_like()这两个函数zero_similar = tf.Variable(tf.zeros_like(tensor_zeros))sess.run(zero_similar.initializer)print(sess.run(zero_similar))# 注意上面的两个函数的参数是为了确定生成Tensor的大小，而产生的值是通过函数决定的tf.fill([row, col], -1) # 用具体的数字填充tf.linspace(start=0.0, stop=1.0, num=3) # 线性分布 包括endtf.range(start=6, limit=15, delta=3) # 也是线性均匀 不包括endtf.random_normal([row_dim, col_dim], mean=0.0, stddev=1.0) # 随机 均值0 方差1.0tf.random_uniform([row_dim, col_dim], minval=0, maxval=4) # 或者最小最大值随机初始化 1234567891011121314151617import tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()my_var = tf.Variable(tf.zeros([1,20]))merged = tf.summary.merge_all()writer = tf.summary.FileWriter(\"./tmp/variable_logs\", graph=sess.graph)initialize_op = tf.global_variables_initializer()sess.run(initialize_op)# 上面的就是一个Tensor放在一个变量里面，我们使用了一条语句 merged = tf.summary.merge_all() 还有writer = tf.summary.FileWriter(\"/tmp/variable_logs\", graph=sess.graph)，这两句这是为了将变量在TensorBoard里面显示出来，让我们更加了解TensorFLow的一些操作。# 上面的操作过程会在当前文件夹里面创建一个/tmp/variable_logs文件夹然后会将变量信息存储在一个文件里面 那怎么使用tensorboard 12#进去我们的环境变量，然后执行tensorboard --logdir=tmp的绝对路径 可以看到我上面执行的命令。然后在浏览器里面输入127.0.0.1:6006然后你就可以看到刚才那个变量的操作过程，这就是tensorboard的魅力 上面就是一个变量在进行初始化时候可视化显示 Placeholders使用(一样可以使用tensorboard来查看) 123456789101112131415import numpy as npimport tensorflow as tffrom tensorflow.python.framework import opsops.reset_default_graph()sess = tf.Session()# 定义一个placeholderx = tf.placeholder(tf.float32, shape = (4, 4))# 随机生成4 * 4的矩阵reand_array = np.random.rand(4, 4)y = tf.identity(x) # 返回与输入对象相同的内容和大小print(sess.run(y, feed_dict=&#123;x: rand_array&#125;))merged = tf.summary.merge_all()writer = tf.summary.FileWriter(&quot;./tmp/variable_logs&quot;, sess.graph) 总结 这次我们就TensorFlow的一些基础概念的介绍，也是为了让大家在以后的TensorFlow使用过程中少一些疑问，后面的章节，我们会慢慢深入。小伙伴们不要着急，我的邮箱是air@weaf.top，依旧是那个可以交流学习的milittle。谢谢大家的驻足。 第一篇 TensorFlow安装 第二篇 TensorFlow初体验（fasion-mnist识别） 修改pip全局镜像方法","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]},{"title":"文本聚类系列教程：（三）构建词袋空间VSM（Vector Space Model）","slug":"2018-03-26/文本聚类系列教程：（三）构建词袋空间VSM（Vector-Space-Model）","date":"2018-03-30T06:00:08.000Z","updated":"2018-04-23T11:00:19.542Z","comments":true,"path":"posts/a751f7e5/","link":"","permalink":"http://weafteam.github.io/posts/a751f7e5/","excerpt":"","text":"咱们今天先聊个概念吧，著名的聚类假设，这也是文本聚类的依据，内容如下：该假设认为，同类的文档相似度较大，而不同类的文档相似度较小。 概念： 对于上述概念，也就是做文本聚类的基础，如果不相关的文档反而相似度高，我们便无法做文本聚类。 接下来再说VSM(Vector Space Model),对于VSM的定义，我在网上搜罗了些资料，如下所示： Vector space model (or term vector model) is an algebraic model for representing text documents (and any objects, in general) as vectors of identifiers, such as, for example, index terms. It is used in information filtering, information retrieval, indexing and relevancy rankings. Its first use was in the SMART Information Retrieval System. A document is represented as a vector. Each dimension corresponds to a separate term. If a term occurs in the document, its value in the vector is non-zero. Several different ways of computing these values, also known as (term) weights, have been developed. One of the best known schemes is tf-idf weighting. The definition of term depends on the application. Typically terms are single words, keywords, or longer phrases. If the words are chosen to be the terms, the dimensionality of the vector is the number of words in the vocabulary (the number of distinct words occurring in the corpus). 拙劣的翻译： 向量空间模型是用来表示文本文档（通常也包含一些对象）的特征向量的代数模型，例如索引词项。它被应用于信息过滤、信息检索、索引和相关度计算。这个模型最早被应用于SMART信息检索系统。 一个文本文档表示一个向量。每一个维度相当于一个单独的词项（term）。如果一个词项（term）出现在一个文档中，那么它在表示该文档的向量中对应项不为0.有一些计算这些词项（term）权重的方法被逐渐提出来，其中最著名的方法就是tf-idf权重计算方法。 对于词项（term）的定义依赖于应用。一般而言，词项（term）可以是单词、关键字、或者长短语。如果单词作为词项（term），那么向量中的维度就是词汇表中的单词的个数（出现在文档全集中所有不同的单词的数量）。 小荔枝： 举个荔枝吧 ，方便理解上述的概念。首先假设有这样两个文本 1.我来到北京清华大学 2.他来到了网易杭研大厦 分词结果为：我/来到/北京/清华大学和他/来到/了/网易/杭研/大厦统计所有文档的词集合：我/来到/北京/清华大学/他/了/网易/杭研/大厦，按照1983停用词去除停用词后结果为：来到/北京/清华大学/网易/杭研/大厦 我们对这两个文本构建向量，结果如下 来到 北京 清华大学 网易 杭研 大厦 文本1 1 1 1 0 0 0 文本2 1 0 0 1 1 1 相信你已经对VSM的认识有了一个大致的轮廓，但是细心的你也可能发现了，我们在上述的例子中计算term值的方法仅仅只是计数，这样的term值是否有意义呢？我们是否能用这样的方法直接进行接下来的计算呢？对于前一个问题，答案是肯定的。不管在此基础上做什么样的改进，我们最基础的就是统计单词出现的次数，那就让我们先把上述的代码实现一下吧(与该文件同目录下有个名为txt1的文件夹，里面有1.txt和2.txt两个文件，内容分别是上述所说的两个文档，我们在上次RmStopWord.py的基础上再做修改)： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import sysimport jiebaimport osimport numpy as npfrom os import pathd = path.dirname(__file__) # 获取当前文件的dir路径stopwords_path = 'stopwords1893.txt' # 停用词表路径#text = open(path.join(d, text_path),'rb').read()def read_from_file(file_name): with open(file_name,\"r\") as fp: words = fp.read() return wordsdef RmStopWords(text): mywordlist = [] seg_list = jieba.cut(text, cut_all=False) liststr=\"/ \".join(seg_list) # 添加切分符 f_stop = open(stopwords_path) try: f_stop_text = f_stop.read() finally: f_stop.close( ) f_stop_seg_list=f_stop_text.split('\\n') # 停用词是每行一个，所以用/n分离 for myword in liststr.split('/'): #对于每个切分的词都去停用词表中对比 if not(myword.strip() in f_stop_seg_list) and len(myword.strip())&gt;1: mywordlist.append(myword) return mywordlistdef get_all_vector(file_path): names = [ os.path.join(file_path,f) for f in os.listdir(file_path) ] txts = [ open(name).read() for name in names] docs = [] word_set = set() for txt in txts: doc = RmStopWords(txt) docs.append(doc) word_set |= set(doc) word_set = list(word_set) docs_vsm = [] # 这里只是想显示有多少term for word in word_set[:30]: print(word) for doc in docs: temp_vector = [] for word in word_set: temp_vector.append(doc.count(word) * 1.0) docs_vsm.append(temp_vector) docs_matrix = np.array(docs_vsm) return docs_matrix # txt2 = RmStopWords(read_from_file(text1_path))# print(txt2)#文件路径为txt1/1.txt和2.txt，只不过我让程序循环扫描txt1下所有的文本文件txt3 = get_all_vector('txt1')print(txt3) 运行结果： 分析： 上述结果不言而喻，那么我们接着讨论，显而易见，我既然提出了第二个疑问就一定有它被提出的道理，仅仅只计算term值的方法显然存在问题，我们再随便举个例子，文本1中北京只出现了1次，但是文本1中只有3个单词，文本2中北京出现了10次但是文本2中有1000个单词，那我们用上述的方法显然不合适。所以接下来我们便要讲一个最著名的方法tf-idf计算权值的方法。 TF-IDF(term frequency–inverse document frequency) 维基百科和百度百科上的讲的很清楚，这里截取概念方便大家阅读，更详细的内容请参考前面所说的两个百科。 TF-IDF是一种统计方法，用以评估一个词(term)对于一个文件集或者一个语料库中的一份文件的重要程度。一个词(term)的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。 原理： TF-IDF的主要思想是：如果某个词或短语(term)在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语(term)具有很好的类别区分能力，适合用来分类。如果包含词条term的文档越少，也就是n越小，则IDF越大，则说明词条term也具有很好的类别区分能力。 思考： 现在想一下我们刚才提出的问题，针对我们上述的问题：同一词语在长文件里可能会比短文件有更高的词数，而不管该词重要与否。那么我们对词数做归一化就可以了，而TF就帮我们做了这样的事。那么我们就先给出TF的运算公式吧。 \\(tf_i,_j = \\frac{n_i,_j}{\\sum_k n_k,_j}\\) TF公式解读：上式中分子是该词在文件中出现的次数，而分母则是该词在文件中出现的词数之和。 我们再讲个小问题： 如果某一类文档C中包含词条t的文档数为m，而其他类包含t的文档总数为k，显然所有包含t的文档数n=m+k，而当m变大的时候，n也变大，这是后按照IDF的计算方法计算得到的IDF值会变小，也就相对应的说明该词条t类别区分能力不强。但是实际上，如果一个词条在一个类的文件中频繁出现，则说明该词条能够很好的代表这个类的文本的特征，这样的词条应该给它们赋予较高的权重，并选来作为该类文本的特征词以区别与其它类文档。其实这就是IDF的不足。 针对这个问题，我的想法是TF-IDF用来做信息检索和数据挖掘，为了获取更精准的效果，我们宁愿忽略这样不足来换取更加理想的效果（也就是TF-IDF计算出更大的权值）。（这里我的理解是这样的，如果有人有更好的解释，欢迎与我进行讨论，邮箱：well@weaf.top） 那么接下来就该给出IDF的计算公式了： \\(idf(t,D) = log(\\frac{N}{\\lvert {d \\in D, t \\in d}\\rvert})\\) IDF公式解读： |D|：语料库中文件的总数 分子为包含该词条t的文件数目，如果该词条不在语料库中，就会导致分母为零，因此一般使用1。 那就接着我们上述代码，运用TF-IDF，把对应的矩阵的单纯计数转换成权值计算吧： 123456789101112131415161718192021222324252627282930313233343536373839404142434445···def get_all_vector(file_path): names = [ os.path.join(file_path,f) for f in os.listdir(file_path) ] posts = [ open(name).read() for name in names] docs = [] word_set = set() for post in posts: doc = RmStopWords(post) docs.append(doc) word_set |= set(doc) word_set = list(word_set) docs_vsm = [] for word in word_set[:30]: print(word) for doc in docs: temp_vector = [] for word in word_set: temp_vector.append(doc.count(word) * 1.0) docs_vsm.append(temp_vector) docs_matrix = np.array(docs_vsm) #return docs_matrix column_sum = [ float(len(np.nonzero(docs_matrix[:,i])[0])) for i in range(docs_matrix.shape[1]) ] column_sum = np.array(column_sum) column_sum = docs_matrix.shape[0] / column_sum idf = np.log(column_sum) idf = np.diag(idf) i = 0 for doc_v in docs_matrix: if doc_v.sum() == 0: docs_matrix[i] = docs_matrix[i]/1 else: docs_matrix[i] = docs_matrix[i] / (doc_v.sum()) i+=1 tfidf = np.dot(docs_matrix,idf) return names,tfidftxt3 = get_all_vector(&apos;txt1&apos;)print(txt3) 结果： 本次的学习会用到很多numpy的知识，请大家自行查阅。如有兴趣，请思考为什么在新的权值矩阵中“来到”一词的权重变成了0。感谢大家的阅读~","categories":[{"name":"文本聚类","slug":"文本聚类","permalink":"http://weafteam.github.io/categories/文本聚类/"}],"tags":[{"name":"文本聚类","slug":"文本聚类","permalink":"http://weafteam.github.io/tags/文本聚类/"}]},{"title":"修改pip全局镜像","slug":"2018-03-26/alterM","date":"2018-03-27T12:29:36.000Z","updated":"2018-04-23T11:00:19.535Z","comments":true,"path":"posts/233074e6/","link":"","permalink":"http://weafteam.github.io/posts/233074e6/","excerpt":"","text":"修改pip全局镜像 第一次我们在windows上面安装了Anaconda，在使用pip安装Tensorflow中速度过慢，所以我为大家介绍一中修改全局pip源的方法（这样在使用pip下载依赖库的时候就会快一些）： 打开用户主目录：我的是C:\\Users\\milittle。 在里面新建pip文件夹，在pip文件夹中建立pip.ini文件。 在pip.ini文件中添加如下配置信息，我使用的豆瓣源： 123[global]timeout = 6000index-url = https://pypi.douban.com/simple 最后的目录结构就是：C:\\Users\\milittle\\pip\\pip.ini","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]},{"title":"如何理解描述符","slug":"2018-03-19/how-to-understand-descriptor","date":"2018-03-25T15:52:32.000Z","updated":"2018-04-23T11:00:19.527Z","comments":true,"path":"posts/5dd0238f/","link":"","permalink":"http://weafteam.github.io/posts/5dd0238f/","excerpt":"","text":"前言 上篇文章中挖了 property 和描述符的坑，这篇就把它填上好了_(:з)∠)_ property 是用描述符实现的，所以先说说 property。 property property 本身是一个实现了描述符协议的类，在不改变类接口的情况下，提供了一组对实例属性的读取、写入和删除操作。下面举个例子，一个银行账户的抽象，很容易实现： 12345class Account: def __init__(self, name, balance): self.name = name self.balance = balance 银行账户最常见的操作就是存款和取款了： 1234567891011121314In [1]: account = Account('zhang', 100) # 创建一个有 100 块存款的账户In [2]: account.balanceOut[2]: 100In [3]: account.balance -= 90 # 取 90 块In [4]: account.balance # 还剩 10 块Out[4]: 10In [5]: account.balance += 30 # 存 30 块In [6]: account.balance # 现在有 40 块Out[6]: 40 但是这里有个问题： 123456...In [7]: account.balance -= 50 # 再取 50 块In [8]: account.balance # 存款变成了负数！Out[8]: -10 当然这种操作是不该被允许的，我们需要对 balance 的写入做限制。Jawa 之类的语言会创建一组 getter、setter 方法来管理属性，但是这并不 Python，也对现有的代码不友好。正确的方式是使用 property。 12345678910111213141516class Account: def __init__(self, name, balance): self.name = name self.balance = balance @property def balance(self): return self._balance @balance.setter def balance(self, value): if value &lt; 0: raise ValueError('balance must greater than 0.') else: self._balance = value 现在 balance 被禁止设为小于 0 的数： 123456789101112131415161718192021In [1]: account = Account('zhang', 100)In [2]: account.balanceOut[2]: 100In [3]: account.balance += 40In [4]: account.balance -= 200---------------------------------------------------------------------------ValueError Traceback (most recent call last)...ValueError: balance must greater than 0.In [5]: account.balanceOut[5]: 140In [6]: account = Account('zhang', -1) # 初始化的时候也不行！---------------------------------------------------------------------------ValueError Traceback (most recent call last)...ValueError: balance must greater than 0. 可以看到我们使用 balance 的方式没有发生变化，但是对值的限制已经生效了。 property 还有一个 deleter 装饰器，处理应用于属性的 del；当然，del 本身用的也不多，大多数时候把销毁操作交给 Python 就可以了。不过如果涉及到复杂对象的引用，要做到 RAII（误，还是要手动实现的。 property 是类 property 本身是用 C 实现的，这里有一个纯 Python 的实现。正如上文所说，它本身是一个类，构造方法的签名如下： 123class property(object): def __init__(self, fget=None, fset=None, fdel=None, doc=None): pass 熟悉一点装饰器用法的话就可以看出上面的 123456class Account: ... @property def balance(self): pass 实际上就是 1234567class Account: ... def get_balance(self): pass balance = property(fget=get_balance) 如果不熟悉的话，下一篇就讲装饰器好了（误 property 的实例是类属性 上面的代码段同时展示了这样一个事实：property 的实例是类属性。这就涉及到了属性查找顺序的问题，简单试一下： 123456class Foo: data = 'data!' @property def bar(self): return 'bar!' 123456789101112In [1]: f = Foo()In [2]: f.dataOut[2]: 'data!'In [3]: f.data = 'f.data!'In [4]: f.dataOut[4]: 'f.data!'In [5]: Foo.dataOut[5]: 'data!' 实例属性覆盖了类属性，符合直觉。那么对 property 的实例来说呢？ 12345678In [6]: f.barOut[6]: 'bar!'In [7]: f.bar = 'bar'---------------------------------------------------------------------------AttributeError Traceback (most recent call last)...AttributeError: can't set attribute 尝试给 bar 赋值，失败了，也符合 property 的工作方式：执行赋值时，如果没有 setter 方法就抛出异常。那么直接修改 f.__dict__ 呢？ 1234In [8]: f.__dict__['bar'] = 'bar'In [9]: f.barOut[9]: 'bar!' 也不行，property 的实例完全覆盖了实例属性。但是，它是一个类属性，所以我们可以这样做： 123456789101112In [10]: Foo.barOut[10]: &lt;property at 0x29c44800408&gt;In [11]: Foo.bar = 'bar'In [12]: f.barOut[12]: 'bar'In [13]: f.bar = 'ba'In [14]: f.barOut[14]: 'ba' 对类属性的覆盖使 bar 不再是一个 property 的实例，所以也就不会覆盖后续的赋值了。 当然我们仍然可以用一个 property 的实例再次覆盖 Foo.bar： 1234In [15]: Foo.bar = property(fget=lambda self: 'bar!')In [16]: f.barOut[16]: 'bar!' 恢复原样。 property 的实例这种先从类中开始属性查找的方式，是一类描述符的工作模式。接下来就说说描述符。 描述符 描述符是指实现了描述符协议的类，这个协议包含四个方法，分别是 __get__，__set__，__delete__ 和 Python 3.6 新增的 __set_name__。通常，只要实现了 __get__ 或 __set__，就可以被称之为描述符。在某个角度上说，描述符的作用相当于抽象的 property，可以为一组属性提供相同的读取、写入和删除逻辑。接下来，还是从数据验证的例子开始。 下面是商店中一项商品的抽象，包含商品名、数量和单价： 12345678class Item: amount = Storage('amount') price = Storage('price') def __init__(self, name, amount, price): self.name = name self.amount = amount self.price = price 其中的 amount 和 price 都必须大于 0，所以可以用统一的描述符实现： 12345678910class Storage: def __init__(self, name): self.name = name def __set__(self, instance, value): if value &gt; 0: instance.__dict__[self.name] = value else: raise ValueError(f'&#123;self.name&#125; must greater than 0.') 由于我们并没有对读取方法有特别的需求，所以不用实现 __get__ 方法。 试一下： 1234567891011In [1]: item = Item('orange', 100, 0)---------------------------------------------------------------------------ValueError Traceback (most recent call last)...ValueError: price must greater than 0.In [2]: item = Item('orange', 0, 100)---------------------------------------------------------------------------ValueError Traceback (most recent call last)...ValueError: amount must greater than 0. 如果 amount 或 price 中的任何一个不大于 0，都会被禁止。 这里需要解释一下 __set__ 的签名中的 instance： 12def __set__(self, instance, value): pass instance 是 Item 的实例。因为描述符应该管理实例的属性，所以需要额外的参数提供相应的实例。这也是为什么我们不能这样写： 12def __set__(self, instance, value): self.__dict__[self.name] = value 这实际上是为描述符实例设置了值，而描述符实例是Item 类的类属性，所有的 Item 实例都共享相同的描述符实例。修改了某个描述符实例，相当于修改了所有的 Item 实例。 上面的例子有个缺点，初始化描述符实例的时候需要重复属性的名字。我们希望可以简单的写成： 1234class Item: amount = Storage() price = Storage() ... 而不需要在描述符的构造方法中重复属性名。这就是 Python 3.6 新增的 __set_name__ 方法的作用。只要实现 __set_name__ 方法： 12345class Storage: ... def __set_name__(self, owner, name): self.name = name 同样解释一下函数签名： 12def __set_name__(self, owner, name): pass owner 是 Item 类本身，name 是引用描述符实例的变量的名字。 如果使用的 Python 版本在 3.6 以下呢？有两个方法：第一个是用元类接管Item类的创建过程，这个不在这篇文章的内容之内（可能又挖了一个坑；第二个就是为每个描述符实例生成与属性名无关但是唯一字符串，用来代替属性名： 1234567891011121314151617class Storage: _counter = 0 def __init__(self): cls = self.__class__ self.name = f'_&#123;cls.__name__&#125;#&#123;cls._counter&#125;' cls._counter += 1 def __get__(self, instance, owner): return getattr(instance, self.name) def __set__(self, instance, value): if value &gt; 0: setattr(instance, self.name, value) else: raise ValueError('must greater than 0.') 由于 Item 中的属性名和我们实际保存的属性名不同，所以需要实现 __get__ 方法。与 __set_name__ 签名中的 owner 含义相同，__get__ 方法签名中的 owner 也是 Item 类本身。 现在，我们使用 _Storage#N 这样的名称在 Item 实例中保存属性。当然，这样的名称会让人有点困惑，特别是以类属性访问的时候： 12345In [1]: Item.amount---------------------------------------------------------------------------AttributeError Traceback (most recent call last)...AttributeError: 'NoneType' object has no attribute '_Storage#0' 为了避免在如此明显的地方暴露我们的实现细节，我们可以修改异常的错误消息，或者，内省描述符实例： 12345def __get__(self, instance, owner): if instance is None: return self else: return getattr(instance, self.name) 两类描述符 上述例子中对数据属性的控制和管理是描述符的典型用途之一。这种实现了 __set__ 方法，接管了设置属性行为的描述符，被称为覆盖型描述符，没有定义 __set__ 方法的描述符，被称为非覆盖型描述符。由于 Python 中对实例属性和类属性的处理方式不同，这两类描述符也有不同的行为。 覆盖型描述符 实现了 __set__ 方法的描述符就是覆盖型描述符。这类描述符虽然是类属性，但是会覆盖实例属性的赋值操作： 123456789101112class Override: def __get__(self, instance, owner): print('get!') def __set__(self, instance, value): print('set!')class Manager: override = Override() 下面做一些实验： 123456789101112131415161718In [1]: m = Manager()In [2]: m.overrideget!In [3]: m.override = 1set!In [4]: Manager.overrideget!In [5]: m.__dict__['override'] = 1In [6]: m.__dict__Out[6]: &#123;'override': 1&#125;In [7]: m.overrideget! 可以看出，无论以实例属性还是类属性访问 override，都会触发 __get__ 方法；为实例属性 override 赋值会触发 __set__ 方法；即使跳过描述符直接为 m.__dict__ 赋值，读取 override 的操作仍然会被描述符覆盖。 没有 __get__ 方法的覆盖型描述符 如果只实现了 __set__ 会发生什么呢？ 123456789class OverrideNoGet: def __set__(self, instance, value): print('set!')class Manager: override_no_get = OverrideNoGet() 123456789101112131415161718192021222324In [1]: m = Manager()In [2]: m.override_no_getOut[2]: &lt;__main__.OverrideNoGet at 0x29c44a97668&gt;In [3]: Manager.override_no_getOut[3]: &lt;__main__.OverrideNoGet at 0x29c44a97668&gt;In [4]: m.override_no_get = 1set!In [5]: m.override_no_getOut[5]: &lt;__main__.OverrideNoGet at 0x29c44a97668&gt;In [6]: m.__dict__['override_no_get'] = 1In [7]: m.override_no_getOut[7]: 1In [8]: m.override_no_get = 2set!In [9]: m.override_no_getOut[9]: 1 可以看到，没实现 __get__ 方法，无论以实例属性还是类属性访问 override_no_get，都会返回描述符实例；而赋值操作可以触发 __set__ 方法；由于我们的 __set__ 方法并没有真正修改实例属性，所以再次访问 override_no_get 仍然会得到描述符实例；通过 m.__dict__ 修改实例属性后，实例属性就会覆盖描述符；不过只有访问实例属性时才是如此，赋值仍然由 __set__ 处理。 非覆盖型描述符 没有实现 __set__ 方法的描述符就是非覆盖型描述符： 123456789class NonOverride: def __get__(self, instance, owner): print('get!')class Manager: non_override = NonOverride() 1234567891011121314151617181920In [1]: m = Manager()In [2]: m.non_overrideget!In [3]: Manager.non_overrideget!In [4]: m.non_override = 1In [5]: m.non_overrideOut[5]: 1In [6]: Manager.non_overrideget!In [7]: del m.non_overrideIn [8]: m.non_overrideget! 无论访问实例属性还是类属性，都会触发 __get__ 方法；由于没有 __set__ 方法，对属性的赋值不会被干涉；对属性复制之后，实例属性就会覆盖同名的描述符，但是访问类属性仍然可以触发 __get__ 方法；如果把 non_override 从实例中删除，访问 non_override 的操作又会交给 __get__。 当然，描述符都是定义在类上的，如果对同名的类属性进行赋值，就会完全替换掉描述符。这里表现出读、写属性时的不对等：对类属性的读操作可以被 __get__ 处理，但是写操作不会。当然，了解一些 Python 的话就会知道还存在着另一种不对等：读取实例属性时，会返回实例属性，如果实例属性不存在，会返回类属性；但是为实例属性赋值时，如果实例属性不存在，会在实例中创建属性，不会影响到类属性。 结语 描述符充斥在 Python 底层（举个例子：Python 中的方法是怎么实现的？）与各种框架中，理解描述符是体会 Python 世界工作原理和设计美学的重要方式。","categories":[],"tags":[]},{"title":"TensorFlow 初体验 （Fashion-mnist）","slug":"2018-03-19/chapter-02-AIR","date":"2018-03-25T13:18:37.000Z","updated":"2018-04-23T11:00:19.525Z","comments":true,"path":"posts/b0821049/","link":"","permalink":"http://weafteam.github.io/posts/b0821049/","excerpt":"","text":"TensorFlow 初体验（Fashion-mnist） 接着上一讲的内容，想必大家已经通过我的教程安装好了TensorFlow了吧，那我们这节课通过安装简单的跨平台的集成开发环境Spyder，在这个集成开发环境上面实现一些python程序。具体安装过程见如下阐述： 首先在应用程序里面找到Anaconda应用程序，打开里面的Anaconda Navigator，然后打开以后，选中我们上次建立好的环境tensorflow。 选中tensorflow这个环境变量以后，看到里面有一个集成开发环境叫spyder，这个工具就是今天我们要安装的，我的已经安装好了，所以是Launch，你们的没有安装好，所以是install状态，点解安装就好。（这个地方也可能需要翻墙）。 这个安装好以后，你就会在应用文件夹里面出现一个Spyder(tensorflow)这个应用程序，以后你就从应用文件夹启动就好。 那么启动以后：我也是启动了，出现了以下的情况：不慌，慢慢来。 看到上面的错误，这个错误提示是因为没有安装jedi这个依赖库，而且要求版本要大于0.9.0。那我们接下来解决一下这个问题。 小插曲，一下就可以解决，具体操作步骤: 还是打开上次那个AnacondaPrompt的命令行 进去以后，执行activate tensorflow 相当于你要在这个环境下面给这个spyder安装这个依赖 进去以后，执行pip install jedi==0.9.0 就可以了，然后重启spyder（可以直接在这个环境里面输入spyder命令就可以实现spyder的启动，你也可以在应用文件夹里面启动，性质是一样的） 不出什么意外的话，spyder使用就没有问题了，有什么问题可以发邮件给我！！！ 解决了上面的小插曲以后，我们在spyder中输入以下代码进行测试。 123456789101112import tensorflow as tfsess = tf.Session()init = tf.global_variables_initializer() # 此处的init是全局变量初始化器，# TensorFlow的session必须执行这个初始化器才能执行前面建立好的图，# 所以，这个是很重要的一点，后续也会强调#（也就是后续再网络中建立变量就是通过那个初始化器来进行初始化工作的）# 其实在没有变量的时候，这个初始化器是不需要的# 但是为了让大家形成习惯，还是写上sess.run(init)hello = tf.constant('hello world')print(sess.run(hello)) 上图中左面是代码书写区域，右面上半部分是变量查看区域，还有文件夹区域可以切换，右面下半部分是执行console区域，我输入上面的代码，执行以后console区域打出hello world字符串。 从上面的一些简单的测试以后，我们进入今天的主题，fashion-minist的识别，fashion-minist是一个服装识别的一个数据集，在这个数据集之前有一个mnist手写体识别数据集，这个手写数据集对应我们手写的十个数字，然后通过设计网络来识别手写体。但是今天我们不做手写体识别，直接来做fashion-minist识别。 闲话少说，上代码，边写边说。 首先目标是实现衣服种类的识别。 数据可以在 Zalando_Fashion_MNIST_repository这个Github仓库获取。 数据分为60000训练数据和10000测试数据，图片都是灰度图片，大小为28 X 28，总共也是由10类组成。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305# -*- coding: utf-8 -*-\"\"\"Created on Sun Mar 25 15:16:23 2018@author: milittle\"\"\"# 导入一些必要的库import numpy as np # 数学计算库import matplotlib.pyplot as plt # 画图的一个库import tensorflow as tf # TensorFlow的库from tensorflow.examples.tutorials.mnist import input_datafashion_mnist = input_data.read_data_sets('input/data', one_hot = True)# 定义一个服装对应表label_dict = &#123; 0: 'T-shirt/top', 1: 'Trouser', 2: 'Pullover', 3: 'Dress', 4: 'Coat', 5: 'Sandal', 6: 'Shirt', 7: 'Sneaker', 8: 'Bag', 9: 'Ankle boot'&#125;# 获取随机的数据和它的labelsample_1 = fashion_mnist.train.images[47].reshape(28,28)sample_label_1 = np.where(fashion_mnist.train.labels[47] == 1)[0][0]sample_2 = fashion_mnist.train.images[23].reshape(28,28)sample_label_2 = np.where(fashion_mnist.train.labels[23] == 1)[0][0]# 用matplot画出这个image和labelprint(\"y = &#123;label_index&#125; (&#123;label&#125;)\".format(label_index=sample_label_1, label=label_dict[sample_label_1]))plt.imshow(sample_1, cmap='Greys')plt.show()print(\"y = &#123;label_index&#125; (&#123;label&#125;)\".format(label_index=sample_label_2, label=label_dict[sample_label_2]))plt.imshow(sample_2, cmap='Greys')plt.show()# 接下来就是设计网络参数n_hidden_1 = 128 # 第一个隐藏层的单元个数n_hidden_2 = 128 # 第二个隐藏层的单元个数n_input = 784 # fashion mnist输入图片的维度（单元个数） (图片大小: 28*28)n_classes = 10 # fashion mnist的种类数目 (0-9 数字)# 创建 placeholdersdef create_placeholders(n_x, n_y): \"\"\" 为sess创建一个占位对象。 参数: n_x -- 向量, 图片大小 (28*28 = 784) n_y -- 向量, 种类数目 (从 0 到 9, 所以是 -&gt; 10种) 返回参数: X -- 为输入图片大小的placeholder shape是[784, None] Y -- 为输出种类大小的placeholder shape是[10, None] None在这里表示以后输入的数据可以任意多少 \"\"\" X = tf.placeholder(tf.float32, [n_x, None], name=\"X\") Y = tf.placeholder(tf.float32, [n_y, None], name=\"Y\") return X, Y# 测试上面的create_placeholders()X, Y = create_placeholders(n_input, n_classes)print(\"Shape of X: &#123;shape&#125;\".format(shape=X.shape))print(\"Shape of Y: &#123;shape&#125;\".format(shape=Y.shape))# 定义初始化参数参数def initialize_parameters(): \"\"\" 参数初始化，下面是每个参数的shape，总共有三层 W1 : [n_hidden_1, n_input] b1 : [n_hidden_1, 1] W2 : [n_hidden_2, n_hidden_1] b2 : [n_hidden_2, 1] W3 : [n_classes, n_hidden_2] b3 : [n_classes, 1] 返回: 包含所有权重和偏置项的dic \"\"\" # 设置随机数种子 tf.set_random_seed(42) # 为每一层的权重和偏置项进行初始化工作 W1 = tf.get_variable(\"W1\", [n_hidden_1, n_input], initializer = tf.contrib.layers.xavier_initializer(seed = 42)) b1 = tf.get_variable(\"b1\", [n_hidden_1, 1], initializer = tf.zeros_initializer()) W2 = tf.get_variable(\"W2\", [n_hidden_2, n_hidden_1], initializer = tf.contrib.layers.xavier_initializer(seed = 42)) b2 = tf.get_variable(\"b2\", [n_hidden_2, 1], initializer = tf.zeros_initializer()) W3 = tf.get_variable(\"W3\", [n_classes, n_hidden_2], initializer=tf.contrib.layers.xavier_initializer(seed = 42)) b3 = tf.get_variable(\"b3\", [n_classes, 1], initializer = tf.zeros_initializer()) # 将参数存储在一个dict对象里面返回去 parameters = &#123; \"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, \"b3\": b3 &#125; return parameters# 测试初始化参数tf.reset_default_graph()with tf.Session() as sess: parameters = initialize_parameters() print(\"W1 = &#123;w1&#125;\".format(w1=parameters[\"W1\"])) print(\"b1 = &#123;b1&#125;\".format(b1=parameters[\"b1\"])) print(\"W2 = &#123;w2&#125;\".format(w2=parameters[\"W2\"])) print(\"b2 = &#123;b2&#125;\".format(b2=parameters[\"b2\"])) # 前向传播算法（就是神经网络的前向步骤）def forward_propagation(X, parameters): \"\"\" 实现前向传播的模型 LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SOFTMAX 上面的显示就是三个线性层，每一层结束以后，实现relu的作用，实现非线性功能，最后三层以后用softmax实现分类 参数: X -- 输入训练数据的个数[784, n] 这里的n代表可以一次训练多个数据 parameters -- 包括上面所有的定义参数三个网络中的权重W和偏置项B 返回: Z3 -- 最后的一个线性单元输出 \"\"\" # 从参数dict里面取到所有的参数 W1 = parameters['W1'] b1 = parameters['b1'] W2 = parameters['W2'] b2 = parameters['b2'] W3 = parameters['W3'] b3 = parameters['b3'] # 前向传播过程 Z1 = tf.add(tf.matmul(W1,X), b1) # Z1 = np.dot(W1, X) + b1 A1 = tf.nn.relu(Z1) # A1 = relu(Z1) Z2 = tf.add(tf.matmul(W2,A1), b2) # Z2 = np.dot(W2, a1) + b2 A2 = tf.nn.relu(Z2) # A2 = relu(Z2) Z3 = tf.add(tf.matmul(W3,A2), b3) # Z3 = np.dot(W3,Z2) + b3 return Z3# 测试前向传播喊出tf.reset_default_graph()with tf.Session() as sess: X, Y = create_placeholders(n_input, n_classes) parameters = initialize_parameters() Z3 = forward_propagation(X, parameters) print(\"Z3 = &#123;final_Z&#125;\".format(final_Z=Z3))# 定义计算损失函数# 是计算loss的时候了def compute_cost(Z3, Y): \"\"\" 计算cost 参数: Z3 -- 前向传播的最终输出（[10, n]）n也是你输入的训练数据个数 Y -- 返回: cost - 损失函数 张量（Tensor） \"\"\" # 获得预测和准确的label logits = tf.transpose(Z3) labels = tf.transpose(Y) # 计算损失 cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels)) return cost# 测试计算损失函数tf.reset_default_graph()with tf.Session() as sess: X, Y = create_placeholders(n_input, n_classes) parameters = initialize_parameters() Z3 = forward_propagation(X, parameters) cost = compute_cost(Z3, Y) print(\"cost = &#123;cost&#125;\".format(cost=cost))# 这个就是关键了，因为每一层的参数都是通过反向传播来实现权重和偏置项参数更新的# 总体的原理就是经过前向传播，计算到最后的层，利用softmax加交叉熵，算出网络的损失函数# 然后对损失函数进行求偏导，利用反向传播算法实现每一层的权重和偏置项的更新def model(train, test, learning_rate=0.0001, num_epochs=16, minibatch_size=32, print_cost=True, graph_filename='costs'): \"\"\" 实现了一个三层的网络结构: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SOFTMAX. 参数: train -- 训练集 test -- 测试集 learning_rate -- 优化权重时候所用到的学习率 num_epochs -- 训练网络的轮次 minibatch_size -- 每一次送进网络训练的数据个数（也就是其他函数里面那个n参数） print_cost -- 每一轮结束以后的损失函数 返回: parameters -- 被用来学习的参数 \"\"\" # 确保参数不被覆盖重写 tf.reset_default_graph() tf.set_random_seed(42) seed = 42 # 获取输入和输出大小 (n_x, m) = train.images.T.shape n_y = train.labels.T.shape[0] costs = [] # 创建输入输出数据的占位符 X, Y = create_placeholders(n_x, n_y) # 初始化参数 parameters = initialize_parameters() # 进行前向传播 Z3 = forward_propagation(X, parameters) # 计算损失函数 cost = compute_cost(Z3, Y) # 使用AdamOptimizer优化器实现反向传播算法（最小化cost） # 其实我们这个地方的反向更新参数的过程都是tensorflow给做了 optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost) # 变量初始化器 init = tf.global_variables_initializer() # 开始tensorflow的sess 来计算tensorflow构建好的图 with tf.Session() as sess: # 这个就是之前说过的要进行初始化的 sess.run(init) # 训练轮次 for epoch in range(num_epochs): epoch_cost = 0. num_minibatches = int(m / minibatch_size) seed = seed + 1 for i in range(num_minibatches): # 获取下一个batch的训练数据和label数据 minibatch_X, minibatch_Y = train.next_batch(minibatch_size) # 执行优化器 _, minibatch_cost = sess.run([optimizer, cost], feed_dict=&#123;X: minibatch_X.T, Y: minibatch_Y.T&#125;) # 更新每一轮的损失 epoch_cost += minibatch_cost / num_minibatches # 打印每一轮的损失 if print_cost == True: print(\"Cost after epoch &#123;epoch_num&#125;: &#123;cost&#125;\".format(epoch_num=epoch, cost=epoch_cost)) costs.append(epoch_cost) # 使用matplot画出损失的变化曲线图 plt.figure(figsize=(16,5)) plt.plot(np.squeeze(costs), color='#2A688B') plt.xlim(0, num_epochs-1) plt.ylabel(\"cost\") plt.xlabel(\"iterations\") plt.title(\"learning rate = &#123;rate&#125;\".format(rate=learning_rate)) plt.savefig(graph_filename, dpi = 300) plt.show() # 保存参数 parameters = sess.run(parameters) print(\"Parameters have been trained!\") # 计算预测准率 correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y)) # 计算测试准率 accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) print (\"Train Accuracy:\", accuracy.eval(&#123;X: train.images.T, Y: train.labels.T&#125;)) print (\"Test Accuracy:\", accuracy.eval(&#123;X: test.images.T, Y: test.labels.T&#125;)) return parameters# 要开始训练我们的fashion mnist网络了train = fashion_mnist.train # 训练的数据test = fashion_mnist.test # 测试的数据parameters = model(train, test, learning_rate = 0.001, num_epochs = 16, graph_filename = 'fashion_mnist_costs') 上面的代码是写好了，这里有一个python的依赖库（matplotlib）需要安装以下，同样的办法，就是进去tensorflow这个环境里面，然后执行pip install matplotlib就可以了。 在这个过程中，可能从tensorflow下载数据的时候会很慢。（我们选择直接从上面给出下载数据集的github网址，直接下载以后，将数据拷贝在代码所在文件夹的input/data/文件夹里面，总共由四个文件组成）分别是训练数据图片、训练数据label和测试数据图片、测试数据label。这样就可以省去下载数据时候漫长的等待。 上面就是我们使用TensorFlow实现的fashion-mnist的识别，总体根据实验结果来说，从测试集的数据来看，我达到的准确率结果是88.5%，还算可以。后续我们可能使用其他一些现有的网络结构来实现fashion-mnist的识别，看看准确率会不会提高。 如下是我对上面TensorFLow出现的方法介绍： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748tf.placeholders( dtype, shape=None, name=None)从参数上面看到，总共有三个参数： dtype：在tensor中被喂数据的元素类型 shape: tensor的shape name：命名说明一下，这个函数返回的是一个tensor，在TensorFlow里面，tensor是一个很重要的概念，大家务必掌握，也叫张量，比如我们的一个数:就是0-阶张量，也叫标量。一个向量，就是1-阶张量。一个矩阵，就是2-阶张量，后面的就是一直往高维了走，对应的就是多少阶张量。这个方法，很重要的原因也在于它是定义在Session执行run的时候，在后面填充数据的占位符，也就是feed_dict这个变量里面的数据，所以大家，务必记住这一关键的概念。后续用起来就会很顺手。tf.get_variable()这个方法后续在展开来说，你先理解就是使用它可以定义变量（保存权重和偏置项的），还可以加一些优化器，比如说正则优化器等等tf.matmul( a, b,)展示给你们列出这两个参数： a：就是待操作的矩阵1 b: 就是待操作的矩阵2函数功能就是实现矩阵的相乘运算（当然要符合基本的矩阵运算格式）tf.transpose( a,)先列出来一个参数，就是矩阵的转置Session().run( fetches, feed_list=None, )这个方法就是运行图。很关键，先掌握两个参数: fetches: 你要从图里面取出的数据（） feed_list: 你要给图喂的数据（输入和label数据就是用这样的方式来做的） 比如我们训练的网络中输入的图片信息和对应的label信息tf.reduce_mean( input_tensor, axis=None, keepdims=None, name=None, redcution_indices=None, keep_dims=None )计算输入tensor的总和： input_tensor: 要叠加的tensor axis: 选择那个维度叠加 keepdims: 叠加元素以后，保留原来的维度信息 name：就是名字 redcution_indices：被axis取代 keep_dims：被keepdims取代 我们今天的任务量可能有一些大，大家坚持。总的来说就是使用神经网络对实际的一个fashion-mnist数据集进行服装种类的识别，大家主要看看我的代码。有什么不明白的我在代码里面都做出了注释。 邮箱——air@weaf.top欢迎来探讨","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]},{"title":"rsync的使用与配置","slug":"2018-03-19/rsync-configuration-and-use","date":"2018-03-25T13:08:56.000Z","updated":"2018-04-23T11:00:19.530Z","comments":true,"path":"posts/cfc65600/","link":"","permalink":"http://weafteam.github.io/posts/cfc65600/","excerpt":"一、什么是rsync rsync，remote synchronize顾名思意就知道它是一款实现远程同步功能的软件，它在同步文件的同时，可以保持原来文件的权限、时间、软硬链接等附加信息。 rsync是用 “rsync 算法”提供了一个客户机和远程文件服务器的文件同步的快速方法，而且可以通过ssh方式来传输文件，这样其保密性也非常好，另外它还是免费的软件。 二、rsync的安装 rysnc的官方网站：http://rsync.samba.org 可以从上面得到最新的版本。目前最新版是3.1.2。当然，因为rsync是一款如此有用的软件，所以很多Linux的发行版本都将它收录在内了。","text":"一、什么是rsync rsync，remote synchronize顾名思意就知道它是一款实现远程同步功能的软件，它在同步文件的同时，可以保持原来文件的权限、时间、软硬链接等附加信息。 rsync是用 “rsync 算法”提供了一个客户机和远程文件服务器的文件同步的快速方法，而且可以通过ssh方式来传输文件，这样其保密性也非常好，另外它还是免费的软件。 二、rsync的安装 rysnc的官方网站：http://rsync.samba.org 可以从上面得到最新的版本。目前最新版是3.1.2。当然，因为rsync是一款如此有用的软件，所以很多Linux的发行版本都将它收录在内了。 软件包安装 命令 平台 # sudo apt-get install rsync 注：在debian、ubuntu 等在线安装方法； # yum install rsync 注：Fedora、Redhat 等在线安装方法； # rpm -ivh rsync 注：Fedora、Redhat 等rpm包安装方法； 其它Linux发行版，请用相应的软件包管理方法来安装。 源码包安装 123 tar xvf rsync-xxx.tar.gz cd rsync-xxx ./configure --prefix=/usr ;make ;make install 注：在用源码 包编译安装之前，您得安装gcc等编译开具才行； 三、rsync的配置 ———– rsync的主要有以下三个配置文件rsyncd.conf(主配置文件)、rsyncd.secrets(密码文件)、rsyncd.motd(rysnc服务器信息) 比如我们要备份服务器上的/home和/opt，在/home中我想把easylife和samba目录排除在外； 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849 # Distributed under the terms of the GNU General Public License v2 # Minimal configuration file for rsync daemon # See rsync(1) and rsyncd.conf(5) man pages for help # This line is required by the /etc/init.d/rsyncd script pid file = /var/run/rsyncd.pid port = 873 address = 192.168.1.171 #uid = nobody #gid = nobody uid = root gid = root use chroot = yes read only = yes #limit access to private LANs hosts allow=192.168.1.0/255.255.255.0 10.0.1.0/255.255.255.0 hosts deny=* max connections = 5 motd file = /etc/rsyncd.motd #This will give you a separate log file #log file = /var/log/rsync.log #This will log every file transferred - up to 85,000+ per user, per sync #transfer logging = yes log format = %t %a %m %f %b syslog facility = local3 timeout = 300 [rhel4home] path = /home list=yes ignore errors auth users = root secrets file = /etc/rsyncd.secrets comment = This is RHEL 4 data exclude = easylife/ samba/ [rhel4opt] path = /opt list=no ignore errors comment = This is RHEL 4 opt auth users = easylife secrets file = /etc/rsyncd/rsyncd.secrets 注：关于auth users是必须在服务器上存在的真实的系统用户，如果你想用多个用户以,号隔开，比如auth users = easylife,root 设定密码文件 密码文件格式很简单，rsyncd.secrets的内容格式为： 用户名:密码 我们在例子中rsyncd.secrets的内容如下类似的；在文档中说，有些系统不支持长密码，自己尝试着设置一下吧。 12 easylife:keer root:mike 12 chown root.root rsyncd.secrets #修改属主 chmod 600 rsyncd.secrets #修改权限 注：1、将rsyncd.secrets这个密码文件的文件属性设为root拥有, 且权限要设为600, 否则无法备份成功! 出于安全目的，文件的属性必需是只有属主可读。 2、这里的密码值得注意，为了安全你不能把系统用户的密码写在这里。比如你的系统用户easylife密码是000000，为了安全你可以让rsync中的easylife为keer。这和samba的用户认证的密码原理是差不多的。 设定rsyncd.motd 文件; 它是定义rysnc服务器信息的，也就是用户登录信息。比如让用户知道这个服务器是谁提供的等；类似ftp服务器登录时，我们所看到的 linuxsir.org ftp ……。 当然这在全局定义变量时，并不是必须的，你可以用#号注掉，或删除；我在这里写了一个 rsyncd.motd的内容为： 1234 ++++++++++++++++++++++++++++++++++++++++++++++ Welcome to use the mike.org.cn rsync services!2002------2009 ++++++++++++++++++++++++++++++++++++++++++++++ 四、启动rsync服务器 相当简单，有以下几种方法 A、–daemon参数方式，是让rsync以服务器模式运行 1 #/usr/bin/rsync --daemon --config=/etc/rsyncd/rsyncd.conf #--config用于指定rsyncd.conf的位置,如果在/etc下可以不写 B、xinetd方式 12345 修改services加入如下内容 # nano -w /etc/services rsync 873/tcp # rsync rsync 873/udp # rsync 这一步一般可以不做，通常都有这两行(我的RHEL4和GENTOO默认都有)。修改的目的是让系统知道873端口对应的服务名为rsync。如没有的话就自行加入。 设定 /etc/xinetd.d/rsync, 简单例子如下: 12345678910111213 # default: off # description: The rsync server is a good addition to am ftp server, as it \\ # allows crc checksumming etc. service rsync &#123;disable = nosocket_type = streamwait = nouser = rootserver = /usr/bin/rsyncserver_args = --daemonlog_on_failure += USERID &#125; 上述, 主要是要打开rsync這個daemon, 一旦有rsync client要连接時, xinetd会把它转介給 rsyncd(port 873)。然后service xinetd restart, 使上述设定生效. rsync服务器和防火墙 Linux 防火墙是用iptables，所以我们至少在服务器端要让你所定义的rsync 服务器端口通过，客户端上也应该让通过。 12 #iptables -A INPUT -p tcp -m state --state NEW -m tcp --dport 873 -j ACCEPT #iptables -L 查看一下防火墙是不是打开了 873端口 如果你不太懂防火墙的配置，可以先service iptables stop 将防火墙关掉。当然在生产环境这是很危险的，做实验才可以这么做哟！ 五、通过rsync客户端来同步数据 B1、列出rsync 服务器上的所提供的同步内容； 首先：我们看看rsync服务器上提供了哪些可用的数据源 # rsync –list-only root@192.168.145.5:: ++++++++++++++++++++++++++++++++++++++++++++++ Welcome to use the mike.org.cn rsync services! 2002——2009 ++++++++++++++++++++++++++++++++++++++++++++++ rhel4home This is RHEL 4 data 注：前面是rsync所提供的数据源，也就是我们在rsyncd.conf中所写的[rhel4home]模块。而“This is RHEL 4 data”是由[rhel4home]模块中的 comment = This is RHEL 4 data 提供的；为什么没有把rhel4opt数据源列出来呢？因为我们在[rhel4opt]中已经把list=no了。 $ rsync –list-only root@192.168.145.5::rhel4home ++++++++++++++++++++++++++++++++++++++++++++++ Welcome to use the mike.org.cn rsync services! 2002——2009 ++++++++++++++++++++++++++++++++++++++++++++++ Password: drwxr-xr-x 4096 2009/03/15 21:33:13 . -rw-r–r– 1018 2009/03/02 02:33:41 ks.cfg -rwxr-xr-x 21288 2009/03/15 21:33:13 wgetpaste drwxrwxr-x 4096 2008/10/28 21:04:05 cvsroot drwx—— 4096 2008/11/30 16:30:58 easylife drwsr-sr-x 4096 2008/09/20 22:18:05 giddir drwx—— 4096 2008/09/29 14:18:46 quser1 drwx—— 4096 2008/09/27 14:38:12 quser2 drwx—— 4096 2008/11/14 06:10:19 test drwx—— 4096 2008/09/22 16:50:37 vbird1 drwx—— 4096 2008/09/19 15:28:45 vbird2 后面的root@ip中，root是指定密码文件中的用户名，之后的::rhel4home这是rhel4home模块名 ### B2、rsync客户端同步数据； #rsync -avzP root@192.168.145.5::rhel4home rhel4home Password: 这里要输入root的密码，是服务器端rsyncd.secrets提供的。在前面的例子中我们用的是mike，输入的密码并不回显，输好就回车。 注： 这个命令的意思就是说，用root用户登录到服务器上，把rhel4home数据，同步到本地当前目录rhel4home上。当然本地的目录是可以你自己 定义的。如果当你在客户端上当前操作的目录下没有rhel4home这个目录时，系统会自动为你创建一个；当存在rhel4home这个目录中，你要注意 它的写权限。 1 #rsync -avzP --delete linuxsir@linuxsir.org::rhel4home rhel4home 这回我们引入一个–delete 选项，表示客户端上的数据要与服务器端完全一致，如果 linuxsirhome目录中有服务器上不存在的文件，则删除。最终目的是让linuxsirhome目录上的数据完全与服务器上保持一致；用的时候要 小心点，最好不要把已经有重要数所据的目录，当做本地更新目录，否则会把你的数据全部删除； 設定 rsync client 设定密码文件 1 #rsync -avzP --delete --password-file=rsyncd.secrets root@192.168.145.5::rhel4home rhel4home 这次我们加了一个选项 –password-file=rsyncd.secrets，这是当我们以root用户登录rsync服务器同步数据时，密码将读取rsyncd.secrets这个文件。这个文件内容只是root用户的密码。我们要如下做； # touch rsyncd.secrets # chmod 600 rsyncd.secrets # echo “mike”&gt; rsyncd.secrets # rsync -avzP –delete –password-file=rsyncd.secrets root@192.168.145.5::rhel4home rhel4home 注：这里需要注意的是这份密码文件权限属性要设得只有属主可读。 这样就不需要密码了；其实这是比较重要的，因为服务器通过crond 计划任务还是有必要的； ### B3、让rsync客户端自动与服务器同步数据 服务器是重量级应用，所以数据的网络备份还是极为重要的。我们可以在生产型服务器上配置好rsync 服务器。我们可以把一台装有rysnc机器当做是备份服务器。让这台备份服务器，每天在早上4点开始同步服务器上的数据；并且每个备份都是完整备份。有时 硬盘坏掉，或者服务器数据被删除，完整备份还是相当重要的。这种备份相当于每天为服务器的数据做一个镜像，当生产型服务器发生事故时，我们可以轻松恢复数 据，能把数据损失降到最低；是不是这么回事？？ step1：创建同步脚本和密码文件 12345678 #mkdir /etc/cron.daily.rsync #cd /etc/cron.daily.rsync #touch rhel4home.sh rhel4opt.sh #chmod 755 /etc/cron.daily.rsync/*.sh #mkdir /etc/rsyncd/ #touch /etc/rsyncd/rsyncrhel4root.secrets #touch /etc/rsyncd/rsyncrhel4easylife.secrets #chmod 600 /etc/rsyncd/rsync.* 注： 我们在 /etc/cron.daily/中创建了两个文件rhel4home.sh和rhel4opt.sh ，并且是权限是755的。创建了两个密码文件root用户用的是rsyncrhel4root.secrets ，easylife用户用的是 rsyncrhel4easylife.secrets，权限是600； 我们编辑rhel4home.sh，内容是如下的： 123 #!/bin/sh #backup 192.168.145.5:/home /usr/bin/rsync -avzP --password-file=/etc/rsyncd/rsyncrhel4root.secrets root@192.168.145.5::rhel4home /home/rhel4homebak/$(date +&apos;%m-%d-%y&apos;) 我们编辑 rhel4opt.sh ，内容是： 123 #!/bin/sh #backup 192.168.145.5:/opt /usr/bin/rsync -avzP --password-file=/etc/rsyncd/rsyncrhel4easylife.secrets easylife@192.168.145.5::rhel4opt /home/rhel4hoptbak/$(date +&apos;%m-%d-%y&apos;) 注：你可以把rhel4home.sh和rhel4opt.sh的内容合并到一个文件中，比如都写到rhel4bak.sh中； 接着我们修改 /etc/rsyncd/rsyncrhel4root.secrets和rsyncrhel4easylife.secrets的内容； 12 # echo &quot;mike&quot; &gt; /etc/rsyncd/rsyncrhel4root.secrets # echo &quot;keer&quot;&gt; /etc/rsyncd/rsyncrhel4easylife.secrets 然后我们再/home目录下创建rhel4homebak 和rhel4optbak两个目录，意思是服务器端的rhel4home数据同步到备份服务器上的/home/rhel4homebak 下，rhel4opt数据同步到 /home/rhel4optbak/目录下。并按年月日归档创建目录；每天备份都存档； 12 #mkdir /home/rhel4homebak #mkdir /home/rhel4optbak step2：修改crond服务器的配置文件 加入到计划任务 1 #crontab -e 加入下面的内容： # Run daily cron jobs at 4:10 every day backup rhel4 data: 10 4 * * * /usr/bin/run-parts /etc/cron.daily.rsync 1&gt; /dev/null 注：第一行是注释，是说明内容，这样能自己记住。 第二行表示在每天早上4点10分的时候，运行 /etc/cron.daily.rsync 下的可执行脚本任务； 配置好后，要重启crond 服务器； 123456 # killall crond 注：杀死crond 服务器的进程； # ps aux |grep crond 注：查看一下是否被杀死； # /usr/sbin/crond 注：启动 crond 服务器； # ps aux |grep crond 注：查看一下是否启动了？ root 3815 0.0 0.0 1860 664 ? S 14:44 0:00 /usr/sbin/crond root 3819 0.0 0.0 2188 808 pts/1 S+ 14:45 0:00 grep crond","categories":[{"name":"Linux","slug":"Linux","permalink":"http://weafteam.github.io/categories/Linux/"}],"tags":[{"name":"Linux运维","slug":"Linux运维","permalink":"http://weafteam.github.io/tags/Linux运维/"}]},{"title":"文本聚类系列教程：（二）jieba中文分词工具进阶","slug":"2018-03-19/文本聚类系列教程：（二）jieba中文分词工具进阶","date":"2018-03-19T11:57:19.000Z","updated":"2018-04-23T11:00:19.532Z","comments":true,"path":"posts/931939a5/","link":"","permalink":"http://weafteam.github.io/posts/931939a5/","excerpt":"","text":"jieba中文分词工具使用进阶篇，废话不多说吗，我们开始本次的学习吧~ 如何让分词的更加准确 我们之前举得例子有些文本其实很简单，我们后来确实换了官方的测试文本《围城》，但是均没避免一个问题，这些测试例都十分地中规中矩。在实际中需要我们做分词的文本可能是多种多样的，这时候的切词有可能会不太特别理想，导致分词的不准确。 那我们不妨下一个别的电子书（这里我下载的是《斗破苍穹》，为了测试我只用了第一章的文本），然后再进行切词，看下是否存在这样的问题。这里我们稍微改改上次的去停用词的代码，代码如下： 1234567891011121314151617181920212223import sysimport jiebafrom os import pathd = path.dirname(__file__) # 获取当前文件的dir路径text_path = 'txt/chapter2.txt' #《斗破苍穹》第一章的文本路径text = open(path.join(d, text_path),'rb').read()def CutWords(text): mywordlist = [] seg_list = jieba.cut(text, cut_all=False) liststr=\"/ \".join(seg_list) # 添加切分符 for myword in liststr.split('/'): if len(myword.strip())&gt;1: mywordlist.append(myword) return ''.join(mywordlist) #返回一个字符串txt5 = CutWords(text)text_write = 'txt/5.txt'with open(text_write,'w') as f: f.write(txt5) print(\"Success\") 结果如下： 终于被我们找到了一个切词错误，原文是这样的： 萧媚脑中忽然浮现出三年前那意气风发的少年 按照我们正常的断句，应为： 萧媚/脑中/忽然/浮现….，而jieba却认为“萧媚脑”是一个单词，从而导致此处分词不理想。 jieba考虑了这种情况，而且有很多的应对方案，下面我们先说最简单的。 调整词典 方法1：动态修改词典 使用add_word(word,freq=None,tag=None)和del_word(word)可在程序中动态的修改词典，具体操作如下： 12345678910import sysimport jiebafrom os import pathd = path.dirname(__file__) # 获取当前文件的dir路径# 此处增加代码jieba.add_word('脑中') ···· 结果如下： 果然，这样的方法很直接的把我们原来切错的词变成了正确的词。与add_word()相对应的是delete_word()方法，根据字面意思我们也很容易理解delete_word()方法的作用，这里我就不做过多的演示了，大家在实际场景中直接运用就好了。 方法2：调节词频 使用suggest_freq(segment, tune=True)调节单个词语的词频，使得它更容易被分出来，或者不被分出来。 但是需要注意的是：自动计算的词频在使用 HMM 新词发现功能时可能无效。 所以此时我们在做切词的时候需要把是HMM置为False。我们看下官方给的Demo（如果关闭HMM，很多新发现的词都消失了，所以‘萧媚脑’也消失了，无法做测试，我们的例子也是为了方便大家理解，所以也没必要非得针对这一个词做词频调节），具体的做法如下： 12345678910111213import jiebaprint('/'.join(jieba.cut('如果放到post中将出错。', HMM=False)))jieba.suggest_freq(('中', '将'), True)print('/'.join(jieba.cut('如果放到post中将出错。', HMM=False)))print('/'.join(jieba.cut('「台中」正确应该不会被切开', HMM=False)))jieba.suggest_freq('台中', True)print('/'.join(jieba.cut('「台中」正确应该不会被切开', HMM=False))) 结果： 对比下结果，不难发现suggest_freq()的使用方法，通过这样的强调高频词和低频词的方法可以做到分词更准确。 添加自定义词典 比起默认的词典，我们自定义的词典更适合我们自己的文本，这一点是毋庸置疑的。 词典格式和 dict.txt 一样，一个词占一行；每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。file_name 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码。 这里我们的词典为： 12345678910云计算 5李小福 2 nr创新办 3 ieasy_install 3 eng好用 300韩玉赏鉴 3 nz八一双鹿 3 nz台中凱特琳 nzEdu Trust认证 2000 我们这个例子也用官方的Demo，代码如下： 12345678910111213141516171819202122232425262728293031323334import syssys.path.append(\"../\")import jiebajieba.load_userdict(\"userdict.txt\")# jieba在0.28版本之后采用延迟加载方式# “import jieba”不会立即触发词典的加载，而是在有必要的时候才会加载词典# 如果想手动加载，可执行代码： jieba.initialize() 进行手动初始化操作# 也正是有了延迟加载机制，我们现在可以改变主词典的路径：# jieba.set_dictionary('data/dict.txt.big')# 官方还提供了占用内存较小的词典和适用于繁体字的词典，均在官方的GitHub上，有需要的可以自行下载。import jieba.posseg as pseg# pseg切分可以显示词性# 以下三个操作是修改词典的巩固jieba.add_word('石墨烯')jieba.add_word('凱特琳')jieba.del_word('自定义词')test_sent = (\"李小福是创新办主任也是云计算方面的专家; 什么是八一双鹿\\n\"\"例如我输入一个带“韩玉赏鉴”的标题，在自定义词库中也增加了此词为N类\\n\"\"「台中」正確應該不會被切開。mac上可分出「石墨烯」；此時又可以分出來凱特琳了。\")words = jieba.cut(test_sent)print('/'.join(words))print(\"=\"*40)result = pseg.cut(test_sent)for w in result: print(w.word, \"/\", w.flag, \", \", end=' ') 结果如下： 像‘云计算’、‘创新办’等词在没加载词典的时候是不能被识别出来的。像‘石墨烯’等在没有add_word()的时候也是不能识别出来的。可见效果还是不错的。 并行分词 原理：将目标文本按行分隔后，把各行文本分配到多个 Python 进程并行分词，然后归并结果，从而获得分词速度的可观提升 但是令人遗憾的是，这个模块并不支持Windows平台，原因是因为jieba的该模块是基于python自带的 multiprocessing 模块，而这个模块并不支持Windows。这里我就贴一下用法，使用Linux系统的同学可以自行体验下这个可观的速度提升。 用法： jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数 jieba.disable_parallel() # 关闭并行分词模式 最后 以上所讲的内容在日常的使用中应该是够用了，当然像基于TextRank算法的关键词抽取等内容，我这里并没涉及，并不是因为不重要，而是我对这个算法还不是很了解，硬着头皮写肯定也是照本宣科，效果肯定很差，所以先挖个坑吧，以后再填。 感谢阅读~","categories":[{"name":"文本聚类","slug":"文本聚类","permalink":"http://weafteam.github.io/categories/文本聚类/"}],"tags":[{"name":"文本聚类","slug":"文本聚类","permalink":"http://weafteam.github.io/tags/文本聚类/"}]},{"title":"普通的 SQLAlchemy ORM 使用姿势","slug":"2018-03-12/usage-of-sqlalchemy","date":"2018-03-18T13:38:54.000Z","updated":"2018-04-23T11:00:19.519Z","comments":true,"path":"posts/39277c31/","link":"","permalink":"http://weafteam.github.io/posts/39277c31/","excerpt":"","text":"前言 SQLAlchemy 是 Python 世界中最常用的 SQL 工具之一，包含 SQL 渲染引擎和 ORM 两大部分，平时使用最多的就是 ORM。在我看来平时很多使用 ORM 的姿势是有问题的，或者说是不优雅的。所以这篇文章打算讲讲（搬运）其中一些普通的姿势和技巧（API 文档）。 property 和混合属性 property 下面是一个简单的用户表映射： 12345class User(Base): __tablename__ = 'user' id = Column(Integer, primary_key=True) name = Column(String(64)) password = Column(String(128)) 通常情况下，我们会加密用户的密码，在数据库中保存密文，但是这里有一个问题，我们得这么写： 1234# 创建用户user = User(name='zhang', password=encrypt('123456'))# 修改密码user.password = encrypt('654321') 这意味着我们需要不断的重复书写 encrypt 函数来保证加密了用户密码。 有没有什么方法能省去这一步呢？答案是 property。 现在把用户表映射改成这样： 12345678910111213class User(Base): __tablename__ = 'user' id = Column(Integer, primary_key=True) name = Column(String(64)) _password = Column(String(128)) @property def password(self): raise ValueError('write only!') @password.setter def password(self, value): self._password = encrypt(value) 现在只需要简单的写成： 1234# 创建用户user = User(name='zhang', password='123456')# 修改密码user.password = '654321' 就可以了。 关于 Python 中 property和描述符的使用值得再另写一篇文章描述，在这里就不详细说明了。 混合属性（hybrid_property） 上面的例子看上去让代码清爽了不少，但是有时候这种用法是无法满足需要的，譬如下面这个例子： 12345class Student(Base): __tablename__ = 'student' id = Column(Integer, primary_key=True) name = Column(String(64)) birthday = Column(DateTime) 这是一个学生表映射，增加了 birthday 字段。通常我们会保存用户的生日，再通过生日获取用户年龄。有了上面的例子，很容易写出获取年龄的代码： 123456class Student(Base): ... @property def age(self): return datetime.now().year - self.birthday.year 现在可以简单的使用 student.age 获取具体的生日。 这样做是有缺陷的：如果需要获取所有 18 岁的学生呢？我们希望可以这样写： 1session.query(Student).filter_by(age=18).all() 但是却没有任何结果返回。如果改成这样呢？ 1234now = datetime.now()start = datetime(now.year - 18, 1, 1)end = end = datetime(now.year + 1 - 18, 1, 1)session.query(Student).filter(Student.birthday &gt;= start, Student.birthday &lt; end).all() 这样倒是可以获取正确的结果了，但是也太丑了点吧？难道没办法写出像第一条一样的既清晰又简洁的查询么？ 答案自然是有的，SQLAlchemy 提供了混合属性（hybrid_property）来处理类似的情况，于是我们可以改写获取年龄的代码： 12345678910111213from sqlalchemy.ext.hybrid import hybrid_propertyfrom sqlalchemy import funcclass Student(Base): ... @hybrid_property def age(self): return datetime.now().year - self.birthday.year @age.expression def age(self): return datetime.now().year - func.year(self.birthday) 这里将原本的 property 替换为 SQLAlchemy 中的 hybrid_property，同时提供了一个 expression 装饰器，在被装饰的方法中把 Python 代码翻译成 SQL（代码示例的目标数据库为 MySQL，获取日期中的年份的函数为YEAR()，使用其他数据库请查阅对应数据库的相关文档）。有了这个方法，SQLAlchemy 就知道如何在 SQL 语句中处理 age 属性了。 接下来稍微提一下 hybrid_method。 和 hybrid_property 类似，只不过可以给 hybrid_method 传参数。下面这个例子不太合适，只为了展示hybrid_method 的功能。 如何找到所有 90 后同学？当然我们可以复用上面的 age 属性，先计算一下 90 后的同学现在多少岁，然后直接写在查询里就好： 1session.query(Student).filter(Student.age &gt;= now.year - 1990, Student.age &lt; now.year - 2000).all() 如果要判断某个学生是否是 90 后呢？又需要再写一遍： 12if now.year - 2000 &gt; student.age &gt;= now.year - 1990: ... 出现了很多不直观的代码，这时候可以使用 hybrid_method 简化： 123456789class Student(Base): ... @hybrid_method def born_after(self, years): return years + 10 &gt; self.birthday.year &gt;= years @born_after.expression def born_after(self, years): return and_(func.year(self.birthday) &lt; years + 10, func.year(self.birthday) &gt;= years) 于是现在可以这样做： 1234session.query(Student).filter(Student.born_after(1990)).all()if student.born_after(1990): ... 看上去好了一些（误 这一部分就到此为止，当然 hybrid 在 SQLAlchemy 中的用法不止上述这些，更详细和复杂的内容参见官方文档。 关联代理（association_proxy） 简化标量集合 关联代理用在有关联的表中，所以我们先创建如下映射关系： 1234567891011121314151617association = Table('association', Base.metadata, Column('blog_id', Integer, ForeignKey('blog.id'), primary_key=True), Column('tag_id', Integer, ForeignKey('tag.id'), primary_key=True))class Blog(Base): __tablename__ = 'blog' id = Column(Integer, primary_key=True) name = Column(String(64)) tags = relationship( 'Tag', secondary=association, backref=backref('blogs', lazy='dynamic'), lazy='dynamic')class Tag(Base): __tablename__ = 'tag' id = Column(Integer, primary_key=True) name = Column(String(64)) 一个经常被拿出来作为演示的 Many-To-Many 模型。 先填充一些数据： 12345In [1]: blog = Blog(name='first')In [2]: blog.tags.append(Tag(name='t1'))In [3]: blog.tags.append(Tag(name='t2'))In [4]: session.add(blog)In [5]: session.commit() 接下来就可以获取这些对象的所有信息了： 12345678In [4]: blog.tags.all()Out[4]: [&lt;Tag at 0x1fdbab6f198&gt;, &lt;Tag at 0x1fdbab6f208&gt;]In [5]: blog.tags.all()[0].nameOut[5]: 't1'In [6]: [t.name for t in blog.tags]Out[6]: ['t1', 't2'] 上面的操作有点复杂。对我们而言，Tag 对象只有 name 字段是有用的，为了获取 name 字段，我们要写很多额外的代码把 name 字段从 Tag 对象中剥离出来。association_proxy 就可以用来简化这个操作。 现在修改一下上面的 Blog 映射： 123456789from sqlalchemy.ext.associationproxy import association_proxyclass Blog(Base): ... tag_objects = relationship( 'Tag', secondary=association, backref=backref('blogs', lazy='dynamic'), lazy='dynamic') tags = association_proxy('tag_objects', 'name') 增加了一行 association_proxy 对象的声明，现在我们可以这样做： 12In [7]: blog.tagsOut[7]: ['t1', 't2'] 现在查询操作变得很简单了，但是新增标签的操作还是很麻烦： 1blog.tag_objects.append(Tag(name='t3')) 还是需要实例化一个 Tag 对象，能不能直接写： 1blog.tags.append('t4') 当然是可以的，只要再修改一下 association_proxy 的声明： 1234class Blog(Base): ... tags = association_proxy('tag_objects', 'name', creator=lambda name: Tag(name=name)) 参数 creator 接受一个可调用对象，它告诉 association_proxy 如何处理“新增”操作。 注意：creator 的默认参数是被代理对象的构造函数，如果提供了一个单参数的构造函数，那么可以省略 creator 参数。 简化关联对象 上面的例子里把 association 表作为一个普通的 Table 对象，是因为 association 中不需要保存额外信息，只需要作为 Blog 和 Tag 的中转。现在有了新的需求，我们需要知道每篇博客的标签是在什么时候加上的，这就需要在 association 表中增加一个额外的字段用来表示创建时间，同时为了获取这个时间，还要把 association 改造成一个真正的映射： 123456789101112131415161718192021class Association(Base): __tablename__ = 'association' blog_id = Column(Integer, ForeignKey('blog.id'), primary_key=True) tag_id = Column(Integer, ForeignKey('tag.id'), primary_key=True) created_at = Column(DateTime, default=datetime.now) blog = relationship('Blog', backref=backref('blog_tags', lazy='dynamic'), lazy='joined') tag = relationship('Tag', backref=backref('tag_blogs', lazy='dynamic'), lazy='joined')class Tag(Base): __tablename__ = 'tag' id = Column(Integer, primary_key=True) name = Column(String(64))class Blog(Base): __tablename__ = 'blog' id = Column(Integer, primary_key=True) name = Column(String(64)) 这里实际上是把 Many-To-Many 拆成了两个 One-To-Many。 然后构造一些数据： 1234567In [1]: blog = Blog(name='first') ...: tags = [Tag(name='t1'), Tag(name='t2')] ...: for tag in tags: ...: session.add(Association(blog=blog, tag=tag)) ...: session.add(blog) ...: session.add_all(tags) ...: session.commit() 现在就可以获取 Tag 和被添加的时间了： 12345In [2]: blog.blog_tags[0].tag.nameOut[2]: 't1'In [3]: blog.blog_tags[0].created_atOut[3]: datetime.datetime(2018, 3, 18, 16, 4, 17) 可以看到，给 Blog 增加标签要经过 Association 这个中间对象。虽然表结构的确如此，但是我们仍然希望 Association 表是透明的，仅当需要获取其中的创建时间时才明确获取 Association 对象。只需要在 Blog 中声明一个关联代理： 1234class Blog(Base): ... tags = association_proxy('blog_tags', 'tag', creator=lambda tag: Association(tag=tag)) 然后就可以这样写了： 12In [4]: blog.tags[0].nameOut[4]: 't1' 添加新的 Tag 也方便了很多： 12In [3]: for tag in [Tag(name='t3'), Tag(name='t4')]: ...: blog.tags.append(tag) 混合关联代理 现在回到了第一个问题的出发点，能不能在上一个例子的基础上简化 tags 的调用呢？同样没问题，只要在 Association 中加一个关联代理： 12345class Association(Base): ... tag_objects = relationship('Tag', backref=backref('tag_blogs', lazy='dynamic'), lazy='joined') tags = association_proxy('tag_objects', 'name', creator=lambda name: Tag(name=name)) 然后用起来就和第一个例子一样了： 123456In [1]: blog.tagsOut[1]: ['t1', 't2']In [2]: blog.tags.append('t3')In [3]: blog.tagsOut[3]: ['t1', 't2', 't3'] 结语 上述内容并没有很复杂的操作，都是一些易于实现并且可以改善日常使用体验的方法。SQLAlchemy 还有很多骚操作可以讲，但是受限于本人的姿势水平，很多并没有实际使用过，也谈不上有什么见解。那就这样吧~","categories":[],"tags":[]},{"title":"文本聚类系列教程：（一）jieba中文分词工具入门","slug":"2018-03-12/文本聚类系列教程：（一）jieba中文分词工具入门","date":"2018-03-17T09:20:22.000Z","updated":"2018-04-23T11:00:19.522Z","comments":true,"path":"posts/575e441b/","link":"","permalink":"http://weafteam.github.io/posts/575e441b/","excerpt":"","text":"最近在学习文本分类（聚类）的相关知识，所以接下来准备先写一个关于这个方面的系列博客。 写在前面： 先介绍下由我们四个人组成的组织：FOUR ELEMENTS。四元素分别对应WELL、EARTH、AIR、FLAME，根据首字母缩写，我们的博客主页得名WEAF。 接下来介绍下我自己，我叫Leno，对应于四元素里面的Well，目前研究生在读，方向为智能信息处理。我的博客主要会以日常遇到的问题以及学习的知识为主。 简单的介绍： 首先，我们要做的是对中文文本的聚类，如果做聚类的话，我们需要对文本的内容做分析，而分析的最小单位肯定是词。 其次，中文和英文的词是有区别的，最大的区别就是中文的词与词之间并不是用空格分隔开的，而且由于中国文化的博大精深，切词的时候我们需要考虑的词语组合情况就更多了。显然让我们自己去造一个这样的轮子有点不现实，其实像这样的工具，前辈们已经为我们做好了，而且超好用。 本文介绍的就是jieba中文分词，正如它的口号那样。如下图所示： 当然，这里有两本秘籍GitHub &amp;&amp; OSChina，既然你我有缘，便免费赠予你。 安装 这年头，没有什么是一句pip install 解决不了的。不管2或者3，直接pip即可。 1pip install jieba 结合官方Demo理解jieba的三种切词模式 三种模式： 精确模式（默认模式）：它会试图将句子最精确的切开，适合文本分析。 全模式：不考虑歧义，这个模式会将所有的可以成词的词语都扫描出来，因而速度会非常快。 搜索引擎模式：该模式是在精确模式的基础上，对长词再进行切分，提高召回率，适用于搜索引擎分词。 官方Demo： 12345678910111213import jiebaseg_list = jieba.cut(&quot;我来到北京清华大学&quot;, cut_all=True)print(&quot;全模式: &quot; + &quot;/ &quot;.join(seg_list)) # 全模式seg_list = jieba.cut(&quot;我来到北京清华大学&quot;, cut_all=False)print(&quot;精确模式: &quot; + &quot;/ &quot;.join(seg_list)) # 精确模式seg_list = jieba.cut(&quot;他来到了网易杭研大厦&quot;) # 默认是精确模式print(&quot;默认模式：&quot; + &quot;/ &quot;.join(seg_list))seg_list = jieba.cut_for_search(&quot;小明硕士毕业于中国科学院计算所，后在日本京都大学深造&quot;) # 搜索引擎模式print(&quot;搜索引擎模式：&quot; + &quot;/ &quot;.join(seg_list)) 结果： 模式分析： 这里我们先分析这三种模式，对于cut方法的讲解在后边会给出，so不要问我为啥不给出cut方法中第三个参数HMM。 通过对比前两条输出可以看出全模式情况下，它会找出所有可以组成词的划分，而精确模式与其对比给出的答案就会很清爽。所以结合上文所说，不难理解这两个模式的区别。 接下来我们看第四条输出，它是在精确模式的基础上对长词再做的划分。所以‘日本京都大学’，它会再次切分为‘日本’，‘京都’，‘大学’三个词，同理适用于‘中国科学院’。所以这个模式也不难理解吧。 补充分析： 最后看第三条输出内容，也许你会问，既然知道默认模式是精确模式了，为啥还要给出试例，况且还是一个不具有对比性质的对比。这里其实想说明的是： ‘杭研’并没有在词典中，但是jieba的Viterbi算法也将其识别了出来。 这时我们就需要考虑HMM这个参数了，关于HMM（Hidden Markov Model，HMM：隐马尔可夫模型），如果深究，那就需要另外一篇博文了，所以我们只要能理解官方给出的这句话即可：对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法。 可能说的比较干涩，我们实际测一下吧。 补充测试代码： 1234567import jiebaseg_list = jieba.cut(&quot;他来到了网易杭研大厦&quot;,HMM=False)print(&quot;HMM为False：&quot; + &quot;/ &quot;.join(seg_list))seg_list = jieba.cut(&quot;他来到了网易杭研大厦&quot;,HMM=True)print(&quot;HMM为True：&quot; + &quot;/ &quot;.join(seg_list)) 补充测试结果： 所以一般情况下，使用cut方法，不用考虑HMM这个参数就可以，让它默认为True即可，让Viterbi算法为我们识别新词。HMM也能有效的解决中文中的歧义问题。 启用HMM并不适用所有情况，根据需要开启！！！ 关于切词的方法以及切词的注意事项，请大家参考上文给出的两个链接，这里我不再赘述。 基于TF-IDF的关键词提取 相关知识： 对于一个文档，我们肯定不会对所有的词进行聚类，所以我们需要对文档进行关键词提取。 下面我们对TF-IDF做一下简单的说明。如果单讲这个知识点，拿出来又是一篇博文。不过后续我也会写一篇关于它的博文。暂时请大家自行查阅相关资料学习。 TF-IDF是一种统计方法，用于评估一个词对于一个文件集或者语料库中的一份文件的重要程度。 TF(term frequency)：指的是某一个给定的词语在该文件中出现的频率。公式如下： \\(tf_i,_j = \\frac{n_i,_j}{\\sum_k n_k,_j}\\) IDF(Inverse document frequency)：是一个词语普遍重要性的度量。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到： \\(idf(t,D) = log(\\frac{N}{\\lvert {d \\in D, t \\in d}\\rvert})\\) 关键词提取： 官方给了一个代码示例文件，源代码在这里：关键词提取源码 但是为了结果显示得更清晰一点，我做了些许的改动： 12345678910111213141516171819202122232425262728293031import syssys.path.append(&apos;../&apos;)import jiebaimport jieba.analysefrom optparse import OptionParserUSAGE = &quot;usage: python extract_tags.py [file name] -k [top k]&quot;parser = OptionParser(USAGE)parser.add_option(&quot;-k&quot;, dest=&quot;topK&quot;)opt, args = parser.parse_args()if len(args) &lt; 1: print(USAGE) sys.exit(1)file_name = args[0]if opt.topK is None: topK = 20else: topK = int(opt.topK)content = open(file_name, &apos;rb&apos;).read()tags = jieba.analyse.extract_tags(content, topK=topK,withWeight=True)for i in tags : print(i) 先说下用法，官方在文件的第8行给出了用法，即： 1python extract_tags.py [file name] -k [top k] 将这个Extract_tags.py文件和文本文件放在同一目录下，然后给利用如上命令便可得到文本的关键词。默认取得是top10，我改了下取了top20，我们这里做下测试（使用jieba的官方测试文档：《围城》），结果如下： 分析： 官方给的代码看着挺长，实际上超简单，其中重要的无非两句话，一句是读文件，另一句则是调用extract_tags()方法，我在原有的基础上设置了withWight=True，因而返回了一个权重值。大家如果嫌麻烦可以对上述关键代码进行抽取，写一个自己的测试。 正如上图所示，‘自己’、‘知道’、‘先生’等等等等，像这些词语都是些没有实际意义的单词，所以在聚类的时候这些单词不应该做为聚类（或者分类）的标准，它们属于stop_words，中文的意思就是停用词，所以我们接下来处理这个问题。 去除停用词 去除停用词，我们需要知道哪些属于停用词，我在CSDN上找到了一个1893规模的停用词表，链接如下：最全中文停用词表整理（1893个）。 我们接下来的工作思路是这样的，对《围城》（文件1.txt）进行切词，方法就是之前的cut()，读取StopWords文件，对比每个切分出来的单词是否是停用词，如果不是则加入到一个list中，然后再将这个list的内容存到另一个文件2.txt中，对文件2.txt使用之前说到的官方给的关键词提取文件做关键词提取即可。 去除停用词代码如下： 12345678910111213141516171819202122232425262728293031import sysimport jiebafrom os import pathd = path.dirname(__file__) # 获取当前文件的dir路径stopwords_path = &apos;stopwords1893.txt&apos; # 停用词表路径text_path = &apos;txt/1.txt&apos; #《围城》的文本路径text = open(path.join(d, text_path),&apos;rb&apos;).read()def RmStopWords(text): mywordlist = [] seg_list = jieba.cut(text, cut_all=False) liststr=&quot;/ &quot;.join(seg_list) # 添加切分符 f_stop = open(stopwords_path) try: f_stop_text = f_stop.read() finally: f_stop.close( ) f_stop_seg_list=f_stop_text.split(&apos;\\n&apos;) # 停用词是每行一个，所以用/n分离 for myword in liststr.split(&apos;/&apos;): #对于每个切分的词都去停用词表中对比 if not(myword.strip() in f_stop_seg_list) and len(myword.strip())&gt;1: mywordlist.append(myword) return &apos;&apos;.join(mywordlist) #返回一个字符串txt2 = RmStopWords(text)text_write = &apos;txt/2.txt&apos;with open(text_write,&apos;w&apos;) as f: f.write(txt2) print(&quot;Success&quot;) 结果： 分析： 由上图可见，我们的去停用词的效果还不错。 最后： 这篇博客先写到这里，下一篇博客我会讲到jieba中文分词的进阶篇。感谢阅读，如有问题可以通过邮件与我交流，邮箱：cliugeek@us-forever.com","categories":[{"name":"文本聚类","slug":"文本聚类","permalink":"http://weafteam.github.io/categories/文本聚类/"}],"tags":[{"name":"文本聚类","slug":"文本聚类","permalink":"http://weafteam.github.io/tags/文本聚类/"}]},{"title":"MySQL主从数据库的设置与Xtrabackup备份InnoDB(MySQL)","slug":"2018-03-12/linux-mysql","date":"2018-03-17T08:08:56.000Z","updated":"2018-04-23T11:00:19.517Z","comments":true,"path":"posts/2f5dded6/","link":"","permalink":"http://weafteam.github.io/posts/2f5dded6/","excerpt":"一、准备环境 两台服务器：服务器A、服务器B 服务器A：Red Hat Enterprise Linux Server release 6.5 (Santiago) 服务器B：Red Hat Enterprise Linux Server release 6.5 (Santiago) 服务器A IP：172.16.125.50 服务器B IP：172.16.125.52 MySQL版本：5.6.23 二、安装MySQL 具体安装请见 LinuxMySQL的安装(1) LinuxMySQL的安装(2) LinuxMySQL的安装(3)","text":"一、准备环境 两台服务器：服务器A、服务器B 服务器A：Red Hat Enterprise Linux Server release 6.5 (Santiago) 服务器B：Red Hat Enterprise Linux Server release 6.5 (Santiago) 服务器A IP：172.16.125.50 服务器B IP：172.16.125.52 MySQL版本：5.6.23 二、安装MySQL 具体安装请见 LinuxMySQL的安装(1) LinuxMySQL的安装(2) LinuxMySQL的安装(3) 三、主从库配置 1、主库在/etc/my.cnf里添加以下内容 12345678910#log日志log_bin=mysql_bin#server IDserver_id=2#忽略同步的库binlog-ignore-db=information_schemabinlog-ignore-db=clusterbinlog-ignore-db=mysql#需要同步的库binlog-do-db=test 2、从库在/etc/my.cnf里添加以下内容 12345678910log_bin=mysql_binserver_id=3binlog-ignore-db=information_schemabinlog-ignore-db=clusterbinlog-ignore-db=mysqlreplicate-do-db=ufind_dbreplicate-ignore-db=mysqllog-slave-updatesslave-skip-errors=allslave-net-timeout=60 四、主从库设置 1、进入主库，我们在主库中创建一个的账户，从库通过使用这个账号来同步数据。 1CREATE USER 'repl'@'172.16.125.52' IDENTIFIED BY '123456'; 2、赋予相应的权限 12345GRANT FILE ON *.* TO 'repl'@'172.16.125.52' IDENTIFIED BY '123456';GRANT REPLICATION SLAVE ON *.* TO 'repl'@'172.16.125.52' IDENTIFIED BY '123456';FLUSH PRIVILEGES; 3、重启数据库（主库）执行以下命令 1SHOW MASTER STATUS; 要记住以上的信息，在设置从库的时候需要填写并设置。 4、在从库里边执行以下命令 123stop slave;change master to master_host=&apos;172.16.125.50&apos;,master_user=&apos;repl&apos;,master_password=&apos;123456&apos;,master_log_file=&apos;mysql_bin.000023&apos;, master_log_pos=120;start slave; 5、然后执行一下命令查看状态 1show slave status \\G; 内容如下： 6、测试与提示 后期的测试中我们只针对test库进行了同步。 所以只能针对test进行的操作才有效。 如果后期对一些列库进行操作，需要 添加相应的配置 1234#主库配置文件binlog-do-db=test#从库配置文件replicate-do-db=test 并查询出最新的master的状态，停止从库。并改变从库的配置重启同步。 五、Xtrabackup的简单介绍 ——————- Percona XtraBackup 是世界上唯一的开源免费的MySQL热备份软件，可以执行非阻塞操作 InnoDB和XtraDB数据库的备份。 Percona XtraBackup可提供以下优点： 备份快速安全可靠 备份期间不间断的事务处理 节省磁盘空间和网络带宽 自动备份验证 更快的恢复时间保证正常工作 Percona XtraBackup 为所有版本的Percona服务器，MySQL和MariaDB提供MySQL热备份。 它可执行 流媒体，压缩和增量MySQL备份。 六、Xtrabackup的安装 如果在互联网下 可使用以下命令安装 1wget https://www.percona.com/downloads/XtraBackup/Percona-XtraBackup-2.4.4/binary/redhat/7/x86_64/percona-xtrabackup-24-2.4.4-1.el7.x86_64.rpm 获取相应rpm包 安装部分依赖(不同的操作系统可能已安装的库不尽相同) 1234rpm -ivh mysql-community-libs-compat-5.7.20-1.el7.x86_64.rpm#根据mysql版本而定yum list|grep perlyum -y install perl-DBI.x86_64 perl-DBD-MySQL.x86_64 然后安装Xtrabackup 1rpm -ivh percona-xtrabackup-24-2.4.4-1.el7.x86_64.rpm 参考： 1yum install cmake gcc gcc-c++ libaio libaio-devel automake autoconf bison libtool ncurses-devel libgcrypt-devel libev-devel libcurl-devel vim-common 七、Xtrabackup备份MySQL 12xtrabackup --defaults-file=/etc/my.cnf --user=root --password=root --host=localhost --backup --target-dir=/data/backups/可指定数据库--databases=test 八、Xtrabackup的备份恢复 备份之前必须先关闭MySQL server 然后删除data目录（/var/lib/mysql一般情况是这个） 1xtrabackup --copy-back --target-dir=/data/backups/ 执行完恢复之后需要设置文件权限 1chown -R mysql:mysql /var/lib/mysql 然后启动mysql 123systemctl start mysqld.service#或者使用服务service mysqld start 九、使用脚本自动备份7天之内的数据 12345678910111213141516171819202122#!/bin/sh# Database infoDB_USER=\"root\"DB_PASS=\"root\"DB_HOST=\"localhost\"# Others varsBCK_DIR=\"/opt/app/mysqlbackup\" #the backup file directoryCONF_DIR=\"/etc/my.cnf\"DATE=`date +%F`RMDATE=`date -d '-7 day' +%F`# TODOmkdir -p $BCK_DIR/$DATE/#Create dir for save backup dataxtrabackup --defaults-file=$CONF_DIR --user=$DB_USER --password=$DB_PASS --host=$DB_HOST --backup --target-dir=$BCK_DIR/$DATE/#Backup mysql datarm -rf $BCK_DIR/$RMDATE#Delete the backup 7 days ago#热备份数据库 加入crontab 130 2 * * * /bin/sh /home/scripts/mysqlbackup.sh 更多请参考官方文档","categories":[{"name":"Linux","slug":"Linux","permalink":"http://weafteam.github.io/categories/Linux/"}],"tags":[{"name":"Linux运维","slug":"Linux运维","permalink":"http://weafteam.github.io/tags/Linux运维/"}]},{"title":"chapter-01-AIR","slug":"2018-03-12/chapter-01-AIR","date":"2018-03-14T10:49:23.000Z","updated":"2018-04-23T11:00:19.512Z","comments":true,"path":"posts/8e8e4531/","link":"","permalink":"http://weafteam.github.io/posts/8e8e4531/","excerpt":"","text":"第一篇文章-TensorFlow Install 首先介绍一些我们这个组织，这是有四个人构成得一个组织，组织可以叫FOUR ELEMENTS。（也可以叫WEAF）分别对应WELL、EARTH、AIR、FLAME。（WEAF）。 其次我想做一下自我介绍，我的英文学名叫milittle。我开设的这个周刊名字叫AIR-周刊。希望把自己学习的一些内容分享给大家，也激励自己。学更多的知识。以后大家有什么要交流的，也可以一起交流。（邮箱地址会在文章末尾给出） 接下来我讲一下我后续每周在AIR-周刊里面会讲到的内容： 主要涉及TensorFlow框架使用多一些 后续也会分享一些机器学习方面的算法 也会有一些在人工智能方面的杂谈 上面说了一些，我想把这块做好，文章内容有什么变化，后续的文章里面会有所提及。 今天就介绍一些TensorFlow的简述和安装： TensorFlow是Google公司在2015年12月份开源的一个机器学习库，代码链接TensorFLow。 第二点为什么现在TensorFlow这么火，在人工智能界已经算得上是称霸的地位，我们可以从下面的图中可以看出TensorFlow的数据占据了一大半市场。 原因是什么呢 最主要的原因就是本身具有图运算的这个概念。使用简单，而且可以让程序员快捷的实现一些算法。从而可以用TensorFlow解决一些现实中的问题。图运算的概念我们后续会慢慢深入。大家不要着急。 还有一个原因，我想不用说大家也都知道，既然说了是Google的开源框架，那么技术就一定很牛逼。引得广大程序员的喜爱也是必然发生的事情。 而且用这个框架可以快速的解决一些机器学习的算法问题。是的编程效率也不断提高。 TensorFlow支持Mac、Windows、Linux。以后我们的实验有可能通过Windows进行，也有可能在Linux进行，而且以后的代码都是基于python3.X，所以希望大家可以实现基本的python3的语法知识和编程知识。还有就是TensorFlow支持CPU版本和GPU版本，安装的时候都有很多的注意事项，基于GPU版本的可能会比较麻烦。但是后续我会给大家出一个教程，分别在Windows下面和Linux下面配置自己的独立环境。让你的机器学习算法跑在你自己的机器上面。完成一些看起来炫酷的程序。 接下来我介绍一下TensorFlow的Windows CPU安装方法： 首先打开电脑，这个是一定的~ 去TensorFlow的官网下载Windows的版本。点击下面红色箭头的地方—随意，都可以跳转到一个关于windows安装的界面。（可能需要科学上网，逃） 点开界面以后的注意事项： windows7及其以后的操作系统版本 决定安装哪个TensorFlow的版本，GPU还是CPU（GPU会有有一些第三方的库依赖，CUDA），接下来我们的教程是CPU版本安装。 决定怎么安装TensorFlow：可选方式有native pip 和 Anaconda等（我们使用Anaconda） 最后一步验证你的安装效果 接下来一步一步来： 第一步、我们决定用Anaconda来安装TensorFlow，你要知道Anaconda是什么呢，它就是可以很好的管理python的一些依赖库。让你在不同python版本之间切换自如。所以我们使用这个工具来安装我们的TensorFlow。Anaconda也可以集成Spyder这些编程工具，使得你编写代码会方便一些。 第二步、首先你去Anaconda官网下载windows版本的Anaconda，具体安装就和普通的安装软件类似。这个地方需要注意的是不同python版本需要不同的Anaconda，别下错了。 第三步、安装好以后，我们打开Anaconda的控制台，就是开始里面找到Anaconda的应用，然后里面有一个Anaconda Prompt。打开以后，我们就开始了我们创建一个独立的TensorFlow独立的环境。 conda create -n tensorflow pip python=3.5 上面这命令的意思就是说在Anaconda管理的环境里面给我独立的创建一个python环境来，这个里面python的版本是3.5。注意一下，这个地方还没有安装tensorflow呢，上面的tensorflow只不过是创建的一个环境名字而已。 activate tensorflow 上面的命令是激活这个tensorflow的环境，你可以通过这个环境，添加一些你自己的python库，定制自己的python环境，这也是我使用Anaconda的原因，但是并不是只有Anaconda支持这样的方式。不要和我抬杠。 第四步，也就是正儿八经的安装TensorFlow的阶段，这里解释一下，上面为什么我执行的是tensorflow1，因为我的电脑上面已经有tensorflow这个环境了 pip install --ignore-installed --upgrade tensorflow 这个命令就是使用pip正常的安装tensorflow，这里的pip管理起来和普通的pip管理是一个道理，这里就不赘述了。 第五步，测试TensorFlow是否安装上 python 上面的命令是进入python解释器，然后执行下面的import语句 import tensorflow as tf 如果上面的命令执行完，如下图中一样，就算安装成功了，下面的那些语句是写了一个hello world！！！ 今天是为了我们以后在TensorFlow上开发所做的准备。希望大家安装顺利。我的个人邮箱是air@weaf.top。有什么问题可以单独发邮件问我。感谢你们的驻足。有什么不好的地方，可以给出意见。","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/categories/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://weafteam.github.io/tags/TensorFlow/"}]}]}